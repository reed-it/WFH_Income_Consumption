{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c604c8-ddbd-4f6b-bd96-40229b473155",
   "metadata": {},
   "source": [
    "In this notebook, we will calculate consumption using Aguiar & Bills (2015) measure and calculate the Consmption Inequality index of WFH and non-WFH households across time.\n",
    "Since the LCF survey is made up of thousands of respondents, we transform the data into a format that is suitable for a trend analysis.\n",
    "In a nutshell, we will aggregate the incomes and expenses for each year, grouped by WFH and non-WFH, and combine them into one timeseries dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72cdf469-443d-4c3c-89c5-518bb101ce24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\Anaconda3 Files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\AppData\\Local\\Temp\\ipykernel_42804\\2586666413.py:8: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  pd.set_option('use_inf_as_na', True)\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#set no limits on data display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "#getting the work directory\n",
    "import os \n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968e287-44d6-4830-9567-0b2cbfe5770f",
   "metadata": {},
   "source": [
    "Import data then join with raw_per table on case ID\n",
    "In this section, we will join the data extracted from the raw_per table to the extracted dv_hh table using case ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd483b2-8347-41ec-97be-1c6ecdf4b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "df_16 = pd.read_csv('UK LCF Data Set Clean/LCFS_2016_Clean.csv')\n",
    "df_17 = pd.read_csv('UK LCF Data Set Clean/LCFS_2017_Clean.csv')\n",
    "df_18 = pd.read_csv('UK LCF Data Set Clean/LCFS_2018_Clean.csv')\n",
    "df_19 = pd.read_csv('UK LCF Data Set Clean/LCFS_2019_Clean.csv')\n",
    "df_20 = pd.read_csv('UK LCF Data Set Clean/LCFS_2020_Clean.csv')\n",
    "df_21 = pd.read_csv('UK LCF Data Set Clean/LCFS_2021_Clean.csv')\n",
    "df_22 = pd.read_csv('UK LCF Data Set Clean/LCFS_2022_Clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9368840-862a-4c4b-8a5a-9895aa18d422",
   "metadata": {},
   "source": [
    "Filter dataframes to show only salaried wages.\n",
    "In this section we filter the dataframes to only show observations where the respondents are salaried workers. Retirees and self-employed respondents are filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7114baa-79fb-46cb-852a-f449fd632b00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Filtering only respondents with regular salaried wages\n",
    "\n",
    "df_16 = df_16[df_16['p425'] == 1]\n",
    "df_17 = df_17[df_17['p425'] == 1]\n",
    "df_18 = df_18[df_18['p425'] == 1]\n",
    "df_19 = df_19[df_19['p425'] == 1]\n",
    "df_20 = df_20[df_20['p425'] == 1]\n",
    "df_21 = df_21[df_21['p425'] == 1]\n",
    "df_22 = df_22[df_22['p425'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8774935-0c6f-4b3e-897a-46cdba064db6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Round up all values to closest 4 decimal points\n",
    "df_16 = df_16.round(4)\n",
    "df_17 = df_17.round(4)\n",
    "df_18 = df_18.round(4)\n",
    "df_19 = df_19.round(4)\n",
    "df_20 = df_20.round(4)\n",
    "df_21 = df_21.round(4)\n",
    "df_22 = df_22.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e1863-692b-47ae-87ad-6e2d742dcfa2",
   "metadata": {},
   "source": [
    "Calculate the 90th & 10th percentile of income as a grouping method. As per Aguiar and Bills (2015), the income grouping will be used to calculate the 90/10 percentile ratio of consumption since we do not use the 90th and 10th percentile of the consumption measure to do the 90/10 ratio for consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7d779-1959-40ab-84f0-ab09b26fceb5",
   "metadata": {},
   "source": [
    "Calculate the 90/10th percentile ratio as measure of income inequality\n",
    "\n",
    "Steps:\n",
    "- determine the 90th and 10th percentile of income in each year, and for each group\n",
    "- calculate the 90/10 ratio by dividing the 90th and 10th percentile of income\n",
    "- the ratio uses the income at the 10th and 90th percentile as opposed to the mean of income within the bottom 10th and top 10th percentile of income per Paul's advice on 4th July 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8273f81f-8c62-467f-90b2-91cf011e7e14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 Income Percentile Grouping - Overall\n",
      "10th percentile: 407.08\n",
      "90th percentile: 1,827.81\n"
     ]
    }
   ],
   "source": [
    "# calculate 2016's 10th and 90th percentile income\n",
    "\n",
    "df16_income_clean = pd.to_numeric(df_16['p344p'], errors='coerce').dropna()\n",
    "df16_percentiles = df16_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2016 Income Percentile Grouping - Overall\")\n",
    "print(f\"10th percentile: {df16_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df16_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6856c4f1-2073-4fc0-bad2-9849b5901c80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 Income Percentile Grouping - wfh\n",
      "10th percentile: 505.07\n",
      "90th percentile: 2,021.36\n"
     ]
    }
   ],
   "source": [
    "# calculate 2016's 10th and 90th percentile income for WFH group\n",
    "\n",
    "df16_wfh_income_clean = pd.to_numeric(df_16['p344p-wfh'], errors='coerce').dropna()\n",
    "df16_wfh_percentiles = df16_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2016 Income Percentile Grouping - wfh\")\n",
    "print(f\"10th percentile: {df16_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df16_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33ce8b1-6eca-436f-b3cf-3878ba461b70",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 Income Percentile Grouping - non_wfh\n",
      "10th percentile: 357.09\n",
      "90th percentile: 1,491.49\n"
     ]
    }
   ],
   "source": [
    "# calculate 2016's 10th and 90th percentile income for non_wfh group\n",
    "\n",
    "df16_non_wfh_income_clean = pd.to_numeric(df_16['p344p-non_wfh'], errors='coerce').dropna()\n",
    "df16_non_wfh_percentiles = df16_non_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2016 Income Percentile Grouping - non_wfh\")\n",
    "print(f\"10th percentile: {df16_non_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df16_non_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce7ee1e3-8b08-40bf-93f8-ccdc5f115921",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Income Percentile Grouping - Overall\n",
      "10th percentile: 402.10\n",
      "90th percentile: 1,933.38\n"
     ]
    }
   ],
   "source": [
    "# calculate 2017's 10th and 90th percentile income\n",
    "\n",
    "df17_income_clean = pd.to_numeric(df_17['p344p'], errors='coerce').dropna()\n",
    "df17_percentiles = df17_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2017 Income Percentile Grouping - Overall\")\n",
    "print(f\"10th percentile: {df17_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df17_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a5b4af-cdf9-46a5-b64a-4d962291a7b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Income Percentile Grouping - wfh\n",
      "10th percentile: 512.61\n",
      "90th percentile: 2,254.88\n"
     ]
    }
   ],
   "source": [
    "# calculate 2017's 10th and 90th percentile income for WFH group\n",
    "\n",
    "df17_wfh_income_clean = pd.to_numeric(df_17['p344p-wfh'], errors='coerce').dropna()\n",
    "df17_wfh_percentiles = df17_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2017 Income Percentile Grouping - wfh\")\n",
    "print(f\"10th percentile: {df17_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df17_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd263887-287d-40ef-a660-414f7f03b4e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Income Percentile Grouping - non_wfh\n",
      "10th percentile: 334.68\n",
      "90th percentile: 1,273.67\n"
     ]
    }
   ],
   "source": [
    "# calculate 2017's 10th and 90th percentile income for non_wfh group\n",
    "\n",
    "df17_non_wfh_income_clean = pd.to_numeric(df_17['p344p-non_wfh'], errors='coerce').dropna()\n",
    "df17_non_wfh_percentiles = df17_non_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2017 Income Percentile Grouping - non_wfh\")\n",
    "print(f\"10th percentile: {df17_non_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df17_non_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66914fb5-9c2f-4526-ab6e-3814bd3ad259",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 Income Percentile Grouping - Overall\n",
      "10th percentile: 405.18\n",
      "90th percentile: 1,863.80\n"
     ]
    }
   ],
   "source": [
    "# calculate 2018's 10th and 90th percentile income\n",
    "\n",
    "df18_income_clean = pd.to_numeric(df_18['p344p'], errors='coerce').dropna()\n",
    "df18_percentiles = df18_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2018 Income Percentile Grouping - Overall\")\n",
    "print(f\"10th percentile: {df18_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df18_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afd82a86-1935-46cc-bb77-6384094abbb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 Income Percentile Grouping - wfh\n",
      "10th percentile: 599.50\n",
      "90th percentile: 2,159.66\n"
     ]
    }
   ],
   "source": [
    "# calculate 2018's 10th and 90th percentile income for WFH group\n",
    "\n",
    "df18_wfh_income_clean = pd.to_numeric(df_18['p344p-wfh'], errors='coerce').dropna()\n",
    "df18_wfh_percentiles = df18_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2018 Income Percentile Grouping - wfh\")\n",
    "print(f\"10th percentile: {df18_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df18_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e33df84f-0918-40d9-b277-ad4d9553bdc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 Income Percentile Grouping - non_wfh\n",
      "10th percentile: 335.59\n",
      "90th percentile: 1,225.04\n"
     ]
    }
   ],
   "source": [
    "# calculate 2018's 10th and 90th percentile income for non_wfh group\n",
    "\n",
    "df18_non_wfh_income_clean = pd.to_numeric(df_18['p344p-non_wfh'], errors='coerce').dropna()\n",
    "df18_non_wfh_percentiles = df18_non_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2018 Income Percentile Grouping - non_wfh\")\n",
    "print(f\"10th percentile: {df18_non_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df18_non_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8727ec58-ee1e-4330-8d54-969235ce9d2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 Income Percentile Grouping - Overall\n",
      "10th percentile: 440.57\n",
      "90th percentile: 1,925.42\n"
     ]
    }
   ],
   "source": [
    "# calculate 2019's 10th and 90th percentile income\n",
    "\n",
    "df19_income_clean = pd.to_numeric(df_19['p344p'], errors='coerce').dropna()\n",
    "df19_percentiles = df19_income_clean.quantile([0.1, 0.9])\n",
    "    \n",
    "print(\"2019 Income Percentile Grouping - Overall\")\n",
    "print(f\"10th percentile: {df19_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df19_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14b5381d-7f3b-4566-bca1-33f74773ed44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 Income Percentile Grouping - wfh\n",
      "10th percentile: 578.79\n",
      "90th percentile: 2,287.94\n"
     ]
    }
   ],
   "source": [
    "# calculate 2019's 10th and 90th percentile income for WFH group\n",
    "\n",
    "df19_wfh_income_clean = pd.to_numeric(df_19['p344p-wfh'], errors='coerce').dropna()\n",
    "df19_wfh_percentiles = df19_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2019 Income Percentile Grouping - wfh\")\n",
    "print(f\"10th percentile: {df19_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df19_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "651acce1-73d3-4529-ac70-ae69e54f7d05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 Income Percentile Grouping - non_wfh\n",
      "10th percentile: 336.82\n",
      "90th percentile: 1,210.29\n"
     ]
    }
   ],
   "source": [
    "# calculate 2019's 10th and 90th percentile income for non_wfh group\n",
    "\n",
    "df19_non_wfh_income_clean = pd.to_numeric(df_19['p344p-non_wfh'], errors='coerce').dropna()\n",
    "df19_non_wfh_percentiles = df19_non_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2019 Income Percentile Grouping - non_wfh\")\n",
    "print(f\"10th percentile: {df19_non_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df19_non_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c02c3a4-8254-4eb3-b911-029e2fdd268e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 Income Percentile Grouping - Overall\n",
      "10th percentile: 452.46\n",
      "90th percentile: 1,998.58\n"
     ]
    }
   ],
   "source": [
    "# calculate 2020's 10th and 90th percentile income\n",
    "\n",
    "df20_income_clean = pd.to_numeric(df_20['p344p'], errors='coerce').dropna()\n",
    "df20_percentiles = df20_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "    \n",
    "print(\"2020 Income Percentile Grouping - Overall\")\n",
    "print(f\"10th percentile: {df20_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df20_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3db2ada9-e899-4eaf-a1bf-a7534370d674",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 Income Percentile Grouping - wfh\n",
      "10th percentile: 558.01\n",
      "90th percentile: 2,237.70\n"
     ]
    }
   ],
   "source": [
    "# calculate 2020's 10th and 90th percentile income for WFH group\n",
    "\n",
    "df20_wfh_income_clean = pd.to_numeric(df_20['p344p-wfh'], errors='coerce').dropna()\n",
    "df20_wfh_percentiles = df20_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2020 Income Percentile Grouping - wfh\")\n",
    "print(f\"10th percentile: {df20_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df20_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c335ee-e348-43c9-8495-c5f22459b9f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 Income Percentile Grouping - non_wfh\n",
      "10th percentile: 372.39\n",
      "90th percentile: 1,570.24\n"
     ]
    }
   ],
   "source": [
    "# calculate 2020's 10th and 90th percentile income for non_wfh group\n",
    "\n",
    "df20_non_wfh_income_clean = pd.to_numeric(df_20['p344p-non_wfh'], errors='coerce').dropna()\n",
    "df20_non_wfh_percentiles = df20_non_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2020 Income Percentile Grouping - non_wfh\")\n",
    "print(f\"10th percentile: {df20_non_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df20_non_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "532ca678-be67-4148-8c44-ebcad56806c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 Income Percentile Grouping - Overall\n",
      "10th percentile: 479.76\n",
      "90th percentile: 2,155.85\n"
     ]
    }
   ],
   "source": [
    "# calculate 2021's 10th and 90th percentile income\n",
    "\n",
    "df21_income_clean = pd.to_numeric(df_21['p344p'], errors='coerce').dropna()\n",
    "df21_percentiles = df21_income_clean.quantile([0.1, 0.9])\n",
    "    \n",
    "print(\"2021 Income Percentile Grouping - Overall\")\n",
    "print(f\"10th percentile: {df21_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df21_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "505e7c90-6287-43a9-97fd-b2a2e52f2ea0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 Income Percentile Grouping - wfh\n",
      "10th percentile: 630.11\n",
      "90th percentile: 2,405.92\n"
     ]
    }
   ],
   "source": [
    "# calculate 2021's 10th and 90th percentile income for WFH group\n",
    "\n",
    "df21_wfh_income_clean = pd.to_numeric(df_21['p344p-wfh'], errors='coerce').dropna()\n",
    "df21_wfh_percentiles = df21_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2021 Income Percentile Grouping - wfh\")\n",
    "print(f\"10th percentile: {df21_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df21_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c56271a1-a868-42d5-b1fa-2354146760b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 Income Percentile Grouping - non_wfh\n",
      "10th percentile: 392.51\n",
      "90th percentile: 1,640.65\n"
     ]
    }
   ],
   "source": [
    "# calculate 2021's 10th and 90th percentile income for non_wfh group\n",
    "\n",
    "df21_non_wfh_income_clean = pd.to_numeric(df_21['p344p-non_wfh'], errors='coerce').dropna()\n",
    "df21_non_wfh_percentiles = df21_non_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2021 Income Percentile Grouping - non_wfh\")\n",
    "print(f\"10th percentile: {df21_non_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df21_non_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cab4ff09-cdca-44da-b468-2a787c512d6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 Income Percentile Grouping - Overall\n",
      "10th percentile: 489.10\n",
      "90th percentile: 2,153.03\n"
     ]
    }
   ],
   "source": [
    "# calculate 2022's 10th and 90th percentile income\n",
    "\n",
    "df22_income_clean = pd.to_numeric(df_22['p344p'], errors='coerce').dropna()\n",
    "df22_percentiles = df22_income_clean.quantile([0.1, 0.9])\n",
    "    \n",
    "print(\"2022 Income Percentile Grouping - Overall\")\n",
    "print(f\"10th percentile: {df22_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df22_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59514eef-377a-430c-9354-0b504b9ea5ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 Income Percentile Grouping - wfh\n",
      "10th percentile: 630.17\n",
      "90th percentile: 2,519.05\n"
     ]
    }
   ],
   "source": [
    "# calculate 2022's 10th and 90th percentile income for WFH group\n",
    "\n",
    "df22_wfh_income_clean = pd.to_numeric(df_22['p344p-wfh'], errors='coerce').dropna()\n",
    "df22_wfh_percentiles = df22_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2022 Income Percentile Grouping - wfh\")\n",
    "print(f\"10th percentile: {df22_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df22_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f29a3df-dd68-4f6c-b996-e0332618423e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 Income Percentile Grouping - non_wfh\n",
      "10th percentile: 448.88\n",
      "90th percentile: 1,844.88\n"
     ]
    }
   ],
   "source": [
    "# calculate 2022's 10th and 90th percentile income for non_wfh group\n",
    "\n",
    "df22_non_wfh_income_clean = pd.to_numeric(df_22['p344p-non_wfh'], errors='coerce').dropna()\n",
    "df22_non_wfh_percentiles = df22_non_wfh_income_clean.quantile([0.1, 0.9])\n",
    "\n",
    "print(\"2022 Income Percentile Grouping - non_wfh\")\n",
    "print(f\"10th percentile: {df22_non_wfh_percentiles[0.1]:,.2f}\")\n",
    "print(f\"90th percentile: {df22_non_wfh_percentiles[0.9]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cce85eda-a562-491b-b30e-56c200ba09e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/10 ratio - income - Overall\n",
      "90/10 ratio for 2016: 4.49\n",
      "90/10 ratio for 2017: 4.81\n",
      "90/10 ratio for 2018: 4.60\n",
      "90/10 ratio for 2019: 4.37\n",
      "90/10 ratio for 2020: 4.42\n",
      "90/10 ratio for 2021: 4.49\n",
      "90/10 ratio for 2022: 4.40\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 90/10 ratio for each year - overall\n",
    "\n",
    "df16_ratio_income = (df16_percentiles[0.9]/df16_percentiles[0.1]).round(2)\n",
    "df17_ratio_income = (df17_percentiles[0.9]/df17_percentiles[0.1]).round(2)\n",
    "df18_ratio_income = (df18_percentiles[0.9]/df18_percentiles[0.1]).round(2)\n",
    "df19_ratio_income = (df19_percentiles[0.9]/df19_percentiles[0.1]).round(2)\n",
    "df20_ratio_income = (df20_percentiles[0.9]/df20_percentiles[0.1]).round(2)\n",
    "df21_ratio_income = (df21_percentiles[0.9]/df21_percentiles[0.1]).round(2)\n",
    "df22_ratio_income = (df22_percentiles[0.9]/df22_percentiles[0.1]).round(2)\n",
    "\n",
    "print(\"90/10 ratio - income - Overall\")\n",
    "print(f'90/10 ratio for 2016: {df16_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2017: {df17_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2018: {df18_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2019: {df19_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2020: {df20_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2021: {df21_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2022: {df22_ratio_income:,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dd8dc26-d119-4182-86ac-1da9ce324be1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/10 ratio - income - WFH \n",
      "\n",
      "90/10 ratio for 2016: 4.00\n",
      "90/10 ratio for 2017: 4.40\n",
      "90/10 ratio for 2018: 3.60\n",
      "90/10 ratio for 2019: 3.95\n",
      "90/10 ratio for 2020: 4.01\n",
      "90/10 ratio for 2021: 3.82\n",
      "90/10 ratio for 2022: 4.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 90/10 ratio for each year - WFH group\n",
    "\n",
    "df16_wfh_ratio_income = (df16_wfh_percentiles[0.9]/df16_wfh_percentiles[0.1]).round(2)\n",
    "df17_wfh_ratio_income = (df17_wfh_percentiles[0.9]/df17_wfh_percentiles[0.1]).round(2)\n",
    "df18_wfh_ratio_income = (df18_wfh_percentiles[0.9]/df18_wfh_percentiles[0.1]).round(2)\n",
    "df19_wfh_ratio_income = (df19_wfh_percentiles[0.9]/df19_wfh_percentiles[0.1]).round(2)\n",
    "df20_wfh_ratio_income = (df20_wfh_percentiles[0.9]/df20_wfh_percentiles[0.1]).round(2)\n",
    "df21_wfh_ratio_income = (df21_wfh_percentiles[0.9]/df21_wfh_percentiles[0.1]).round(2)\n",
    "df22_wfh_ratio_income = (df22_wfh_percentiles[0.9]/df22_wfh_percentiles[0.1]).round(2)\n",
    "\n",
    "print(\"90/10 ratio - income - WFH \\n\")\n",
    "print(f'90/10 ratio for 2016: {df16_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2017: {df17_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2018: {df18_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2019: {df19_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2020: {df20_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2021: {df21_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2022: {df22_wfh_ratio_income:,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53b53e85-770d-4a7b-997d-59e8107def3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/10 ratio - income - non_WFH \n",
      "\n",
      "90/10 ratio for 2016: 4.18\n",
      "90/10 ratio for 2017: 3.81\n",
      "90/10 ratio for 2018: 3.65\n",
      "90/10 ratio for 2019: 3.59\n",
      "90/10 ratio for 2020: 4.22\n",
      "90/10 ratio for 2021: 4.18\n",
      "90/10 ratio for 2022: 4.11\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 90/10 ratio for each year - non_WFH group\n",
    "\n",
    "df16_non_wfh_ratio_income = (df16_non_wfh_percentiles[0.9]/df16_non_wfh_percentiles[0.1]).round(2)\n",
    "df17_non_wfh_ratio_income = (df17_non_wfh_percentiles[0.9]/df17_non_wfh_percentiles[0.1]).round(2)\n",
    "df18_non_wfh_ratio_income = (df18_non_wfh_percentiles[0.9]/df18_non_wfh_percentiles[0.1]).round(2)\n",
    "df19_non_wfh_ratio_income = (df19_non_wfh_percentiles[0.9]/df19_non_wfh_percentiles[0.1]).round(2)\n",
    "df20_non_wfh_ratio_income = (df20_non_wfh_percentiles[0.9]/df20_non_wfh_percentiles[0.1]).round(2)\n",
    "df21_non_wfh_ratio_income = (df21_non_wfh_percentiles[0.9]/df21_non_wfh_percentiles[0.1]).round(2)\n",
    "df22_non_wfh_ratio_income = (df22_non_wfh_percentiles[0.9]/df22_non_wfh_percentiles[0.1]).round(2)\n",
    "\n",
    "print(\"90/10 ratio - income - non_WFH \\n\")\n",
    "print(f'90/10 ratio for 2016: {df16_non_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2017: {df17_non_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2018: {df18_non_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2019: {df19_non_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2020: {df20_non_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2021: {df21_non_wfh_ratio_income:,.2f}')\n",
    "print(f'90/10 ratio for 2022: {df22_non_wfh_ratio_income:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30258a-ed67-435f-b882-05b0843dd814",
   "metadata": {},
   "source": [
    "Calculate the log variance of income for secondary measure of income inequality\n",
    "Steps:\n",
    "- log transform income of each observation in df\n",
    "- calculate the mean of the log transformed income\n",
    "- calculate log variance (I) = mean((log(Ii) - mean (log(I))^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eff24dae-09b5-49c2-b3df-e50d6f7bdf6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_log_variance_income(df, income_column='p344p'):\n",
    "    \"\"\"\n",
    "    Calculate log variance of income from UK LCFS survey data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing LCFS survey data\n",
    "    income_column (str): Name of the income column\n",
    "    weights_column (str): Name of the survey weights column (optional)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing log variance statistics\n",
    "    \"\"\"\n",
    "    # Clean the income data\n",
    "    income_data = df[income_column].copy()\n",
    "    # Remove missing values and non-positive values (log requires positive values)\n",
    "    valid_mask = (income_data.notna()) & (income_data > 0)\n",
    "    clean_income = income_data[valid_mask]\n",
    "    # Calculate log income\n",
    "    log_income = np.log(clean_income)\n",
    "    # Standard variance calculation\n",
    "    mean_log_income = np.mean(log_income)\n",
    "    log_var = np.mean(np.square(log_income - mean_log_income))\n",
    "    \n",
    "        \n",
    "    # Additional statistics\n",
    "    std_log_income = np.sqrt(log_var)\n",
    "        \n",
    "    # Back-transform for interpretation\n",
    "    geometric_mean = np.exp(mean_log_income)\n",
    "    \n",
    "    # Calculate coefficient of variation in log space\n",
    "    cv_log = std_log_income / mean_log_income if mean_log_income != 0 else np.nan\n",
    "    \n",
    "    results = {\n",
    "        'log_variance': log_var,\n",
    "        'log_std_dev': std_log_income,\n",
    "        'mean_log_income': mean_log_income,\n",
    "        'geometric_mean_income': geometric_mean,\n",
    "        'cv_log_space': cv_log,\n",
    "        'sample_size': len(clean_income),\n",
    "        'original_mean_income': np.mean(clean_income),\n",
    "        'original_median_income': np.median(clean_income),\n",
    "        'log_income_data': log_income.values,\n",
    "        'original_income_data': clean_income.values,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_log_variance_summary(results):\n",
    "    \"\"\"Print formatted summary of log variance analysis.\"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    print(\"LOG VARIANCE OF INCOME ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Sample size: {results['sample_size']:,}\")\n",
    "    print(\"\\nLOG INCOME STATISTICS:\")\n",
    "    print(f\"  Log variance: {results['log_variance']:.4f}\")\n",
    "    print(f\"  Log standard deviation: {results['log_std_dev']:.4f}\")\n",
    "    print(f\"  Mean log income: {results['mean_log_income']:.4f}\")\n",
    "    print(f\"  CV in log space: {results['cv_log_space']:.4f}\")\n",
    "    print(\"\\nORIGINAL INCOME STATISTICS:\")\n",
    "    print(f\"  Arithmetic mean: £{results['original_mean_income']:,.2f}\")\n",
    "    print(f\"  Geometric mean: £{results['geometric_mean_income']:,.2f}\")\n",
    "    print(f\"  Median: £{results['original_median_income']:,.2f}\")\n",
    "    \n",
    "    print(\"\\nINTERPRETation:\")\n",
    "    print(f\"  Higher log variance ({results['log_variance']:.4f}) indicates greater income inequality\")\n",
    "    print(f\"  The geometric mean (£{results['geometric_mean_income']:,.2f}) is typically lower than arithmetic mean\")\n",
    "    print(f\"  Log variance is commonly used to measure income inequality in economics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4e7768c-2778-47b2-be21-8082b2596398",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the log variance of income - overall\n",
    "log_var_16 = calculate_log_variance_income(df_16)\n",
    "log_var_17 = calculate_log_variance_income(df_17)\n",
    "log_var_18 = calculate_log_variance_income(df_18)\n",
    "log_var_19 = calculate_log_variance_income(df_19)\n",
    "log_var_20 = calculate_log_variance_income(df_20)\n",
    "log_var_21 = calculate_log_variance_income(df_21)\n",
    "log_var_22 = calculate_log_variance_income(df_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd4c6caf-ca89-4f04-83fb-736b5437a016",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log variance summary for 2016 - overall\n",
      "LOG VARIANCE OF INCOME ANALYSIS\n",
      "==================================================\n",
      "Sample size: 2,556\n",
      "\n",
      "LOG INCOME STATISTICS:\n",
      "  Log variance: 0.3297\n",
      "  Log standard deviation: 0.5742\n",
      "  Mean log income: 6.7654\n",
      "  CV in log space: 0.0849\n",
      "\n",
      "ORIGINAL INCOME STATISTICS:\n",
      "  Arithmetic mean: £1,006.28\n",
      "  Geometric mean: £867.32\n",
      "  Median: £892.37\n",
      "\n",
      "INTERPRETation:\n",
      "  Higher log variance (0.3297) indicates greater income inequality\n",
      "  The geometric mean (£867.32) is typically lower than arithmetic mean\n",
      "  Log variance is commonly used to measure income inequality in economics\n"
     ]
    }
   ],
   "source": [
    "# if want to see summary of result of Log Variance Income Analysis\n",
    "print(\"Log variance summary for 2016 - overall\")\n",
    "print_log_variance_summary(log_var_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3364218f-5886-4007-8b81-7aca83f37785",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Variance of Income - Overall \n",
      "\n",
      " Log Variance of Income - 2016: 0.3297\n",
      " Log Variance of Income - 2017: 0.3651\n",
      " Log Variance of Income - 2018: 0.3589\n",
      " Log Variance of Income - 2019: 0.3274\n",
      " Log Variance of Income - 2020: 0.3200\n",
      " Log Variance of Income - 2021: 0.3172\n",
      " Log Variance of Income - 2022: 0.3210\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Variance of Income - Overall \\n\")\n",
    "print(f\" Log Variance of Income - 2016: {log_var_16['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2017: {log_var_17['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2018: {log_var_18['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2019: {log_var_19['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2020: {log_var_20['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2021: {log_var_21['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2022: {log_var_22['log_variance']:,.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83db7c41-e3e7-45bf-9b0d-e734343437ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the log variance of income - WFH group\n",
    "log_var_wfh_16 = calculate_log_variance_income(df_16,'p344p-wfh')\n",
    "log_var_wfh_17 = calculate_log_variance_income(df_17,'p344p-wfh')\n",
    "log_var_wfh_18 = calculate_log_variance_income(df_18,'p344p-wfh')\n",
    "log_var_wfh_19 = calculate_log_variance_income(df_19,'p344p-wfh')\n",
    "log_var_wfh_20 = calculate_log_variance_income(df_20,'p344p-wfh')\n",
    "log_var_wfh_21 = calculate_log_variance_income(df_21,'p344p-wfh')\n",
    "log_var_wfh_22 = calculate_log_variance_income(df_22,'p344p-wfh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "168c62cc-c7d9-4f34-92ab-bbca49192cab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log variance summary for 2016 - wfh\n",
      "LOG VARIANCE OF INCOME ANALYSIS\n",
      "==================================================\n",
      "Sample size: 1,347\n",
      "\n",
      "LOG INCOME STATISTICS:\n",
      "  Log variance: 0.2791\n",
      "  Log standard deviation: 0.5283\n",
      "  Mean log income: 6.9297\n",
      "  CV in log space: 0.0762\n",
      "\n",
      "ORIGINAL INCOME STATISTICS:\n",
      "  Arithmetic mean: £1,156.41\n",
      "  Geometric mean: £1,022.17\n",
      "  Median: £1,080.67\n",
      "\n",
      "INTERPRETation:\n",
      "  Higher log variance (0.2791) indicates greater income inequality\n",
      "  The geometric mean (£1,022.17) is typically lower than arithmetic mean\n",
      "  Log variance is commonly used to measure income inequality in economics\n"
     ]
    }
   ],
   "source": [
    "# if want to see summary of result of Log Variance Income Analysis - wfh\n",
    "print(\"Log variance summary for 2016 - wfh\")\n",
    "print_log_variance_summary(log_var_wfh_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f272337-e5c5-42e5-be97-5fb25a619a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Variance of Income - wfh \n",
      "\n",
      " Log Variance of Income - 2016: 0.2791\n",
      " Log Variance of Income - 2017: 0.2921\n",
      " Log Variance of Income - 2018: 0.2358\n",
      " Log Variance of Income - 2019: 0.2510\n",
      " Log Variance of Income - 2020: 0.2548\n",
      " Log Variance of Income - 2021: 0.2391\n",
      " Log Variance of Income - 2022: 0.2548\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Variance of Income - wfh \\n\")\n",
    "print(f\" Log Variance of Income - 2016: {log_var_wfh_16['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2017: {log_var_wfh_17['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2018: {log_var_wfh_18['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2019: {log_var_wfh_19['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2020: {log_var_wfh_20['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2021: {log_var_wfh_21['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2022: {log_var_wfh_22['log_variance']:,.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee3153eb-4925-423f-8cb1-cb55cabd1e49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the log variance of income - non-WFH group\n",
    "log_var_non_wfh_16 = calculate_log_variance_income(df_16,'p344p-non_wfh')\n",
    "log_var_non_wfh_17 = calculate_log_variance_income(df_17,'p344p-non_wfh')\n",
    "log_var_non_wfh_18 = calculate_log_variance_income(df_18,'p344p-non_wfh')\n",
    "log_var_non_wfh_19 = calculate_log_variance_income(df_19,'p344p-non_wfh')\n",
    "log_var_non_wfh_20 = calculate_log_variance_income(df_20,'p344p-non_wfh')\n",
    "log_var_non_wfh_21 = calculate_log_variance_income(df_21,'p344p-non_wfh')\n",
    "log_var_non_wfh_22 = calculate_log_variance_income(df_22,'p344p-non_wfh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "829b78a6-88a3-4276-a5b2-eaf0c252c97e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log variance summary for 2016 - non_wfh\n",
      "LOG VARIANCE OF INCOME ANALYSIS\n",
      "==================================================\n",
      "Sample size: 1,209\n",
      "\n",
      "LOG INCOME STATISTICS:\n",
      "  Log variance: 0.3224\n",
      "  Log standard deviation: 0.5678\n",
      "  Mean log income: 6.5824\n",
      "  CV in log space: 0.0863\n",
      "\n",
      "ORIGINAL INCOME STATISTICS:\n",
      "  Arithmetic mean: £839.01\n",
      "  Geometric mean: £722.26\n",
      "  Median: £736.12\n",
      "\n",
      "INTERPRETation:\n",
      "  Higher log variance (0.3224) indicates greater income inequality\n",
      "  The geometric mean (£722.26) is typically lower than arithmetic mean\n",
      "  Log variance is commonly used to measure income inequality in economics\n"
     ]
    }
   ],
   "source": [
    "# if want to see summary of result of Log Variance Income Analysis - non_wfh\n",
    "print(\"Log variance summary for 2016 - non_wfh\")\n",
    "print_log_variance_summary(log_var_non_wfh_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7b2bc16-fdfb-4256-88ed-46a9a611755f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Variance of Income - non_wfh \n",
      "\n",
      " Log Variance of Income - 2016: 0.3224\n",
      " Log Variance of Income - 2017: 0.3364\n",
      " Log Variance of Income - 2018: 0.3223\n",
      " Log Variance of Income - 2019: 0.2696\n",
      " Log Variance of Income - 2020: 0.3334\n",
      " Log Variance of Income - 2021: 0.3301\n",
      " Log Variance of Income - 2022: 0.3232\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Variance of Income - non_wfh \\n\")\n",
    "print(f\" Log Variance of Income - 2016: {log_var_non_wfh_16['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2017: {log_var_non_wfh_17['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2018: {log_var_non_wfh_18['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2019: {log_var_non_wfh_19['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2020: {log_var_non_wfh_20['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2021: {log_var_non_wfh_21['log_variance']:,.4f}\")\n",
    "print(f\" Log Variance of Income - 2022: {log_var_non_wfh_22['log_variance']:,.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caba274-97c3-48db-be39-280d26569269",
   "metadata": {},
   "source": [
    "Calculating Aguiar and Bills' measeure of consumption; ratio of luxury/necessity spending. Aguiar and Bills categorised luxury spending as entertainment or recreation expense and necessity spending as food at home expense.\n",
    "\n",
    "Aggregating the data and joining them into one dataframe.\n",
    "In this section, we calculate the consumption and then combine them into one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784c420-a5fd-4853-aa29-1618f5b8b16c",
   "metadata": {},
   "source": [
    "Calculate the 90/10th percentile ratio as measure of consumption inequality, grouped by income, using Aguiar and Bills measure of consumption\n",
    "Steps:\n",
    "- determine the 90th and 10th percentile of income in each year\n",
    "- calculate the luxury/necessity ratio for each observation in each year\n",
    "- determine mean ratio of those below the 10th percentile of income and above the 90th percentile of income (top 10% earners) \n",
    "- calculate the 90/10 ratio by dividing the mean of consumption ratio of the bottom 10th and top 10th percentile\n",
    "- format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d2c11e4-953b-4818-8e47-436aa3035609",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Create function to calculate the average derived consumption for the 10th and 90th percentile income group\n",
    "\n",
    "def calculate_consumption_income_group(df, consumption_column='derived_consumption', income_column='p344p'):\n",
    "    \"\"\"\n",
    "    Calculate average consumption of respondents from the bottom 10th and top 10th income group from UK LCFS survey data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing LCFS survey data\n",
    "    consumption_column (str): Name of the consumption column\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # Gather clean income data\n",
    "    income_clean = pd.to_numeric(df[income_column], errors='coerce').dropna()\n",
    "\n",
    "    # Calculate 10th and 90th percentile income\n",
    "    income_percentiles = income_clean.quantile([0.1, 0.9])\n",
    "    \n",
    "    # Subset data to each percentiles\n",
    "    consumption_10th_group = df[consumption_column][df[income_column] <= income_percentiles[0.1]]\n",
    "    consumption_90th_group = df[consumption_column][df[income_column] >= income_percentiles[0.9]]\n",
    "\n",
    "    # Clean consumption data\n",
    "    consumption_10th_clean = pd.to_numeric(consumption_10th_group, errors='coerce').dropna()\n",
    "    consumption_90th_clean = pd.to_numeric(consumption_90th_group, errors='coerce').dropna()\n",
    "\n",
    "    # Average consumption of group\n",
    "    consumption_10th = consumption_10th_clean.mean()\n",
    "    consumption_90th = consumption_90th_clean.mean()\n",
    "    \n",
    "    results = {\n",
    "        '10th_perc': income_percentiles[0.1],\n",
    "        '90th_perc': income_percentiles[0.9],\n",
    "        'sample_size': len(income_clean),\n",
    "        '10th_perc_mean_consumption': consumption_10th,\n",
    "        '90th_perc_mean_consumption': consumption_90th,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_consumption_group_summary(results):\n",
    "    \"\"\"Print formatted summary of consumption by income percentile analysis.\"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    print(\"SUMMARY OF CONSUMPTION BY INCOME PERCENTILE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Sample size: {results['sample_size']:,}\")\n",
    "    print(\"10TH AND 90TH PERCENTILE OF INCOME:\")\n",
    "    print(f\"  10th percentile of Income: £{results['10th_perc']:.2f}\")\n",
    "    print(f\"  10th percentile of Income: £{results['90th_perc']:.2f}\")\n",
    "    print(\"CONSUMPTION GROUPED BY 10TH AND 90TH PERCENTILE OF INCOME:\")\n",
    "    print(f\"  Average consumption of the 10th percentile: {results['10th_perc_mean_consumption']:,.4f}\")\n",
    "    print(f\"  Average consumption of the 90th percentile: {results['90th_perc_mean_consumption']:,.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d84c3895-c441-4fac-8517-c3a57c712825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derived consumption based on the 90th & 10th percentile income group\n",
    "\n",
    "df_16_consumption_analysis = calculate_consumption_income_group(df_16)\n",
    "df_17_consumption_analysis = calculate_consumption_income_group(df_17)\n",
    "df_18_consumption_analysis = calculate_consumption_income_group(df_18)\n",
    "df_19_consumption_analysis = calculate_consumption_income_group(df_19)\n",
    "df_20_consumption_analysis = calculate_consumption_income_group(df_20)\n",
    "df_21_consumption_analysis = calculate_consumption_income_group(df_21)\n",
    "df_22_consumption_analysis = calculate_consumption_income_group(df_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4a22f35-b8a8-44de-aacb-09d37dcf84e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 \n",
      "==================================================\n",
      "SUMMARY OF CONSUMPTION BY INCOME PERCENTILE\n",
      "==================================================\n",
      "Sample size: 2,556\n",
      "10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  10th percentile of Income: £407.08\n",
      "  10th percentile of Income: £1827.81\n",
      "CONSUMPTION GROUPED BY 10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  Average consumption of the 10th percentile: 102.6553\n",
      "  Average consumption of the 90th percentile: 337.3174\n",
      "\n",
      " 2017 \n",
      "==================================================\n",
      "SUMMARY OF CONSUMPTION BY INCOME PERCENTILE\n",
      "==================================================\n",
      "Sample size: 2,852\n",
      "10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  10th percentile of Income: £402.10\n",
      "  10th percentile of Income: £1933.38\n",
      "CONSUMPTION GROUPED BY 10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  Average consumption of the 10th percentile: 109.8997\n",
      "  Average consumption of the 90th percentile: 304.0690\n",
      "\n",
      " 2018 \n",
      "==================================================\n",
      "SUMMARY OF CONSUMPTION BY INCOME PERCENTILE\n",
      "==================================================\n",
      "Sample size: 2,893\n",
      "10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  10th percentile of Income: £405.18\n",
      "  10th percentile of Income: £1863.80\n",
      "CONSUMPTION GROUPED BY 10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  Average consumption of the 10th percentile: 111.9371\n",
      "  Average consumption of the 90th percentile: 238.5692\n",
      "\n",
      " 2019 \n",
      "==================================================\n",
      "SUMMARY OF CONSUMPTION BY INCOME PERCENTILE\n",
      "==================================================\n",
      "Sample size: 2,794\n",
      "10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  10th percentile of Income: £440.57\n",
      "  10th percentile of Income: £1925.42\n",
      "CONSUMPTION GROUPED BY 10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  Average consumption of the 10th percentile: 113.1387\n",
      "  Average consumption of the 90th percentile: 234.0245\n",
      "\n",
      " 2020 \n",
      "==================================================\n",
      "SUMMARY OF CONSUMPTION BY INCOME PERCENTILE\n",
      "==================================================\n",
      "Sample size: 2,884\n",
      "10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  10th percentile of Income: £452.46\n",
      "  10th percentile of Income: £1998.58\n",
      "CONSUMPTION GROUPED BY 10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  Average consumption of the 10th percentile: 61.3694\n",
      "  Average consumption of the 90th percentile: 120.5060\n",
      "\n",
      " 2021 \n",
      "==================================================\n",
      "SUMMARY OF CONSUMPTION BY INCOME PERCENTILE\n",
      "==================================================\n",
      "Sample size: 2,984\n",
      "10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  10th percentile of Income: £479.76\n",
      "  10th percentile of Income: £2155.85\n",
      "CONSUMPTION GROUPED BY 10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  Average consumption of the 10th percentile: 87.3419\n",
      "  Average consumption of the 90th percentile: 200.0528\n",
      "\n",
      " 2022 \n",
      "==================================================\n",
      "SUMMARY OF CONSUMPTION BY INCOME PERCENTILE\n",
      "==================================================\n",
      "Sample size: 2,390\n",
      "10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  10th percentile of Income: £489.10\n",
      "  10th percentile of Income: £2153.03\n",
      "CONSUMPTION GROUPED BY 10TH AND 90TH PERCENTILE OF INCOME:\n",
      "  Average consumption of the 10th percentile: 156.5252\n",
      "  Average consumption of the 90th percentile: 216.0436\n"
     ]
    }
   ],
   "source": [
    "# Summary of the derived consumption analysis based on the 90th & 10th percentile income group\n",
    "\n",
    "print('2016 \\n' + '='*50)\n",
    "print_consumption_group_summary(df_16_consumption_analysis)\n",
    "print('\\n 2017 \\n' + '='*50)\n",
    "print_consumption_group_summary(df_17_consumption_analysis)\n",
    "print('\\n 2018 \\n' + '='*50)\n",
    "print_consumption_group_summary(df_18_consumption_analysis)\n",
    "print('\\n 2019 \\n' + '='*50)\n",
    "print_consumption_group_summary(df_19_consumption_analysis)\n",
    "print('\\n 2020 \\n' + '='*50)\n",
    "print_consumption_group_summary(df_20_consumption_analysis)\n",
    "print('\\n 2021 \\n' + '='*50)\n",
    "print_consumption_group_summary(df_21_consumption_analysis)\n",
    "print('\\n 2022 \\n' + '='*50)\n",
    "print_consumption_group_summary(df_22_consumption_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cddb703d-7126-4549-b3e8-a11e3eb03f6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/10 consumption ratio for 2016: 0.3000\n",
      "90/10 consumption ratio for 2017: 0.3600\n",
      "90/10 consumption ratio for 2018: 0.4700\n",
      "90/10 consumption ratio for 2019: 0.4800\n",
      "90/10 consumption ratio for 2020: 0.5100\n",
      "90/10 consumption ratio for 2021: 0.4400\n",
      "90/10 consumption ratio for 2022: 0.7200\n"
     ]
    }
   ],
   "source": [
    "# Calculate the consumption 90/10 ratio for each year\n",
    "\n",
    "df16_ratio_consumption = (df_16_consumption_analysis['10th_perc_mean_consumption']/df_16_consumption_analysis['90th_perc_mean_consumption']).round(2)\n",
    "df17_ratio_consumption = (df_17_consumption_analysis['10th_perc_mean_consumption']/df_17_consumption_analysis['90th_perc_mean_consumption']).round(2)\n",
    "df18_ratio_consumption = (df_18_consumption_analysis['10th_perc_mean_consumption']/df_18_consumption_analysis['90th_perc_mean_consumption']).round(2)\n",
    "df19_ratio_consumption = (df_19_consumption_analysis['10th_perc_mean_consumption']/df_19_consumption_analysis['90th_perc_mean_consumption']).round(2)\n",
    "df20_ratio_consumption = (df_20_consumption_analysis['10th_perc_mean_consumption']/df_20_consumption_analysis['90th_perc_mean_consumption']).round(2)\n",
    "df21_ratio_consumption = (df_21_consumption_analysis['10th_perc_mean_consumption']/df_21_consumption_analysis['90th_perc_mean_consumption']).round(2)\n",
    "df22_ratio_consumption = (df_22_consumption_analysis['10th_perc_mean_consumption']/df_22_consumption_analysis['90th_perc_mean_consumption']).round(2)\n",
    "\n",
    "print(f'90/10 consumption ratio for 2016: {df16_ratio_consumption:,.4f}')\n",
    "print(f'90/10 consumption ratio for 2017: {df17_ratio_consumption:,.4f}')\n",
    "print(f'90/10 consumption ratio for 2018: {df18_ratio_consumption:,.4f}')\n",
    "print(f'90/10 consumption ratio for 2019: {df19_ratio_consumption:,.4f}')\n",
    "print(f'90/10 consumption ratio for 2020: {df20_ratio_consumption:,.4f}')\n",
    "print(f'90/10 consumption ratio for 2021: {df21_ratio_consumption:,.4f}')\n",
    "print(f'90/10 consumption ratio for 2022: {df22_ratio_consumption:,.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0c7a3bba-46ac-4505-917b-ed49bb04ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create function to calculate the average income level for each year and group\n",
    "\n",
    "def calculate_income_level(df, income_overall='p344p', income_wfh='p344p-wfh',\n",
    "                                      income_non_wfh='p344p-non_wfh'):\n",
    "    \"\"\"\n",
    "    Calculate average income of respondents from all group from UK LCFS survey data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing LCFS survey data\n",
    "    income_column (str): Name of the income column\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # Average income of group\n",
    "    mean_income_overall = df[income_overall].mean()\n",
    "    mean_income_wfh = df[income_wfh].mean()\n",
    "    mean_income_non_wfh = df[income_non_wfh].mean()\n",
    "    \n",
    "    results = {\n",
    "        'mean_income_overall': mean_income_overall,\n",
    "        'mean_income_wfh': mean_income_wfh,\n",
    "        'mean_income_non_wfh': mean_income_non_wfh,\n",
    "        'sample_size': len(df[income_overall])        \n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_income_group_summary(results, year='2016'):\n",
    "    \"\"\"Print formatted summary of income.\"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    print(\"SUMMARY OF income BY GROUP - \" + year)\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Sample size: {results['sample_size']:,}\")\n",
    "    print(\"AVERAGE income BY GROUP:\")\n",
    "    print(f\"  Average income - Overall: {results['mean_income_overall']:.2f}\")\n",
    "    print(f\"  Average income - WFH: {results['mean_income_wfh']:.2f}\")\n",
    "    print(f\"  Average income - non-WFH: {results['mean_income_non_wfh']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a01dac89-a4f2-4e55-bb3a-0b0553ecf8f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY OF income BY GROUP - 2016\n",
      "==================================================\n",
      "Sample size: 2,556\n",
      "AVERAGE income BY GROUP:\n",
      "  Average income - Overall: 1006.28\n",
      "  Average income - WFH: 1156.41\n",
      "  Average income - non-WFH: 839.01\n"
     ]
    }
   ],
   "source": [
    "income_result_2016 = calculate_income_level(df_16)\n",
    "print_income_group_summary(income_result_2016, year='2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8390dadc-854b-4f4b-b21b-2f5d24de09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_result_2017 = calculate_income_level(df_17)\n",
    "income_result_2018 = calculate_income_level(df_18)\n",
    "income_result_2019 = calculate_income_level(df_19)\n",
    "income_result_2020 = calculate_income_level(df_20)\n",
    "income_result_2021 = calculate_income_level(df_21)\n",
    "income_result_2022 = calculate_income_level(df_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "45374d45-f0b2-41c3-a5c9-630914379880",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY OF income BY GROUP - 2016\n",
      "==================================================\n",
      "Sample size: 2,556\n",
      "AVERAGE income BY GROUP:\n",
      "  Average income - Overall: 1006.28\n",
      "  Average income - WFH: 1156.41\n",
      "  Average income - non-WFH: 839.01\n",
      "SUMMARY OF income BY GROUP - 2017\n",
      "==================================================\n",
      "Sample size: 2,852\n",
      "AVERAGE income BY GROUP:\n",
      "  Average income - Overall: 1035.74\n",
      "  Average income - WFH: 1219.20\n",
      "  Average income - non-WFH: 768.54\n",
      "SUMMARY OF income BY GROUP - 2018\n",
      "==================================================\n",
      "Sample size: 2,893\n",
      "AVERAGE income BY GROUP:\n",
      "  Average income - Overall: 1025.44\n",
      "  Average income - WFH: 1268.30\n",
      "  Average income - non-WFH: 734.02\n",
      "SUMMARY OF income BY GROUP - 2019\n",
      "==================================================\n",
      "Sample size: 2,794\n",
      "AVERAGE income BY GROUP:\n",
      "  Average income - Overall: 1079.70\n",
      "  Average income - WFH: 1276.42\n",
      "  Average income - non-WFH: 743.84\n",
      "SUMMARY OF income BY GROUP - 2020\n",
      "==================================================\n",
      "Sample size: 2,884\n",
      "AVERAGE income BY GROUP:\n",
      "  Average income - Overall: 1110.32\n",
      "  Average income - WFH: 1255.44\n",
      "  Average income - non-WFH: 900.76\n",
      "SUMMARY OF income BY GROUP - 2021\n",
      "==================================================\n",
      "Sample size: 2,984\n",
      "AVERAGE income BY GROUP:\n",
      "  Average income - Overall: 1175.94\n",
      "  Average income - WFH: 1349.18\n",
      "  Average income - non-WFH: 936.28\n",
      "SUMMARY OF income BY GROUP - 2022\n",
      "==================================================\n",
      "Sample size: 2,390\n",
      "AVERAGE income BY GROUP:\n",
      "  Average income - Overall: 1191.23\n",
      "  Average income - WFH: 1408.74\n",
      "  Average income - non-WFH: 1057.72\n"
     ]
    }
   ],
   "source": [
    "print_income_group_summary(income_result_2016, year='2016')\n",
    "print_income_group_summary(income_result_2017, year='2017')\n",
    "print_income_group_summary(income_result_2018, year='2018')\n",
    "print_income_group_summary(income_result_2019, year='2019')\n",
    "print_income_group_summary(income_result_2020, year='2020')\n",
    "print_income_group_summary(income_result_2021, year='2021')\n",
    "print_income_group_summary(income_result_2022, year='2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7be91338-d3ce-496e-825b-4d6394e75896",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Create function to calculate the average derived consumption for each year and group\n",
    "\n",
    "def calculate_consumption_income_group(df, consumption_overall='derived_consumption', consumption_wfh='derived_consumption-wfh',\n",
    "                                      consumption_non_wfh='derived_consumption-non_wfh'):\n",
    "    \"\"\"\n",
    "    Calculate average consumption of respondents from the bottom 10th and top 10th income group from UK LCFS survey data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing LCFS survey data\n",
    "    consumption_column (str): Name of the consumption column\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # Average consumption of group\n",
    "    mean_consumption_overall = df[consumption_overall].mean()\n",
    "    mean_consumption_wfh = df[consumption_wfh].mean()\n",
    "    mean_consumption_non_wfh = df[consumption_non_wfh].mean()\n",
    "    \n",
    "    results = {\n",
    "        'mean_consumption_overall': mean_consumption_overall,\n",
    "        'mean_consumption_wfh': mean_consumption_wfh,\n",
    "        'mean_consumption_non_wfh': mean_consumption_non_wfh,\n",
    "        'sample_size': len(df[consumption_overall])        \n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_consumption_group_summary(results, year='2016'):\n",
    "    \"\"\"Print formatted summary of consumption by income percentile analysis.\"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    print(\"SUMMARY OF CONSUMPTION BY GROUP - \" + year)\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Sample size: {results['sample_size']:,}\")\n",
    "    print(\"AVERAGE CONSUMPTION BY GROUP:\")\n",
    "    print(f\"  Average consumption - Overall: {results['mean_consumption_overall']:.2f}\")\n",
    "    print(f\"  Average consumption - WFH: {results['mean_consumption_wfh']:.2f}\")\n",
    "    print(f\"  Average consumption - non-WFH: {results['mean_consumption_non_wfh']:.2f}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7521f794-7737-481d-b37d-ad92c145e5e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY OF CONSUMPTION BY GROUP - 2016\n",
      "==================================================\n",
      "Sample size: 2,556\n",
      "AVERAGE CONSUMPTION BY GROUP:\n",
      "  Average consumption - Overall: 169.67\n",
      "  Average consumption - WFH: 178.94\n",
      "  Average consumption - non-WFH: 159.33\n"
     ]
    }
   ],
   "source": [
    "consumption_result_2016 = calculate_consumption_income_group(df_16)\n",
    "print_consumption_group_summary(consumption_result_2016, year='2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b09fbab-02b0-4b5b-a38c-7534cc349d0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "consumption_result_2017 = calculate_consumption_income_group(df_17)\n",
    "consumption_result_2018 = calculate_consumption_income_group(df_18)\n",
    "consumption_result_2019 = calculate_consumption_income_group(df_19)\n",
    "consumption_result_2020 = calculate_consumption_income_group(df_20)\n",
    "consumption_result_2021 = calculate_consumption_income_group(df_21)\n",
    "consumption_result_2022 = calculate_consumption_income_group(df_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c825d2-e69d-495a-a581-3c5209b44d46",
   "metadata": {},
   "source": [
    "Calculate the log variance of consumption for secondary measure of income inequality\n",
    "Steps:\n",
    "- log transform consumption ratio of each observation in df\n",
    "- calculate the mean of the log transformed consumption\n",
    "- calculate log variance (C) = mean((log(Ci) - mean (log(C))^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b9d71661-a9f6-43e6-afe6-38a76bfe6621",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_log_variance_consumption(df, consumption_column='derived_consumption'):\n",
    "    \"\"\"\n",
    "    Calculate log variance of consumption from UK LCFS survey data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing LCFS survey data\n",
    "    consumption_column (str): Name of the consumption column\n",
    "    weights_column (str): Name of the survey weights column (optional)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing log variance statistics\n",
    "    \"\"\"\n",
    "    # Clean the consumption data\n",
    "    consumption_data = df[consumption_column].copy()\n",
    "    # Remove missing values and non-positive values (log requires positive values)\n",
    "    valid_mask = (consumption_data.notna()) & (consumption_data > 0)\n",
    "    clean_consumption = consumption_data[valid_mask]\n",
    "#    clean_consumption = consumption_data.fillna(df[consumption_column].agg('mean')).copy()\n",
    "    # Calculate log consumption\n",
    "    log_consumption = np.log(clean_consumption)\n",
    "    # Standard variance calculation\n",
    "    mean_log_consumption = np.mean(log_consumption)\n",
    "    log_var = np.mean(np.square(log_consumption - mean_log_consumption))\n",
    "    \n",
    "    # Additional statistics\n",
    "    std_log_consumption = np.sqrt(log_var)\n",
    "        \n",
    "    # Back-transform for interpretation\n",
    "    geometric_mean = np.exp(mean_log_consumption)\n",
    "    \n",
    "    # Calculate coefficient of variation in log space\n",
    "    cv_log = std_log_consumption / mean_log_consumption if mean_log_consumption != 0 else np.nan\n",
    "    \n",
    "    results = {\n",
    "        'log_variance': log_var,\n",
    "        'log_std_dev': std_log_consumption,\n",
    "        'mean_log_consumption': mean_log_consumption,\n",
    "        'geometric_mean_consumption': geometric_mean,\n",
    "        'cv_log_space': cv_log,\n",
    "        'sample_size': len(clean_consumption),\n",
    "        'original_mean_consumption': np.mean(clean_consumption),\n",
    "        'original_median_consumption': np.median(clean_consumption),\n",
    "        'log_consumption_data': log_consumption.values,\n",
    "        'original_consumption_data': clean_consumption.values,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_log_variance_consumption_summary(results):\n",
    "    \"\"\"Print formatted summary of log variance analysis.\"\"\"\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    print(\"LOG VARIANCE OF consumption ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Sample size: {results['sample_size']:,}\")\n",
    "    print(\"\\nLOG consumption STATISTICS:\")\n",
    "    print(f\"  Log variance: {results['log_variance']:.4f}\")\n",
    "    print(f\"  Log standard deviation: {results['log_std_dev']:.4f}\")\n",
    "    print(f\"  Mean log consumption: {results['mean_log_consumption']:.4f}\")\n",
    "    print(f\"  CV in log space: {results['cv_log_space']:.4f}\")\n",
    "    print(\"\\nORIGINAL consumption STATISTICS:\")\n",
    "    print(f\"  Arithmetic mean: {results['original_mean_consumption']:,.2f}\")\n",
    "    print(f\"  Geometric mean: {results['geometric_mean_consumption']:,.2f}\")\n",
    "    print(f\"  Median: {results['original_median_consumption']:,.2f}\")\n",
    "    \n",
    "    print(\"\\nINTERPRETation:\")\n",
    "    print(f\"  Higher log variance ({results['log_variance']:.4f}) indicates greater consumption inequality\")\n",
    "    print(f\"  Log variance is commonly used to measure consumption inequality in economics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f6b1e51d-5f85-420c-be71-a1474150f26b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the log variance of the derived consumption for each year - overall\n",
    "\n",
    "log_var_consumption_16 = calculate_log_variance_consumption(df_16)\n",
    "log_var_consumption_17 = calculate_log_variance_consumption(df_17)\n",
    "log_var_consumption_18 = calculate_log_variance_consumption(df_18)\n",
    "log_var_consumption_19 = calculate_log_variance_consumption(df_19)\n",
    "log_var_consumption_20 = calculate_log_variance_consumption(df_20)\n",
    "log_var_consumption_21 = calculate_log_variance_consumption(df_21)\n",
    "log_var_consumption_22 = calculate_log_variance_consumption(df_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9a368486-487a-43cb-9cbe-a5a4504695ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG VARIANCE OF consumption ANALYSIS\n",
      "==================================================\n",
      "Sample size: 2,531\n",
      "\n",
      "LOG consumption STATISTICS:\n",
      "  Log variance: 1.3415\n",
      "  Log standard deviation: 1.1582\n",
      "  Mean log consumption: 4.3619\n",
      "  CV in log space: 0.2655\n",
      "\n",
      "ORIGINAL consumption STATISTICS:\n",
      "  Arithmetic mean: 170.41\n",
      "  Geometric mean: 78.41\n",
      "  Median: 75.70\n",
      "\n",
      "INTERPRETation:\n",
      "  Higher log variance (1.3415) indicates greater consumption inequality\n",
      "  Log variance is commonly used to measure consumption inequality in economics\n"
     ]
    }
   ],
   "source": [
    "print_log_variance_consumption_summary(log_var_consumption_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a4defc2a-7301-48fe-934d-ddb3a1467323",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the log variance of the derived consumption for each year - wfh\n",
    "\n",
    "log_var_consumption_wfh_16 = calculate_log_variance_consumption(df_16, 'derived_consumption-wfh')\n",
    "log_var_consumption_wfh_17 = calculate_log_variance_consumption(df_17, 'derived_consumption-wfh')\n",
    "log_var_consumption_wfh_18 = calculate_log_variance_consumption(df_18, 'derived_consumption-wfh')\n",
    "log_var_consumption_wfh_19 = calculate_log_variance_consumption(df_19, 'derived_consumption-wfh')\n",
    "log_var_consumption_wfh_20 = calculate_log_variance_consumption(df_20, 'derived_consumption-wfh')\n",
    "log_var_consumption_wfh_21 = calculate_log_variance_consumption(df_21, 'derived_consumption-wfh')\n",
    "log_var_consumption_wfh_22 = calculate_log_variance_consumption(df_22, 'derived_consumption-wfh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "eb94c6de-864a-48a9-b55d-7b3fa6775a16",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG VARIANCE OF consumption ANALYSIS\n",
      "==================================================\n",
      "Sample size: 1,336\n",
      "\n",
      "LOG consumption STATISTICS:\n",
      "  Log variance: 1.2366\n",
      "  Log standard deviation: 1.1120\n",
      "  Mean log consumption: 4.4349\n",
      "  CV in log space: 0.2507\n",
      "\n",
      "ORIGINAL consumption STATISTICS:\n",
      "  Arithmetic mean: 179.48\n",
      "  Geometric mean: 84.35\n",
      "  Median: 80.59\n",
      "\n",
      "INTERPRETation:\n",
      "  Higher log variance (1.2366) indicates greater consumption inequality\n",
      "  Log variance is commonly used to measure consumption inequality in economics\n"
     ]
    }
   ],
   "source": [
    "print_log_variance_consumption_summary(log_var_consumption_wfh_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3b8104a4-3ad4-400f-bfd1-4836e698e8bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the log variance of the derived consumption for each year - non_wfh\n",
    "\n",
    "log_var_consumption_non_wfh_16 = calculate_log_variance_consumption(df_16, 'derived_consumption-non_wfh')\n",
    "log_var_consumption_non_wfh_17 = calculate_log_variance_consumption(df_17, 'derived_consumption-non_wfh')\n",
    "log_var_consumption_non_wfh_18 = calculate_log_variance_consumption(df_18, 'derived_consumption-non_wfh')\n",
    "log_var_consumption_non_wfh_19 = calculate_log_variance_consumption(df_19, 'derived_consumption-non_wfh')\n",
    "log_var_consumption_non_wfh_20 = calculate_log_variance_consumption(df_20, 'derived_consumption-non_wfh')\n",
    "log_var_consumption_non_wfh_21 = calculate_log_variance_consumption(df_21, 'derived_consumption-non_wfh')\n",
    "log_var_consumption_non_wfh_22 = calculate_log_variance_consumption(df_22, 'derived_consumption-non_wfh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "fc4cc6eb-fdb8-462f-a93f-68c3122ebeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>avg_income_overall</th>\n",
       "      <th>90/10_ratio_income_overall</th>\n",
       "      <th>log_var_income_overall</th>\n",
       "      <th>avg_consumption_overall</th>\n",
       "      <th>log_var_consumption_overall</th>\n",
       "      <th>avg_income_wfh</th>\n",
       "      <th>90/10_ratio_income_wfh</th>\n",
       "      <th>log_var_income_wfh</th>\n",
       "      <th>avg_consumption_wfh</th>\n",
       "      <th>log_var_consumption_wfh</th>\n",
       "      <th>avg_income_non_wfh</th>\n",
       "      <th>90/10_ratio_income_non_wfh</th>\n",
       "      <th>log_var_income_non_wfh</th>\n",
       "      <th>avg_consumption_non_wfh</th>\n",
       "      <th>log_var_consumption_non_wfh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>1006.277011</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.329679</td>\n",
       "      <td>169.668904</td>\n",
       "      <td>1.341467</td>\n",
       "      <td>1156.409845</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.279134</td>\n",
       "      <td>178.944069</td>\n",
       "      <td>1.236577</td>\n",
       "      <td>839.007426</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.322427</td>\n",
       "      <td>159.328869</td>\n",
       "      <td>1.446107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>1035.743265</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.365085</td>\n",
       "      <td>173.455859</td>\n",
       "      <td>1.367597</td>\n",
       "      <td>1219.199715</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.292146</td>\n",
       "      <td>187.468722</td>\n",
       "      <td>1.370819</td>\n",
       "      <td>768.538392</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.336387</td>\n",
       "      <td>153.090015</td>\n",
       "      <td>1.332190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>1025.441813</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.358912</td>\n",
       "      <td>154.643541</td>\n",
       "      <td>1.495767</td>\n",
       "      <td>1268.296132</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.235835</td>\n",
       "      <td>165.145627</td>\n",
       "      <td>1.425106</td>\n",
       "      <td>734.016631</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.322255</td>\n",
       "      <td>141.983051</td>\n",
       "      <td>1.534999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>1079.703580</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.327387</td>\n",
       "      <td>147.320048</td>\n",
       "      <td>1.433349</td>\n",
       "      <td>1276.417647</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.250969</td>\n",
       "      <td>162.646392</td>\n",
       "      <td>1.393318</td>\n",
       "      <td>743.840998</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.269596</td>\n",
       "      <td>121.114537</td>\n",
       "      <td>1.450711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>1110.322901</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.319978</td>\n",
       "      <td>86.888968</td>\n",
       "      <td>1.289752</td>\n",
       "      <td>1255.444367</td>\n",
       "      <td>4.01</td>\n",
       "      <td>0.254768</td>\n",
       "      <td>90.182398</td>\n",
       "      <td>1.296364</td>\n",
       "      <td>900.757664</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.333440</td>\n",
       "      <td>82.135217</td>\n",
       "      <td>1.269616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021</td>\n",
       "      <td>1175.940286</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.317228</td>\n",
       "      <td>129.387017</td>\n",
       "      <td>1.453734</td>\n",
       "      <td>1349.181993</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.239080</td>\n",
       "      <td>145.397538</td>\n",
       "      <td>1.437394</td>\n",
       "      <td>936.280034</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.330064</td>\n",
       "      <td>107.226503</td>\n",
       "      <td>1.437441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022</td>\n",
       "      <td>1191.229032</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.321024</td>\n",
       "      <td>169.113073</td>\n",
       "      <td>1.666685</td>\n",
       "      <td>1408.743483</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.254792</td>\n",
       "      <td>203.258318</td>\n",
       "      <td>1.605715</td>\n",
       "      <td>1057.724214</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.323166</td>\n",
       "      <td>148.045225</td>\n",
       "      <td>1.685236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  avg_income_overall  90/10_ratio_income_overall  \\\n",
       "0  2016         1006.277011                        4.49   \n",
       "1  2017         1035.743265                        4.81   \n",
       "2  2018         1025.441813                        4.60   \n",
       "3  2019         1079.703580                        4.37   \n",
       "4  2020         1110.322901                        4.42   \n",
       "5  2021         1175.940286                        4.49   \n",
       "6  2022         1191.229032                        4.40   \n",
       "\n",
       "   log_var_income_overall  avg_consumption_overall  \\\n",
       "0                0.329679               169.668904   \n",
       "1                0.365085               173.455859   \n",
       "2                0.358912               154.643541   \n",
       "3                0.327387               147.320048   \n",
       "4                0.319978                86.888968   \n",
       "5                0.317228               129.387017   \n",
       "6                0.321024               169.113073   \n",
       "\n",
       "   log_var_consumption_overall  avg_income_wfh  90/10_ratio_income_wfh  \\\n",
       "0                     1.341467     1156.409845                    4.00   \n",
       "1                     1.367597     1219.199715                    4.40   \n",
       "2                     1.495767     1268.296132                    3.60   \n",
       "3                     1.433349     1276.417647                    3.95   \n",
       "4                     1.289752     1255.444367                    4.01   \n",
       "5                     1.453734     1349.181993                    3.82   \n",
       "6                     1.666685     1408.743483                    4.00   \n",
       "\n",
       "   log_var_income_wfh  avg_consumption_wfh  log_var_consumption_wfh  \\\n",
       "0            0.279134           178.944069                 1.236577   \n",
       "1            0.292146           187.468722                 1.370819   \n",
       "2            0.235835           165.145627                 1.425106   \n",
       "3            0.250969           162.646392                 1.393318   \n",
       "4            0.254768            90.182398                 1.296364   \n",
       "5            0.239080           145.397538                 1.437394   \n",
       "6            0.254792           203.258318                 1.605715   \n",
       "\n",
       "   avg_income_non_wfh  90/10_ratio_income_non_wfh  log_var_income_non_wfh  \\\n",
       "0          839.007426                        4.18                0.322427   \n",
       "1          768.538392                        3.81                0.336387   \n",
       "2          734.016631                        3.65                0.322255   \n",
       "3          743.840998                        3.59                0.269596   \n",
       "4          900.757664                        4.22                0.333440   \n",
       "5          936.280034                        4.18                0.330064   \n",
       "6         1057.724214                        4.11                0.323166   \n",
       "\n",
       "   avg_consumption_non_wfh  log_var_consumption_non_wfh  \n",
       "0               159.328869                     1.446107  \n",
       "1               153.090015                     1.332190  \n",
       "2               141.983051                     1.534999  \n",
       "3               121.114537                     1.450711  \n",
       "4                82.135217                     1.269616  \n",
       "5               107.226503                     1.437441  \n",
       "6               148.045225                     1.685236  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataset from your provided data\n",
    "data = {\n",
    "    'Year': [2016, 2017, 2018, 2019, 2020, 2021, 2022],\n",
    "    'avg_income_overall' : [income_result_2016['mean_income_overall'],\n",
    "                              income_result_2017['mean_income_overall'],\n",
    "                              income_result_2018['mean_income_overall'],\n",
    "                              income_result_2019['mean_income_overall'],\n",
    "                              income_result_2020['mean_income_overall'],\n",
    "                              income_result_2021['mean_income_overall'],\n",
    "                              income_result_2022['mean_income_overall']],\n",
    "    '90/10_ratio_income_overall': [df16_ratio_income,\n",
    "                                                  df17_ratio_income,\n",
    "                                                  df18_ratio_income,\n",
    "                                                  df19_ratio_income,\n",
    "                                                  df20_ratio_income,\n",
    "                                                  df21_ratio_income,\n",
    "                                                  df22_ratio_income],\n",
    "    'log_var_income_overall': [log_var_16['log_variance'],\n",
    "                                        log_var_17['log_variance'],\n",
    "                                        log_var_18['log_variance'],\n",
    "                                        log_var_19['log_variance'],\n",
    "                                        log_var_20['log_variance'],\n",
    "                                        log_var_21['log_variance'],\n",
    "                                        log_var_22['log_variance']],\n",
    "    'avg_consumption_overall': [consumption_result_2016['mean_consumption_overall'],\n",
    "                                      consumption_result_2017['mean_consumption_overall'],\n",
    "                                      consumption_result_2018['mean_consumption_overall'],\n",
    "                                      consumption_result_2019['mean_consumption_overall'],\n",
    "                                      consumption_result_2020['mean_consumption_overall'],\n",
    "                                      consumption_result_2021['mean_consumption_overall'],\n",
    "                                      consumption_result_2022['mean_consumption_overall']],\n",
    "    'log_var_consumption_overall': [log_var_consumption_16['log_variance'],\n",
    "                                   log_var_consumption_17['log_variance'],\n",
    "                                   log_var_consumption_18['log_variance'],\n",
    "                                   log_var_consumption_19['log_variance'],\n",
    "                                   log_var_consumption_20['log_variance'],\n",
    "                                   log_var_consumption_21['log_variance'],\n",
    "                                   log_var_consumption_22['log_variance']],\n",
    "    ##############################################################\n",
    "     'avg_income_wfh' : [income_result_2016['mean_income_wfh'],\n",
    "                              income_result_2017['mean_income_wfh'],\n",
    "                              income_result_2018['mean_income_wfh'],\n",
    "                              income_result_2019['mean_income_wfh'],\n",
    "                              income_result_2020['mean_income_wfh'],\n",
    "                              income_result_2021['mean_income_wfh'],\n",
    "                              income_result_2022['mean_income_wfh']],\n",
    "    '90/10_ratio_income_wfh': [df16_wfh_ratio_income,\n",
    "                                                  df17_wfh_ratio_income,\n",
    "                                                  df18_wfh_ratio_income,\n",
    "                                                  df19_wfh_ratio_income,\n",
    "                                                  df20_wfh_ratio_income,\n",
    "                                                  df21_wfh_ratio_income,\n",
    "                                                  df22_wfh_ratio_income],\n",
    "    'log_var_income_wfh': [log_var_wfh_16['log_variance'],\n",
    "                                        log_var_wfh_17['log_variance'],\n",
    "                                        log_var_wfh_18['log_variance'],\n",
    "                                        log_var_wfh_19['log_variance'],\n",
    "                                        log_var_wfh_20['log_variance'],\n",
    "                                        log_var_wfh_21['log_variance'],\n",
    "                                        log_var_wfh_22['log_variance']],\n",
    "    'avg_consumption_wfh': [consumption_result_2016['mean_consumption_wfh'],\n",
    "                                      consumption_result_2017['mean_consumption_wfh'],\n",
    "                                      consumption_result_2018['mean_consumption_wfh'],\n",
    "                                      consumption_result_2019['mean_consumption_wfh'],\n",
    "                                      consumption_result_2020['mean_consumption_wfh'],\n",
    "                                      consumption_result_2021['mean_consumption_wfh'],\n",
    "                                      consumption_result_2022['mean_consumption_wfh']],\n",
    "    'log_var_consumption_wfh': [log_var_consumption_wfh_16['log_variance'],\n",
    "                                   log_var_consumption_wfh_17['log_variance'],\n",
    "                                   log_var_consumption_wfh_18['log_variance'],\n",
    "                                   log_var_consumption_wfh_19['log_variance'],\n",
    "                                   log_var_consumption_wfh_20['log_variance'],\n",
    "                                   log_var_consumption_wfh_21['log_variance'],\n",
    "                                   log_var_consumption_wfh_22['log_variance']],\n",
    "    #####################################################################\n",
    "    'avg_income_non_wfh' : [income_result_2016['mean_income_non_wfh'],\n",
    "                              income_result_2017['mean_income_non_wfh'],\n",
    "                              income_result_2018['mean_income_non_wfh'],\n",
    "                              income_result_2019['mean_income_non_wfh'],\n",
    "                              income_result_2020['mean_income_non_wfh'],\n",
    "                              income_result_2021['mean_income_non_wfh'],\n",
    "                              income_result_2022['mean_income_non_wfh']],\n",
    "    '90/10_ratio_income_non_wfh': [df16_non_wfh_ratio_income,\n",
    "                                                  df17_non_wfh_ratio_income,\n",
    "                                                  df18_non_wfh_ratio_income,\n",
    "                                                  df19_non_wfh_ratio_income,\n",
    "                                                  df20_non_wfh_ratio_income,\n",
    "                                                  df21_non_wfh_ratio_income,\n",
    "                                                  df22_non_wfh_ratio_income],\n",
    "    'log_var_income_non_wfh': [log_var_non_wfh_16['log_variance'],\n",
    "                                        log_var_non_wfh_17['log_variance'],\n",
    "                                        log_var_non_wfh_18['log_variance'],\n",
    "                                        log_var_non_wfh_19['log_variance'],\n",
    "                                        log_var_non_wfh_20['log_variance'],\n",
    "                                        log_var_non_wfh_21['log_variance'],\n",
    "                                        log_var_non_wfh_22['log_variance']],\n",
    "    'avg_consumption_non_wfh': [consumption_result_2016['mean_consumption_non_wfh'],\n",
    "                                      consumption_result_2017['mean_consumption_non_wfh'],\n",
    "                                      consumption_result_2018['mean_consumption_non_wfh'],\n",
    "                                      consumption_result_2019['mean_consumption_non_wfh'],\n",
    "                                      consumption_result_2020['mean_consumption_non_wfh'],\n",
    "                                      consumption_result_2021['mean_consumption_non_wfh'],\n",
    "                                      consumption_result_2022['mean_consumption_non_wfh']],\n",
    "    'log_var_consumption_non_wfh': [log_var_consumption_non_wfh_16['log_variance'],\n",
    "                                   log_var_consumption_non_wfh_17['log_variance'],\n",
    "                                   log_var_consumption_non_wfh_18['log_variance'],\n",
    "                                   log_var_consumption_non_wfh_19['log_variance'],\n",
    "                                   log_var_consumption_non_wfh_20['log_variance'],\n",
    "                                   log_var_consumption_non_wfh_21['log_variance'],\n",
    "                                   log_var_consumption_non_wfh_22['log_variance']],\n",
    "}\n",
    "\n",
    "df_measures = pd.DataFrame(data)\n",
    "df_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b1763ed9-0fbc-4e2f-99cc-13ff8cb64d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the calculated measures table for easy access\n",
    "\n",
    "df_measures.to_csv('inequality_measures.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272bd62e-e0a6-48c0-b342-b955ba89ad0e",
   "metadata": {},
   "source": [
    "In this section, we visualise the trend of the inequality measures with a 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81ddab5d-4f33-4cb4-b591-b69c991b2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fee5856b-700c-4ba0-b36f-3b296930ae5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function to bootstrap average income measure\n",
    "\n",
    "class UKLivingCostBootstrapIncome:\n",
    "    \"\"\"\n",
    "    A class to perform bootstrap analysis on UK Living Cost and Food Survey data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, income_column: str = 'total_income'):\n",
    "        \"\"\"\n",
    "        Initialize the bootstrap analysis\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            The UK Living Cost and Food Survey data\n",
    "        income_column : str\n",
    "            Name of the column containing income data\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.income_column = income_column\n",
    "        self.bootstrap_means = None\n",
    "        self.original_mean = None\n",
    "        \n",
    "    def load_sample_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create sample UK Living Cost and Food Survey data for demonstration\n",
    "        (In practice, you would load this from ONS data files)\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        n_households = 1000\n",
    "        \n",
    "        # Simulate realistic UK household income data\n",
    "        sample_data = pd.DataFrame({\n",
    "            'household_id': range(1, n_households + 1),\n",
    "            'total_income': np.random.lognormal(mean=9.5, sigma=0.6, size=n_households),\n",
    "            'food_income': np.random.lognormal(mean=7.8, sigma=0.4, size=n_households),\n",
    "            'housing_income': np.random.lognormal(mean=8.2, sigma=0.5, size=n_households),\n",
    "            'household_size': np.random.choice([1, 2, 3, 4, 5, 6], size=n_households, \n",
    "                                            p=[0.3, 0.35, 0.2, 0.1, 0.04, 0.01]),\n",
    "            'region': np.random.choice(['London', 'South East', 'North West', 'Scotland', \n",
    "                                     'Wales', 'Other'], size=n_households,\n",
    "                                    p=[0.15, 0.2, 0.15, 0.1, 0.05, 0.35])\n",
    "        })\n",
    "        \n",
    "        return sample_data\n",
    "    \n",
    "    def bootstrap_sample(self, n_bootstrap: int = 1000) -> List[float]:\n",
    "        \"\"\"\n",
    "        Perform bootstrap sampling and calculate means\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_bootstrap : int\n",
    "            Number of bootstrap samples to generate\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        List[float]\n",
    "            List of bootstrap sample means\n",
    "        \"\"\"\n",
    "        income_data = self.data[self.income_column].dropna()\n",
    "        self.original_mean = income_data.mean()\n",
    "        \n",
    "        bootstrap_means = []\n",
    "        n_samples = len(income_data)\n",
    "        \n",
    "        print(f\"Performing {n_bootstrap} bootstrap samples...\")\n",
    "        print(f\"Original sample size: {n_samples}\")\n",
    "        print(f\"Original mean income: {self.original_mean:.2f}\")\n",
    "        \n",
    "        for i in range(n_bootstrap):\n",
    "            # Bootstrap sample with replacement\n",
    "            bootstrap_sample = np.random.choice(income_data, size=n_samples, replace=True)\n",
    "            bootstrap_mean = np.mean(bootstrap_sample)\n",
    "            bootstrap_means.append(bootstrap_mean)\n",
    "            \n",
    "            if (i + 1) % 200 == 0:\n",
    "                print(f\"Completed {i + 1} bootstrap samples\")\n",
    "        \n",
    "        self.bootstrap_means = bootstrap_means\n",
    "        return bootstrap_means\n",
    "    \n",
    "    def calculate_confidence_interval(self, confidence_level: float = 0.95) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate confidence interval from bootstrap results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level (default 0.95 for 95% CI)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Tuple[float, float]\n",
    "            Lower and upper bounds of confidence interval\n",
    "        \"\"\"\n",
    "        if self.bootstrap_means is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        alpha = 1 - confidence_level\n",
    "        lower_percentile = (alpha/2) * 100\n",
    "        upper_percentile = (1 - alpha/2) * 100\n",
    "        \n",
    "        ci_lower = np.percentile(self.bootstrap_means, lower_percentile)\n",
    "        ci_upper = np.percentile(self.bootstrap_means, upper_percentile)\n",
    "        \n",
    "        return ci_lower, ci_upper\n",
    "    \n",
    "    def analyze_bootstrap_results(self, confidence_level: float = 0.95) -> dict:\n",
    "        \"\"\"\n",
    "        Comprehensive analysis of bootstrap results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level for CI calculation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing all analysis results\n",
    "        \"\"\"\n",
    "        if self.bootstrap_means is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        ci_lower, ci_upper = self.calculate_confidence_interval(confidence_level)\n",
    "        \n",
    "        results = {\n",
    "            'original_mean': self.original_mean,\n",
    "            'bootstrap_mean': np.mean(self.bootstrap_means),\n",
    "            'bootstrap_std': np.std(self.bootstrap_means),\n",
    "            'bootstrap_se': np.std(self.bootstrap_means),  # Standard error\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'confidence_level': confidence_level,\n",
    "            'n_bootstrap': len(self.bootstrap_means),\n",
    "            'bias': np.mean(self.bootstrap_means) - self.original_mean\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_bootstrap_distribution(self, confidence_level: float = 0.95) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Plot the bootstrap distribution with confidence intervals\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level for CI visualization\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        plt.Figure\n",
    "            Matplotlib figure object\n",
    "        \"\"\"\n",
    "        if self.bootstrap_means is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        ci_lower, ci_upper = self.calculate_confidence_interval(confidence_level)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Histogram of bootstrap means\n",
    "        ax1.hist(self.bootstrap_means, bins=50, density=True, alpha=0.7, color='skyblue', \n",
    "                edgecolor='black', label='Bootstrap Distribution')\n",
    "        ax1.axvline(self.original_mean, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Original Mean: {self.original_mean:.2f}')\n",
    "        ax1.axvline(ci_lower, color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'{confidence_level*100}% CI Lower: {ci_lower:.2f}')\n",
    "        ax1.axvline(ci_upper, color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'{confidence_level*100}% CI Upper: {ci_upper:.2f}')\n",
    "        ax1.set_xlabel('Mean income (£)')\n",
    "        ax1.set_ylabel('Density')\n",
    "        ax1.set_title('Bootstrap Distribution of Sample Means')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Q plot to check normality\n",
    "        stats.probplot(self.bootstrap_means, dist=\"norm\", plot=ax2)\n",
    "        ax2.set_title('Q-Q Plot: Bootstrap Means vs Normal Distribution')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def print_summary_report(self, confidence_level: float = 0.95) -> None:\n",
    "        \"\"\"\n",
    "        Print a comprehensive summary report\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level for reporting\n",
    "        \"\"\"\n",
    "        results = self.analyze_bootstrap_results(confidence_level)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"UK LIVING COST AND FOOD SURVEY - BOOTSTRAP ANALYSIS REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nSample Information:\")\n",
    "        print(f\"  Original sample size: {len(self.data)}\")\n",
    "        print(f\"  Number of bootstrap samples: {results['n_bootstrap']}\")\n",
    "        print(f\"  income variable: {self.income_column}\")\n",
    "        \n",
    "        print(f\"\\nincome Statistics:\")\n",
    "        print(f\"  Original sample mean: {results['original_mean']:.2f}\")\n",
    "        print(f\"  Bootstrap mean: {results['bootstrap_mean']:.2f}\")\n",
    "        print(f\"  Bootstrap standard error: {results['bootstrap_se']:.2f}\")\n",
    "        print(f\"  Bias (Bootstrap - Original): {results['bias']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nConfidence Interval ({confidence_level*100}%):\")\n",
    "        print(f\"  Lower bound: {results['ci_lower']:.2f}\")\n",
    "        print(f\"  Upper bound: {results['ci_upper']:.2f}\")\n",
    "        print(f\"  Interval width: {results['ci_upper'] - results['ci_lower']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"  We are {confidence_level*100}% confident that the true population mean\")\n",
    "        print(f\"  income lies between {results['ci_lower']:.2f} and £{results['ci_upper']:.2f}\")\n",
    "        \n",
    "        if abs(results['bias']) > 0.01:\n",
    "            print(f\"\\nNote: Bootstrap bias of {results['bias']:.2f} detected.\")\n",
    "            print(f\"  Bias-corrected estimate: {results['original_mean'] - results['bias']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5a331397-dc76-48f6-a050-20cf6fca39a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2556\n",
      "Original mean income: 1006.28\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2852\n",
      "Original mean income: 1035.74\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2893\n",
      "Original mean income: 1025.44\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2794\n",
      "Original mean income: 1079.70\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2884\n",
      "Original mean income: 1110.32\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2984\n",
      "Original mean income: 1175.94\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2390\n",
      "Original mean income: 1191.23\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# bootstrap the confidence interval for average income measure - overall\n",
    "\n",
    "# Initialize analysis\n",
    "analyser_income_overall_2016 = UKLivingCostBootstrapIncome(df_16, income_column='p344p')\n",
    "analyser_income_overall_2017 = UKLivingCostBootstrapIncome(df_17, income_column='p344p')\n",
    "analyser_income_overall_2018 = UKLivingCostBootstrapIncome(df_18, income_column='p344p')\n",
    "analyser_income_overall_2019 = UKLivingCostBootstrapIncome(df_19, income_column='p344p')\n",
    "analyser_income_overall_2020 = UKLivingCostBootstrapIncome(df_20, income_column='p344p')\n",
    "analyser_income_overall_2021 = UKLivingCostBootstrapIncome(df_21, income_column='p344p')\n",
    "analyser_income_overall_2022 = UKLivingCostBootstrapIncome(df_22, income_column='p344p')\n",
    "\n",
    "# Run bootstrap (1000 samples)\n",
    "bootstrap_income_means_overall_2016 = analyser_income_overall_2016.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_overall_2017 = analyser_income_overall_2017.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_overall_2018 = analyser_income_overall_2018.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_overall_2019 = analyser_income_overall_2019.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_overall_2020 = analyser_income_overall_2020.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_overall_2021 = analyser_income_overall_2021.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_overall_2022 = analyser_income_overall_2022.bootstrap_sample(n_bootstrap=1000)\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower_income_overall_2016, ci_upper_income_overall_2016 = analyser_income_overall_2016.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_overall_2017, ci_upper_income_overall_2017 = analyser_income_overall_2017.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_overall_2018, ci_upper_income_overall_2018 = analyser_income_overall_2018.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_overall_2019, ci_upper_income_overall_2019 = analyser_income_overall_2019.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_overall_2020, ci_upper_income_overall_2020 = analyser_income_overall_2020.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_overall_2021, ci_upper_income_overall_2021 = analyser_income_overall_2021.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_overall_2022, ci_upper_income_overall_2022 = analyser_income_overall_2022.calculate_confidence_interval(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a0e9abbb-82b5-479d-bb53-c5259e30f940",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1347\n",
      "Original mean income: 1156.41\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1691\n",
      "Original mean income: 1219.20\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1578\n",
      "Original mean income: 1268.30\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1762\n",
      "Original mean income: 1276.42\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1704\n",
      "Original mean income: 1255.44\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1732\n",
      "Original mean income: 1349.18\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 909\n",
      "Original mean income: 1408.74\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# bootstrap the confidence interval for average income measure - wfh\n",
    "\n",
    "# Initialize analysis\n",
    "analyser_income_wfh_2016 = UKLivingCostBootstrapIncome(df_16, income_column='p344p-wfh')\n",
    "analyser_income_wfh_2017 = UKLivingCostBootstrapIncome(df_17, income_column='p344p-wfh')\n",
    "analyser_income_wfh_2018 = UKLivingCostBootstrapIncome(df_18, income_column='p344p-wfh')\n",
    "analyser_income_wfh_2019 = UKLivingCostBootstrapIncome(df_19, income_column='p344p-wfh')\n",
    "analyser_income_wfh_2020 = UKLivingCostBootstrapIncome(df_20, income_column='p344p-wfh')\n",
    "analyser_income_wfh_2021 = UKLivingCostBootstrapIncome(df_21, income_column='p344p-wfh')\n",
    "analyser_income_wfh_2022 = UKLivingCostBootstrapIncome(df_22, income_column='p344p-wfh')\n",
    "\n",
    "# Run bootstrap (1000 samples)\n",
    "bootstrap_income_means_wfh_2016 = analyser_income_wfh_2016.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_wfh_2017 = analyser_income_wfh_2017.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_wfh_2018 = analyser_income_wfh_2018.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_wfh_2019 = analyser_income_wfh_2019.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_wfh_2020 = analyser_income_wfh_2020.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_wfh_2021 = analyser_income_wfh_2021.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_wfh_2022 = analyser_income_wfh_2022.bootstrap_sample(n_bootstrap=1000)\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower_income_wfh_2016, ci_upper_income_wfh_2016 = analyser_income_wfh_2016.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_wfh_2017, ci_upper_income_wfh_2017 = analyser_income_wfh_2017.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_wfh_2018, ci_upper_income_wfh_2018 = analyser_income_wfh_2018.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_wfh_2019, ci_upper_income_wfh_2019 = analyser_income_wfh_2019.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_wfh_2020, ci_upper_income_wfh_2020 = analyser_income_wfh_2020.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_wfh_2021, ci_upper_income_wfh_2021 = analyser_income_wfh_2021.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_wfh_2022, ci_upper_income_wfh_2022 = analyser_income_wfh_2022.calculate_confidence_interval(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "cdcadac5-734c-4ed3-bbe0-6a788b51a04f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1209\n",
      "Original mean income: 839.01\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1161\n",
      "Original mean income: 768.54\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1315\n",
      "Original mean income: 734.02\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1032\n",
      "Original mean income: 743.84\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1180\n",
      "Original mean income: 900.76\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1252\n",
      "Original mean income: 936.28\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1481\n",
      "Original mean income: 1057.72\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# bootstrap the confidence interval for average income measure - non_wfh\n",
    "\n",
    "# Initialize analysis\n",
    "analyser_income_non_wfh_2016 = UKLivingCostBootstrapIncome(df_16, income_column='p344p-non_wfh')\n",
    "analyser_income_non_wfh_2017 = UKLivingCostBootstrapIncome(df_17, income_column='p344p-non_wfh')\n",
    "analyser_income_non_wfh_2018 = UKLivingCostBootstrapIncome(df_18, income_column='p344p-non_wfh')\n",
    "analyser_income_non_wfh_2019 = UKLivingCostBootstrapIncome(df_19, income_column='p344p-non_wfh')\n",
    "analyser_income_non_wfh_2020 = UKLivingCostBootstrapIncome(df_20, income_column='p344p-non_wfh')\n",
    "analyser_income_non_wfh_2021 = UKLivingCostBootstrapIncome(df_21, income_column='p344p-non_wfh')\n",
    "analyser_income_non_wfh_2022 = UKLivingCostBootstrapIncome(df_22, income_column='p344p-non_wfh')\n",
    "\n",
    "# Run bootstrap (1000 samples)\n",
    "bootstrap_income_means_non_wfh_2016 = analyser_income_non_wfh_2016.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_non_wfh_2017 = analyser_income_non_wfh_2017.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_non_wfh_2018 = analyser_income_non_wfh_2018.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_non_wfh_2019 = analyser_income_non_wfh_2019.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_non_wfh_2020 = analyser_income_non_wfh_2020.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_non_wfh_2021 = analyser_income_non_wfh_2021.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_income_means_non_wfh_2022 = analyser_income_non_wfh_2022.bootstrap_sample(n_bootstrap=1000)\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower_income_non_wfh_2016, ci_upper_income_non_wfh_2016 = analyser_income_non_wfh_2016.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_non_wfh_2017, ci_upper_income_non_wfh_2017 = analyser_income_non_wfh_2017.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_non_wfh_2018, ci_upper_income_non_wfh_2018 = analyser_income_non_wfh_2018.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_non_wfh_2019, ci_upper_income_non_wfh_2019 = analyser_income_non_wfh_2019.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_non_wfh_2020, ci_upper_income_non_wfh_2020 = analyser_income_non_wfh_2020.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_non_wfh_2021, ci_upper_income_non_wfh_2021 = analyser_income_non_wfh_2021.calculate_confidence_interval(0.95)\n",
    "ci_lower_income_non_wfh_2022, ci_upper_income_non_wfh_2022 = analyser_income_non_wfh_2022.calculate_confidence_interval(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7b5222e-7922-4b10-a663-ca09b1baf25c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function to bootstrap income ratio and calculate confidence interval\n",
    "\n",
    "class UKLivingCostBootstrap:\n",
    "    \"\"\"\n",
    "    Bootstrap analysis for UK Living Cost and Food Survey\n",
    "    Calculates 90/10 percentile ratio with confidence intervals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, income_column: str = 'income', \n",
    "                 weights_column: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the bootstrap analyzer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Survey data containing income information\n",
    "        income_column : str\n",
    "            Name of the income column\n",
    "        weights_column : str, optional\n",
    "            Name of the survey weights column\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.income_column = income_column\n",
    "        self.weights_column = weights_column\n",
    "        self.bootstrap_results = None\n",
    "        \n",
    "        # Clean data\n",
    "        self._clean_data()\n",
    "    \n",
    "    def _clean_data(self):\n",
    "        \"\"\"Clean the survey data\"\"\"\n",
    "        # Remove missing values\n",
    "        self.data = self.data.dropna(subset=[self.income_column])\n",
    "        \n",
    "        # Remove negative or zero incomes\n",
    "        self.data = self.data[self.data[self.income_column] > 0]\n",
    "        \n",
    "        # Handle weights\n",
    "        if self.weights_column and self.weights_column in self.data.columns:\n",
    "            self.data = self.data.dropna(subset=[self.weights_column])\n",
    "            self.data = self.data[self.data[self.weights_column] > 0]\n",
    "        \n",
    "        print(f\"Clean data shape: {self.data.shape}\")\n",
    "        print(f\"Income range: £{self.data[self.income_column].min():.2f} - £{self.data[self.income_column].max():.2f}\")\n",
    "    \n",
    "    def calculate_percentile_ratio(self, income_data: np.ndarray, \n",
    "                                 weights: np.ndarray = None) -> float:\n",
    "        \"\"\"\n",
    "        Calculate 90/10 percentile ratio\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        income_data : np.ndarray\n",
    "            Income data\n",
    "        weights : np.ndarray, optional\n",
    "            Survey weights\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float : 90/10 percentile ratio\n",
    "        \"\"\"\n",
    "        if weights is not None:\n",
    "            # Weighted percentiles\n",
    "            p10 = self._weighted_percentile(income_data, weights, 10)\n",
    "            p90 = self._weighted_percentile(income_data, weights, 90)\n",
    "        else:\n",
    "            # Unweighted percentiles\n",
    "            p10 = np.percentile(income_data, 10)\n",
    "            p90 = np.percentile(income_data, 90)\n",
    "        \n",
    "        return p90 / p10 if p10 > 0 else np.nan\n",
    "    \n",
    "    def _weighted_percentile(self, data: np.ndarray, weights: np.ndarray, \n",
    "                           percentile: float) -> float:\n",
    "        \"\"\"Calculate weighted percentile\"\"\"\n",
    "        # Sort data and weights\n",
    "        sorted_indices = np.argsort(data)\n",
    "        sorted_data = data[sorted_indices]\n",
    "        sorted_weights = weights[sorted_indices]\n",
    "        \n",
    "        # Calculate cumulative weights\n",
    "        cumulative_weights = np.cumsum(sorted_weights)\n",
    "        total_weight = cumulative_weights[-1]\n",
    "        \n",
    "        # Find percentile position\n",
    "        percentile_weight = (percentile / 100.0) * total_weight\n",
    "        \n",
    "        # Find the value at the percentile\n",
    "        idx = np.searchsorted(cumulative_weights, percentile_weight)\n",
    "        \n",
    "        if idx == 0:\n",
    "            return sorted_data[0]\n",
    "        elif idx >= len(sorted_data):\n",
    "            return sorted_data[-1]\n",
    "        else:\n",
    "            # Linear interpolation\n",
    "            w1 = cumulative_weights[idx - 1]\n",
    "            w2 = cumulative_weights[idx]\n",
    "            v1 = sorted_data[idx - 1]\n",
    "            v2 = sorted_data[idx]\n",
    "            \n",
    "            return v1 + (v2 - v1) * (percentile_weight - w1) / (w2 - w1)\n",
    "    \n",
    "    def bootstrap_sample(self, n_bootstrap: int = 1000, \n",
    "                        random_state: int = 42) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform bootstrap sampling\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_bootstrap : int\n",
    "            Number of bootstrap samples\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Array of bootstrap 90/10 ratios\n",
    "        \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        bootstrap_ratios = []\n",
    "        n_obs = len(self.data)\n",
    "        \n",
    "        income_data = self.data[self.income_column].values\n",
    "        weights = (self.data[self.weights_column].values \n",
    "                  if self.weights_column else None)\n",
    "        \n",
    "        print(f\"Performing {n_bootstrap} bootstrap samples...\")\n",
    "        \n",
    "        for i in range(n_bootstrap):\n",
    "            # Bootstrap sample with replacement\n",
    "            bootstrap_indices = np.random.choice(n_obs, size=n_obs, replace=True)\n",
    "            \n",
    "            bootstrap_income = income_data[bootstrap_indices]\n",
    "            bootstrap_weights = (weights[bootstrap_indices] \n",
    "                               if weights is not None else None)\n",
    "            \n",
    "            # Calculate 90/10 ratio for bootstrap sample\n",
    "            ratio = self.calculate_percentile_ratio(bootstrap_income, bootstrap_weights)\n",
    "            \n",
    "            if not np.isnan(ratio):\n",
    "                bootstrap_ratios.append(ratio)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Completed {i + 1}/{n_bootstrap} bootstrap samples\")\n",
    "        \n",
    "        self.bootstrap_results = np.array(bootstrap_ratios)\n",
    "        return self.bootstrap_results\n",
    "    \n",
    "    def calculate_confidence_interval(self, confidence_level: float = 0.95) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate confidence interval for the 90/10 ratio\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level (default 0.95 for 95% CI)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Tuple[float, float] : Lower and upper bounds of confidence interval\n",
    "        \"\"\"\n",
    "        if self.bootstrap_results is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        alpha = 1 - confidence_level\n",
    "        lower_percentile = (alpha / 2) * 100\n",
    "        upper_percentile = (1 - alpha / 2) * 100\n",
    "        \n",
    "        lower_bound = np.percentile(self.bootstrap_results, lower_percentile)\n",
    "        upper_bound = np.percentile(self.bootstrap_results, upper_percentile)\n",
    "        \n",
    "        return lower_bound, upper_bound\n",
    "    \n",
    "    def get_summary_statistics(self) -> dict:\n",
    "        \"\"\"Get summary statistics of bootstrap results\"\"\"\n",
    "        if self.bootstrap_results is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        # Original sample statistics\n",
    "        income_data = self.data[self.income_column].values\n",
    "        weights = (self.data[self.weights_column].values \n",
    "                  if self.weights_column else None)\n",
    "        \n",
    "        original_ratio = self.calculate_percentile_ratio(income_data, weights)\n",
    "        \n",
    "        # Bootstrap statistics\n",
    "        bootstrap_mean = np.mean(self.bootstrap_results)\n",
    "        bootstrap_std = np.std(self.bootstrap_results)\n",
    "        bootstrap_median = np.median(self.bootstrap_results)\n",
    "        \n",
    "        # Confidence intervals\n",
    "        ci_90 = self.calculate_confidence_interval(0.90)\n",
    "        ci_95 = self.calculate_confidence_interval(0.95)\n",
    "        ci_99 = self.calculate_confidence_interval(0.99)\n",
    "        \n",
    "        return {\n",
    "            'original_sample': {\n",
    "                'n_observations': len(self.data),\n",
    "                'percentile_90_10_ratio': original_ratio,\n",
    "                'mean_income': np.mean(income_data),\n",
    "                'median_income': np.median(income_data)\n",
    "            },\n",
    "            'bootstrap': {\n",
    "                'n_bootstrap_samples': len(self.bootstrap_results),\n",
    "                'mean_ratio': bootstrap_mean,\n",
    "                'std_ratio': bootstrap_std,\n",
    "                'median_ratio': bootstrap_median,\n",
    "                'bias': bootstrap_mean - original_ratio\n",
    "            },\n",
    "            'confidence_intervals': {\n",
    "                '90%': ci_90,\n",
    "                '95%': ci_95,\n",
    "                '99%': ci_99\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def plot_bootstrap_distribution(self, figsize: Tuple[int, int] = (12, 8)):\n",
    "        \"\"\"Plot bootstrap distribution\"\"\"\n",
    "        if self.bootstrap_results is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        fig.suptitle('Bootstrap Analysis: 90/10 Percentile Ratio', fontsize=16)\n",
    "        \n",
    "        # Original sample statistics\n",
    "        income_data = self.data[self.income_column].values\n",
    "        weights = (self.data[self.weights_column].values \n",
    "                  if self.weights_column else None)\n",
    "        original_ratio = self.calculate_percentile_ratio(income_data, weights)\n",
    "        \n",
    "        # 1. Income distribution\n",
    "        axes[0, 0].hist(income_data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_title('Original Income Distribution')\n",
    "        axes[0, 0].set_xlabel('Income (£)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].axvline(np.percentile(income_data, 10), color='red', linestyle='--', \n",
    "                          label='10th percentile')\n",
    "        axes[0, 0].axvline(np.percentile(income_data, 90), color='red', linestyle='--', \n",
    "                          label='90th percentile')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # 2. Bootstrap distribution\n",
    "        axes[0, 1].hist(self.bootstrap_results, bins=50, alpha=0.7, color='lightgreen', \n",
    "                       edgecolor='black')\n",
    "        axes[0, 1].set_title('Bootstrap Distribution of 90/10 Ratio')\n",
    "        axes[0, 1].set_xlabel('90/10 Percentile Ratio')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].axvline(original_ratio, color='red', linestyle='-', \n",
    "                          label=f'Original: {original_ratio:.2f}')\n",
    "        axes[0, 1].axvline(np.mean(self.bootstrap_results), color='blue', linestyle='--', \n",
    "                          label=f'Bootstrap Mean: {np.mean(self.bootstrap_results):.2f}')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # 3. Q-Q plot\n",
    "        stats.probplot(self.bootstrap_results, dist=\"norm\", plot=axes[1, 0])\n",
    "        axes[1, 0].set_title('Q-Q Plot: Bootstrap Results vs Normal')\n",
    "        \n",
    "        # 4. Confidence intervals\n",
    "        ci_levels = [0.90, 0.95, 0.99]\n",
    "        ci_data = []\n",
    "        for level in ci_levels:\n",
    "            lower, upper = self.calculate_confidence_interval(level)\n",
    "            ci_data.append([level * 100, lower, upper, upper - lower])\n",
    "        \n",
    "        ci_df = pd.DataFrame(ci_data, columns=['CI Level (%)', 'Lower', 'Upper', 'Width'])\n",
    "        \n",
    "        # Plot confidence intervals\n",
    "        y_pos = np.arange(len(ci_levels))\n",
    "        axes[1, 1].barh(y_pos, ci_df['Width'], left=ci_df['Lower'], alpha=0.7)\n",
    "        axes[1, 1].set_yticks(y_pos)\n",
    "        axes[1, 1].set_yticklabels([f'{int(level)}%' for level in ci_df['CI Level (%)']])\n",
    "        axes[1, 1].set_xlabel('90/10 Percentile Ratio')\n",
    "        axes[1, 1].set_title('Confidence Intervals')\n",
    "        axes[1, 1].axvline(original_ratio, color='red', linestyle='-', \n",
    "                          label=f'Original: {original_ratio:.2f}')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def export_results(self, filename: str = 'bootstrap_results.csv'):\n",
    "        \"\"\"Export bootstrap results to CSV\"\"\"\n",
    "        if self.bootstrap_results is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'bootstrap_sample': range(1, len(self.bootstrap_results) + 1),\n",
    "            'percentile_90_10_ratio': self.bootstrap_results\n",
    "        })\n",
    "        \n",
    "        results_df.to_csv(filename, index=False)\n",
    "        print(f\"Results exported to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2da06-b5fc-41ea-a706-dfbb6797b4ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SAMPLE\n",
    "# Bootstrap the confidence interval of income ratio for 2016 - overall\n",
    "\n",
    "# Initialize bootstrap analyzer\n",
    "analyzer_income_ratio_overall_2016 = UKLivingCostBootstrap(\n",
    "    data=df_16,\n",
    "    income_column='p344p'\n",
    ")\n",
    "\n",
    "# Perform bootstrap analysis\n",
    "bootstrap_results_income_ratio_overall_2016 = analyzer_income_ratio_overall_2016.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "\n",
    "# Get summary statistics\n",
    "summary_income_ratio_overall_2016 = analyzer_income_ratio_overall_2016.get_summary_statistics()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BOOTSTRAP ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nOriginal Sample:\")\n",
    "print(f\"  Number of observations: {summary_income_ratio_overall_2016['original_sample']['n_observations']:,}\")\n",
    "print(f\"  90/10 percentile ratio: {summary_income_ratio_overall_2016['original_sample']['percentile_90_10_ratio']:.3f}\")\n",
    "print(f\"  Mean income: £{summary_income_ratio_overall_2016['original_sample']['mean_income']:,.2f}\")\n",
    "print(f\"  Median income: £{summary_income_ratio_overall_2016['original_sample']['median_income']:,.2f}\")\n",
    "\n",
    "print(f\"\\nBootstrap Results:\")\n",
    "print(f\"  Number of bootstrap samples: {summary_income_ratio_overall_2016['bootstrap']['n_bootstrap_samples']:,}\")\n",
    "print(f\"  Mean 90/10 ratio: {summary_income_ratio_overall_2016['bootstrap']['mean_ratio']:.3f}\")\n",
    "print(f\"  Standard error: {summary_income_ratio_overall_2016['bootstrap']['std_ratio']:.3f}\")\n",
    "print(f\"  Bias: {summary_income_ratio_overall_2016['bootstrap']['bias']:.3f}\")\n",
    "\n",
    "print(f\"\\nConfidence Intervals:\")\n",
    "for level, (lower, upper) in summary_income_ratio_overall_2016['confidence_intervals'].items():\n",
    "    print(f\"  {level}: ({lower:.3f}, {upper:.3f})\")\n",
    "\n",
    "\n",
    "# Store confidence interval bounds\n",
    "\n",
    "ci_upper_income_overall_2016, ci_lower_income_overall_2016 = summary_income_ratio_overall_2016['confidence_intervals']['95%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3d734dc4-c86b-46ea-8b6b-33d8a111bf05",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data shape: (2556, 27)\n",
      "Income range: £43.26 - £2180.93\n",
      "Clean data shape: (2852, 27)\n",
      "Income range: £15.69 - £2342.01\n",
      "Clean data shape: (2893, 27)\n",
      "Income range: £18.87 - £2362.60\n",
      "Clean data shape: (2794, 27)\n",
      "Income range: £40.00 - £2496.18\n",
      "Clean data shape: (2884, 27)\n",
      "Income range: £77.91 - £2420.03\n",
      "Clean data shape: (2984, 27)\n",
      "Income range: £18.51 - £2453.72\n",
      "Clean data shape: (2390, 27)\n",
      "Income range: £54.15 - £2519.05\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap the confidence interval of income ratio - overall\n",
    "\n",
    "# Initialize bootstrap analyzer\n",
    "analyzer_income_ratio_overall_2016 = UKLivingCostBootstrap(\n",
    "    data=df_16,\n",
    "    income_column='p344p'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_overall_2017 = UKLivingCostBootstrap(\n",
    "    data=df_17,\n",
    "    income_column='p344p'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_overall_2018 = UKLivingCostBootstrap(\n",
    "    data=df_18,\n",
    "    income_column='p344p'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_overall_2019 = UKLivingCostBootstrap(\n",
    "    data=df_19,\n",
    "    income_column='p344p'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_overall_2020 = UKLivingCostBootstrap(\n",
    "    data=df_20,\n",
    "    income_column='p344p'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_overall_2021 = UKLivingCostBootstrap(\n",
    "    data=df_21,\n",
    "    income_column='p344p'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_overall_2022 = UKLivingCostBootstrap(\n",
    "    data=df_22,\n",
    "    income_column='p344p'\n",
    ")\n",
    "\n",
    "# Perform bootstrap analysis\n",
    "bootstrap_results_income_ratio_overall_2016 = analyzer_income_ratio_overall_2016.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_overall_2017 = analyzer_income_ratio_overall_2017.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_overall_2018 = analyzer_income_ratio_overall_2018.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_overall_2019 = analyzer_income_ratio_overall_2019.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_overall_2020 = analyzer_income_ratio_overall_2020.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_overall_2021 = analyzer_income_ratio_overall_2021.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_overall_2022 = analyzer_income_ratio_overall_2022.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "\n",
    "# Get summary statistics\n",
    "summary_income_ratio_overall_2016 = analyzer_income_ratio_overall_2016.get_summary_statistics()\n",
    "summary_income_ratio_overall_2017 = analyzer_income_ratio_overall_2017.get_summary_statistics()\n",
    "summary_income_ratio_overall_2018 = analyzer_income_ratio_overall_2018.get_summary_statistics()\n",
    "summary_income_ratio_overall_2019 = analyzer_income_ratio_overall_2019.get_summary_statistics()\n",
    "summary_income_ratio_overall_2020 = analyzer_income_ratio_overall_2020.get_summary_statistics()\n",
    "summary_income_ratio_overall_2021 = analyzer_income_ratio_overall_2021.get_summary_statistics()\n",
    "summary_income_ratio_overall_2022 = analyzer_income_ratio_overall_2022.get_summary_statistics()\n",
    "\n",
    "# Store confidence interval bounds\n",
    "ci_lower_income_overall_2016, ci_upper_income_overall_2016 = summary_income_ratio_overall_2016['confidence_intervals']['95%']\n",
    "ci_lower_income_overall_2017, ci_upper_income_overall_2017 = summary_income_ratio_overall_2017['confidence_intervals']['95%']\n",
    "ci_lower_income_overall_2018, ci_upper_income_overall_2018 = summary_income_ratio_overall_2018['confidence_intervals']['95%']\n",
    "ci_lower_income_overall_2019, ci_upper_income_overall_2019 = summary_income_ratio_overall_2019['confidence_intervals']['95%']\n",
    "ci_lower_income_overall_2020, ci_upper_income_overall_2020 = summary_income_ratio_overall_2020['confidence_intervals']['95%']\n",
    "ci_lower_income_overall_2021, ci_upper_income_overall_2021 = summary_income_ratio_overall_2021['confidence_intervals']['95%']\n",
    "ci_lower_income_overall_2022, ci_upper_income_overall_2022 = summary_income_ratio_overall_2022['confidence_intervals']['95%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c8e9bcf7-ea29-468d-ae53-5f7f19ce18b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data shape: (1347, 27)\n",
      "Income range: £90.00 - £2180.93\n",
      "Clean data shape: (1691, 27)\n",
      "Income range: £120.00 - £2342.01\n",
      "Clean data shape: (1578, 27)\n",
      "Income range: £126.30 - £2362.60\n",
      "Clean data shape: (1762, 27)\n",
      "Income range: £60.58 - £2496.18\n",
      "Clean data shape: (1704, 27)\n",
      "Income range: £126.93 - £2420.03\n",
      "Clean data shape: (1732, 27)\n",
      "Income range: £54.00 - £2453.72\n",
      "Clean data shape: (909, 27)\n",
      "Income range: £170.01 - £2519.05\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap the confidence interval of income ratio - wfh\n",
    "\n",
    "# Initialize bootstrap analyzer\n",
    "analyzer_income_ratio_wfh_2016 = UKLivingCostBootstrap(\n",
    "    data=df_16,\n",
    "    income_column='p344p-wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_wfh_2017 = UKLivingCostBootstrap(\n",
    "    data=df_17,\n",
    "    income_column='p344p-wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_wfh_2018 = UKLivingCostBootstrap(\n",
    "    data=df_18,\n",
    "    income_column='p344p-wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_wfh_2019 = UKLivingCostBootstrap(\n",
    "    data=df_19,\n",
    "    income_column='p344p-wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_wfh_2020 = UKLivingCostBootstrap(\n",
    "    data=df_20,\n",
    "    income_column='p344p-wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_wfh_2021 = UKLivingCostBootstrap(\n",
    "    data=df_21,\n",
    "    income_column='p344p-wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_wfh_2022 = UKLivingCostBootstrap(\n",
    "    data=df_22,\n",
    "    income_column='p344p-wfh'\n",
    ")\n",
    "\n",
    "# Perform bootstrap analysis\n",
    "bootstrap_results_income_ratio_wfh_2016 = analyzer_income_ratio_wfh_2016.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_wfh_2017 = analyzer_income_ratio_wfh_2017.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_wfh_2018 = analyzer_income_ratio_wfh_2018.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_wfh_2019 = analyzer_income_ratio_wfh_2019.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_wfh_2020 = analyzer_income_ratio_wfh_2020.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_wfh_2021 = analyzer_income_ratio_wfh_2021.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_wfh_2022 = analyzer_income_ratio_wfh_2022.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "\n",
    "# Get summary statistics\n",
    "summary_income_ratio_wfh_2016 = analyzer_income_ratio_wfh_2016.get_summary_statistics()\n",
    "summary_income_ratio_wfh_2017 = analyzer_income_ratio_wfh_2017.get_summary_statistics()\n",
    "summary_income_ratio_wfh_2018 = analyzer_income_ratio_wfh_2018.get_summary_statistics()\n",
    "summary_income_ratio_wfh_2019 = analyzer_income_ratio_wfh_2019.get_summary_statistics()\n",
    "summary_income_ratio_wfh_2020 = analyzer_income_ratio_wfh_2020.get_summary_statistics()\n",
    "summary_income_ratio_wfh_2021 = analyzer_income_ratio_wfh_2021.get_summary_statistics()\n",
    "summary_income_ratio_wfh_2022 = analyzer_income_ratio_wfh_2022.get_summary_statistics()\n",
    "\n",
    "# Store lower and upper threshold of confidence interval\n",
    "ci_lower_income_wfh_2016, ci_upper_income_wfh_2016 = summary_income_ratio_wfh_2016['confidence_intervals']['95%']\n",
    "ci_lower_income_wfh_2017, ci_upper_income_wfh_2017 = summary_income_ratio_wfh_2017['confidence_intervals']['95%']\n",
    "ci_lower_income_wfh_2018, ci_upper_income_wfh_2018 = summary_income_ratio_wfh_2018['confidence_intervals']['95%']\n",
    "ci_lower_income_wfh_2019, ci_upper_income_wfh_2019 = summary_income_ratio_wfh_2019['confidence_intervals']['95%']\n",
    "ci_lower_income_wfh_2020, ci_upper_income_wfh_2020 = summary_income_ratio_wfh_2020['confidence_intervals']['95%']\n",
    "ci_lower_income_wfh_2021, ci_upper_income_wfh_2021 = summary_income_ratio_wfh_2021['confidence_intervals']['95%']\n",
    "ci_lower_income_wfh_2022, ci_upper_income_wfh_2022 = summary_income_ratio_wfh_2022['confidence_intervals']['95%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "34927341-9981-4195-8ab6-89a4778f993a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data shape: (1209, 27)\n",
      "Income range: £43.26 - £2180.93\n",
      "Clean data shape: (1161, 27)\n",
      "Income range: £15.69 - £2342.01\n",
      "Clean data shape: (1315, 27)\n",
      "Income range: £18.87 - £2362.60\n",
      "Clean data shape: (1032, 27)\n",
      "Income range: £40.00 - £2496.18\n",
      "Clean data shape: (1180, 27)\n",
      "Income range: £77.91 - £2420.03\n",
      "Clean data shape: (1252, 27)\n",
      "Income range: £18.51 - £2453.72\n",
      "Clean data shape: (1481, 27)\n",
      "Income range: £54.15 - £2519.05\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Completed 100/1000 bootstrap samples\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 300/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 500/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 700/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 900/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap the confidence interval of income ratio - wfh\n",
    "\n",
    "# Initialize bootstrap analyzer\n",
    "analyzer_income_ratio_non_wfh_2016 = UKLivingCostBootstrap(\n",
    "    data=df_16,\n",
    "    income_column='p344p-non_wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_non_wfh_2017 = UKLivingCostBootstrap(\n",
    "    data=df_17,\n",
    "    income_column='p344p-non_wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_non_wfh_2018 = UKLivingCostBootstrap(\n",
    "    data=df_18,\n",
    "    income_column='p344p-non_wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_non_wfh_2019 = UKLivingCostBootstrap(\n",
    "    data=df_19,\n",
    "    income_column='p344p-non_wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_non_wfh_2020 = UKLivingCostBootstrap(\n",
    "    data=df_20,\n",
    "    income_column='p344p-non_wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_non_wfh_2021 = UKLivingCostBootstrap(\n",
    "    data=df_21,\n",
    "    income_column='p344p-non_wfh'\n",
    ")\n",
    "\n",
    "analyzer_income_ratio_non_wfh_2022 = UKLivingCostBootstrap(\n",
    "    data=df_22,\n",
    "    income_column='p344p-non_wfh'\n",
    ")\n",
    "\n",
    "# Perform bootstrap analysis\n",
    "bootstrap_results_income_ratio_non_wfh_2016 = analyzer_income_ratio_non_wfh_2016.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_non_wfh_2017 = analyzer_income_ratio_non_wfh_2017.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_non_wfh_2018 = analyzer_income_ratio_non_wfh_2018.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_non_wfh_2019 = analyzer_income_ratio_non_wfh_2019.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_non_wfh_2020 = analyzer_income_ratio_non_wfh_2020.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_non_wfh_2021 = analyzer_income_ratio_non_wfh_2021.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "bootstrap_results_income_ratio_non_wfh_2022 = analyzer_income_ratio_non_wfh_2022.bootstrap_sample(n_bootstrap=1000, random_state=42)\n",
    "\n",
    "# Get summary statistics\n",
    "summary_income_ratio_non_wfh_2016 = analyzer_income_ratio_non_wfh_2016.get_summary_statistics()\n",
    "summary_income_ratio_non_wfh_2017 = analyzer_income_ratio_non_wfh_2017.get_summary_statistics()\n",
    "summary_income_ratio_non_wfh_2018 = analyzer_income_ratio_non_wfh_2018.get_summary_statistics()\n",
    "summary_income_ratio_non_wfh_2019 = analyzer_income_ratio_non_wfh_2019.get_summary_statistics()\n",
    "summary_income_ratio_non_wfh_2020 = analyzer_income_ratio_non_wfh_2020.get_summary_statistics()\n",
    "summary_income_ratio_non_wfh_2021 = analyzer_income_ratio_non_wfh_2021.get_summary_statistics()\n",
    "summary_income_ratio_non_wfh_2022 = analyzer_income_ratio_non_wfh_2022.get_summary_statistics()\n",
    "\n",
    "# Store lower and upper threshold of confidence interval\n",
    "ci_lower_income_non_wfh_2016, ci_upper_income_non_wfh_2016 = summary_income_ratio_non_wfh_2016['confidence_intervals']['95%']\n",
    "ci_lower_income_non_wfh_2017, ci_upper_income_non_wfh_2017 = summary_income_ratio_non_wfh_2017['confidence_intervals']['95%']\n",
    "ci_lower_income_non_wfh_2018, ci_upper_income_non_wfh_2018 = summary_income_ratio_non_wfh_2018['confidence_intervals']['95%']\n",
    "ci_lower_income_non_wfh_2019, ci_upper_income_non_wfh_2019 = summary_income_ratio_non_wfh_2019['confidence_intervals']['95%']\n",
    "ci_lower_income_non_wfh_2020, ci_upper_income_non_wfh_2020 = summary_income_ratio_non_wfh_2020['confidence_intervals']['95%']\n",
    "ci_lower_income_non_wfh_2021, ci_upper_income_non_wfh_2021 = summary_income_ratio_non_wfh_2021['confidence_intervals']['95%']\n",
    "ci_lower_income_non_wfh_2022, ci_upper_income_non_wfh_2022 = summary_income_ratio_non_wfh_2022['confidence_intervals']['95%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "06f14d4c-ae29-4cf2-a222-a1fb32ad47ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function to bootstrap log variance of income and calculate confidence interval\n",
    "\n",
    "class UKSurveyBootstrap:\n",
    "    \"\"\"\n",
    "    Bootstrap analysis for UK Living Cost and Food Survey data\n",
    "    focusing on log variance of income with confidence intervals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: Optional[pd.DataFrame] = None, income_column: str = 'income'):\n",
    "        \"\"\"\n",
    "        Initialize the bootstrap analysis\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame, optional\n",
    "            Survey data with income column\n",
    "        income_column : str\n",
    "            Name of the income column in the dataset\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.income_column = income_column\n",
    "        self.bootstrap_results = None\n",
    "        self.original_log_variance = None\n",
    "        \n",
    "    def load_sample_data(self, n_samples: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate sample data mimicking UK Living Cost Survey structure\n",
    "        (Use this if you don't have the actual survey data)\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate realistic income distribution (log-normal)\n",
    "        # Based on UK household income statistics\n",
    "        mean_log_income = 10.5  # ~£36,000 median\n",
    "        std_log_income = 0.7\n",
    "        \n",
    "        income = np.random.lognormal(mean_log_income, std_log_income, n_samples)\n",
    "        \n",
    "        # Add some additional household characteristics\n",
    "        household_size = np.random.choice([1, 2, 3, 4, 5], n_samples, \n",
    "                                        p=[0.3, 0.35, 0.2, 0.1, 0.05])\n",
    "        region = np.random.choice(['London', 'South East', 'North West', 'Yorkshire', 'Other'], \n",
    "                                n_samples, p=[0.15, 0.2, 0.15, 0.15, 0.35])\n",
    "        \n",
    "        sample_data = pd.DataFrame({\n",
    "            'income': income,\n",
    "            'household_size': household_size,\n",
    "            'region': region,\n",
    "            'weight': np.random.uniform(0.8, 1.2, n_samples)  # Survey weights\n",
    "        })\n",
    "        \n",
    "        return sample_data\n",
    "    \n",
    "    def prepare_data(self, remove_zeros: bool = False, min_income: float = 0) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare the data for analysis by handling zero/negative incomes\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        remove_zeros : bool\n",
    "            Whether to remove zero/negative income observations\n",
    "        min_income : float\n",
    "            Minimum income threshold for log transformation\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            print(\"Loading sample data...\")\n",
    "            self.data = self.load_sample_data()\n",
    "        \n",
    "        # Create a copy for processing\n",
    "        clean_data = self.data.copy()\n",
    "        \n",
    "        # Handle zero/negative incomes\n",
    "        if remove_zeros:\n",
    "            initial_count = len(clean_data)\n",
    "            clean_data = clean_data[clean_data[self.income_column] >= min_income]\n",
    "            removed_count = initial_count - len(clean_data)\n",
    "            if removed_count > 0:\n",
    "                print(f\"Removed {removed_count} observations with income < £{min_income}\")\n",
    "        \n",
    "        # Calculate log income\n",
    "        clean_data['log_income'] = np.log(clean_data[self.income_column])\n",
    "        \n",
    "        print(f\"Final sample size: {len(clean_data)} observations\")\n",
    "        print(f\"Income range: £{clean_data[self.income_column].min():.2f} - £{clean_data[self.income_column].max():.2f}\")\n",
    "        \n",
    "        return clean_data\n",
    "    \n",
    "    def calculate_log_variance(self, data: pd.DataFrame, use_weights: bool = True) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the variance of log income\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Data containing log_income column\n",
    "        use_weights : bool\n",
    "            Whether to use survey weights if available\n",
    "        \"\"\"\n",
    "        if use_weights and 'weight' in data.columns:\n",
    "            # Weighted variance calculation\n",
    "            weights = data['weight'].values\n",
    "            log_income = data['log_income'].values\n",
    "            \n",
    "            # Weighted mean\n",
    "            weighted_mean = np.average(log_income, weights=weights)\n",
    "            \n",
    "            # Weighted variance\n",
    "            weighted_variance = np.average((log_income - weighted_mean)**2, weights=weights)\n",
    "            \n",
    "            return weighted_variance\n",
    "        else:\n",
    "            # Simple variance\n",
    "            return data['log_income'].var(ddof=1)\n",
    "    \n",
    "    def bootstrap_analysis(self, n_bootstrap: int = 1000, use_weights: bool = True, \n",
    "                          random_state: int = 42) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform bootstrap resampling and calculate log variance for each sample\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_bootstrap : int\n",
    "            Number of bootstrap samples\n",
    "        use_weights : bool\n",
    "            Whether to use survey weights\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        clean_data = self.prepare_data()\n",
    "        \n",
    "        # Calculate original log variance\n",
    "        self.original_log_variance = self.calculate_log_variance(clean_data, use_weights)\n",
    "        \n",
    "        print(f\"\\nOriginal log variance of income: {self.original_log_variance:.6f}\")\n",
    "        print(f\"Performing {n_bootstrap} bootstrap replications...\")\n",
    "        \n",
    "        # Set random seed\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Bootstrap sampling\n",
    "        n_obs = len(clean_data)\n",
    "        bootstrap_variances = []\n",
    "        \n",
    "        for i in range(n_bootstrap):\n",
    "            # Sample with replacement\n",
    "            if use_weights and 'weight' in clean_data.columns:\n",
    "                # Probability proportional to weights\n",
    "                probs = clean_data['weight'].values / clean_data['weight'].sum()\n",
    "                indices = np.random.choice(n_obs, size=n_obs, replace=True, p=probs)\n",
    "            else:\n",
    "                indices = np.random.choice(n_obs, size=n_obs, replace=True)\n",
    "            \n",
    "            # Create bootstrap sample\n",
    "            bootstrap_sample = clean_data.iloc[indices].copy()\n",
    "            \n",
    "            # Calculate log variance for this bootstrap sample\n",
    "            bootstrap_var = self.calculate_log_variance(bootstrap_sample, use_weights)\n",
    "            bootstrap_variances.append(bootstrap_var)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 200 == 0:\n",
    "                print(f\"Completed {i + 1}/{n_bootstrap} bootstrap samples\")\n",
    "        \n",
    "        self.bootstrap_results = np.array(bootstrap_variances)\n",
    "        return self.bootstrap_results\n",
    "    \n",
    "    def calculate_confidence_interval(self, confidence_level: float = 0.95) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculate confidence interval for log variance\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level (e.g., 0.95 for 95% CI)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple : (lower_bound, upper_bound, standard_error)\n",
    "        \"\"\"\n",
    "        if self.bootstrap_results is None:\n",
    "            raise ValueError(\"Must run bootstrap_analysis first\")\n",
    "        \n",
    "        alpha = 1 - confidence_level\n",
    "        lower_percentile = (alpha / 2) * 100\n",
    "        upper_percentile = (1 - alpha / 2) * 100\n",
    "        \n",
    "        lower_bound = np.percentile(self.bootstrap_results, lower_percentile)\n",
    "        upper_bound = np.percentile(self.bootstrap_results, upper_percentile)\n",
    "        standard_error = np.std(self.bootstrap_results, ddof=1)\n",
    "        \n",
    "        return lower_bound, upper_bound, standard_error\n",
    "    \n",
    "    def print_results(self, confidence_level: float = 0.95):\n",
    "        \"\"\"Print comprehensive results of the bootstrap analysis\"\"\"\n",
    "        if self.bootstrap_results is None:\n",
    "            raise ValueError(\"Must run bootstrap_analysis first\")\n",
    "        \n",
    "        lower_bound, upper_bound, std_error = self.calculate_confidence_interval(confidence_level)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BOOTSTRAP RESULTS - LOG VARIANCE OF INCOME\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Original log variance:           {self.original_log_variance:.6f}\")\n",
    "        print(f\"Bootstrap mean:                  {np.mean(self.bootstrap_results):.6f}\")\n",
    "        print(f\"Bootstrap median:                {np.median(self.bootstrap_results):.6f}\")\n",
    "        print(f\"Bootstrap std error:             {std_error:.6f}\")\n",
    "        print(f\"Bootstrap std deviation:         {np.std(self.bootstrap_results, ddof=1):.6f}\")\n",
    "        print(f\"\\n{confidence_level*100:.0f}% Confidence Interval:\")\n",
    "        print(f\"Lower bound:                     {lower_bound:.6f}\")\n",
    "        print(f\"Upper bound:                     {upper_bound:.6f}\")\n",
    "        print(f\"Interval width:                  {upper_bound - lower_bound:.6f}\")\n",
    "        print(f\"\\nBias (Bootstrap mean - Original): {np.mean(self.bootstrap_results) - self.original_log_variance:.6f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    def plot_results(self, confidence_level: float = 0.95, figsize: Tuple[int, int] = (12, 8)):\n",
    "        \"\"\"Create visualization of bootstrap results\"\"\"\n",
    "        if self.bootstrap_results is None:\n",
    "            raise ValueError(\"Must run bootstrap_analysis first\")\n",
    "        \n",
    "        lower_bound, upper_bound, _ = self.calculate_confidence_interval(confidence_level)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        fig.suptitle('Bootstrap Analysis of Log Variance of Income', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Histogram of bootstrap results\n",
    "        axes[0, 0].hist(self.bootstrap_results, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].axvline(self.original_log_variance, color='red', linestyle='--', \n",
    "                          label=f'Original: {self.original_log_variance:.6f}')\n",
    "        axes[0, 0].axvline(np.mean(self.bootstrap_results), color='green', linestyle='--', \n",
    "                          label=f'Bootstrap Mean: {np.mean(self.bootstrap_results):.6f}')\n",
    "        axes[0, 0].axvline(lower_bound, color='orange', linestyle=':', alpha=0.8, \n",
    "                          label=f'{confidence_level*100:.0f}% CI')\n",
    "        axes[0, 0].axvline(upper_bound, color='orange', linestyle=':', alpha=0.8)\n",
    "        axes[0, 0].set_xlabel('Log Variance')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Distribution of Bootstrap Log Variances')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Q plot to check normality\n",
    "        stats.probplot(self.bootstrap_results, dist=\"norm\", plot=axes[0, 1])\n",
    "        axes[0, 1].set_title('Q-Q Plot: Bootstrap Results vs Normal Distribution')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot\n",
    "        axes[1, 0].boxplot(self.bootstrap_results, vert=True)\n",
    "        axes[1, 0].set_ylabel('Log Variance')\n",
    "        axes[1, 0].set_title('Box Plot of Bootstrap Results')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Convergence plot\n",
    "        cumulative_mean = np.cumsum(self.bootstrap_results) / np.arange(1, len(self.bootstrap_results) + 1)\n",
    "        axes[1, 1].plot(cumulative_mean, color='blue', alpha=0.7)\n",
    "        axes[1, 1].axhline(self.original_log_variance, color='red', linestyle='--', \n",
    "                          label=f'Original: {self.original_log_variance:.6f}')\n",
    "        axes[1, 1].set_xlabel('Bootstrap Sample Number')\n",
    "        axes[1, 1].set_ylabel('Cumulative Mean')\n",
    "        axes[1, 1].set_title('Convergence of Bootstrap Mean')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bfc324eb-7570-49d6-890e-cc2e9b53610b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sample size: 2556 observations\n",
      "Income range: £43.26 - £2180.93\n",
      "\n",
      "Original log variance of income: 0.329808\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2852 observations\n",
      "Income range: £15.69 - £2342.01\n",
      "\n",
      "Original log variance of income: 0.365213\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2893 observations\n",
      "Income range: £18.87 - £2362.60\n",
      "\n",
      "Original log variance of income: 0.359036\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2794 observations\n",
      "Income range: £40.00 - £2496.18\n",
      "\n",
      "Original log variance of income: 0.327504\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2884 observations\n",
      "Income range: £77.91 - £2420.03\n",
      "\n",
      "Original log variance of income: 0.320089\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2984 observations\n",
      "Income range: £18.51 - £2453.72\n",
      "\n",
      "Original log variance of income: 0.317334\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2390 observations\n",
      "Income range: £54.15 - £2519.05\n",
      "\n",
      "Original log variance of income: 0.321158\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap the confidence interval of log variance of income - overall\n",
    "\n",
    "# Initialize the bootstrap analysis\n",
    "analyzer_log_var_income_overall_2016 = UKSurveyBootstrap(df_16, 'p344p')\n",
    "analyzer_log_var_income_overall_2017 = UKSurveyBootstrap(df_17, 'p344p')\n",
    "analyzer_log_var_income_overall_2018 = UKSurveyBootstrap(df_18, 'p344p')\n",
    "analyzer_log_var_income_overall_2019 = UKSurveyBootstrap(df_19, 'p344p')\n",
    "analyzer_log_var_income_overall_2020 = UKSurveyBootstrap(df_20, 'p344p')\n",
    "analyzer_log_var_income_overall_2021 = UKSurveyBootstrap(df_21, 'p344p')\n",
    "analyzer_log_var_income_overall_2022 = UKSurveyBootstrap(df_22, 'p344p')\n",
    "\n",
    "# Run bootstrap analysis\n",
    "bootstrap_log_var_income_overall_2016 = analyzer_log_var_income_overall_2016.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_overall_2017 = analyzer_log_var_income_overall_2017.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_overall_2018 = analyzer_log_var_income_overall_2018.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_overall_2019 = analyzer_log_var_income_overall_2019.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_overall_2020 = analyzer_log_var_income_overall_2020.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_overall_2021 = analyzer_log_var_income_overall_2021.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_overall_2022 = analyzer_log_var_income_overall_2022.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "ci_lower_log_var_income_overall_2016, ci_upper_log_var_income_overall_2016, se = analyzer_log_var_income_overall_2016.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_overall_2017, ci_upper_log_var_income_overall_2017, se = analyzer_log_var_income_overall_2017.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_overall_2018, ci_upper_log_var_income_overall_2018, se = analyzer_log_var_income_overall_2018.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_overall_2019, ci_upper_log_var_income_overall_2019, se = analyzer_log_var_income_overall_2019.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_overall_2020, ci_upper_log_var_income_overall_2020, se = analyzer_log_var_income_overall_2020.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_overall_2021, ci_upper_log_var_income_overall_2021, se = analyzer_log_var_income_overall_2021.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_overall_2022, ci_upper_log_var_income_overall_2022, se = analyzer_log_var_income_overall_2022.calculate_confidence_interval(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2d04f2b0-6d66-49a1-b5f4-4b2305a3dafd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sample size: 2556 observations\n",
      "Income range: £90.00 - £2180.93\n",
      "\n",
      "Original log variance of income: 0.279341\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2852 observations\n",
      "Income range: £120.00 - £2342.01\n",
      "\n",
      "Original log variance of income: 0.292319\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2893 observations\n",
      "Income range: £126.30 - £2362.60\n",
      "\n",
      "Original log variance of income: 0.235984\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2794 observations\n",
      "Income range: £60.58 - £2496.18\n",
      "\n",
      "Original log variance of income: 0.251111\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2884 observations\n",
      "Income range: £126.93 - £2420.03\n",
      "\n",
      "Original log variance of income: 0.254917\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2984 observations\n",
      "Income range: £54.00 - £2453.72\n",
      "\n",
      "Original log variance of income: 0.239218\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2390 observations\n",
      "Income range: £170.01 - £2519.05\n",
      "\n",
      "Original log variance of income: 0.255072\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap the confidence interval of log variance of income - wfh\n",
    "\n",
    "# Initialize the bootstrap analysis\n",
    "analyzer_log_var_income_wfh_2016 = UKSurveyBootstrap(df_16, 'p344p-wfh')\n",
    "analyzer_log_var_income_wfh_2017 = UKSurveyBootstrap(df_17, 'p344p-wfh')\n",
    "analyzer_log_var_income_wfh_2018 = UKSurveyBootstrap(df_18, 'p344p-wfh')\n",
    "analyzer_log_var_income_wfh_2019 = UKSurveyBootstrap(df_19, 'p344p-wfh')\n",
    "analyzer_log_var_income_wfh_2020 = UKSurveyBootstrap(df_20, 'p344p-wfh')\n",
    "analyzer_log_var_income_wfh_2021 = UKSurveyBootstrap(df_21, 'p344p-wfh')\n",
    "analyzer_log_var_income_wfh_2022 = UKSurveyBootstrap(df_22, 'p344p-wfh')\n",
    "\n",
    "# Run bootstrap analysis\n",
    "bootstrap_log_var_income_wfh_2016 = analyzer_log_var_income_wfh_2016.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_wfh_2017 = analyzer_log_var_income_wfh_2017.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_wfh_2018 = analyzer_log_var_income_wfh_2018.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_wfh_2019 = analyzer_log_var_income_wfh_2019.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_wfh_2020 = analyzer_log_var_income_wfh_2020.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_wfh_2021 = analyzer_log_var_income_wfh_2021.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_wfh_2022 = analyzer_log_var_income_wfh_2022.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "ci_lower_log_var_income_wfh_2016, ci_upper_log_var_income_wfh_2016, se = analyzer_log_var_income_wfh_2016.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_wfh_2017, ci_upper_log_var_income_wfh_2017, se = analyzer_log_var_income_wfh_2017.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_wfh_2018, ci_upper_log_var_income_wfh_2018, se = analyzer_log_var_income_wfh_2018.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_wfh_2019, ci_upper_log_var_income_wfh_2019, se = analyzer_log_var_income_wfh_2019.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_wfh_2020, ci_upper_log_var_income_wfh_2020, se = analyzer_log_var_income_wfh_2020.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_wfh_2021, ci_upper_log_var_income_wfh_2021, se = analyzer_log_var_income_wfh_2021.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_wfh_2022, ci_upper_log_var_income_wfh_2022, se = analyzer_log_var_income_wfh_2022.calculate_confidence_interval(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3b08be9e-17ea-42cf-8868-7c140553ae20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sample size: 2556 observations\n",
      "Income range: £43.26 - £2180.93\n",
      "\n",
      "Original log variance of income: 0.322694\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2852 observations\n",
      "Income range: £15.69 - £2342.01\n",
      "\n",
      "Original log variance of income: 0.336677\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2893 observations\n",
      "Income range: £18.87 - £2362.60\n",
      "\n",
      "Original log variance of income: 0.322500\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2794 observations\n",
      "Income range: £40.00 - £2496.18\n",
      "\n",
      "Original log variance of income: 0.269858\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2884 observations\n",
      "Income range: £77.91 - £2420.03\n",
      "\n",
      "Original log variance of income: 0.333722\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2984 observations\n",
      "Income range: £18.51 - £2453.72\n",
      "\n",
      "Original log variance of income: 0.330328\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n",
      "Final sample size: 2390 observations\n",
      "Income range: £54.15 - £2519.05\n",
      "\n",
      "Original log variance of income: 0.323385\n",
      "Performing 1000 bootstrap replications...\n",
      "Completed 200/1000 bootstrap samples\n",
      "Completed 400/1000 bootstrap samples\n",
      "Completed 600/1000 bootstrap samples\n",
      "Completed 800/1000 bootstrap samples\n",
      "Completed 1000/1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap the confidence interval of log variance of income - non_wfh\n",
    "\n",
    "# Initialize the bootstrap analysis\n",
    "analyzer_log_var_income_non_wfh_2016 = UKSurveyBootstrap(df_16, 'p344p-non_wfh')\n",
    "analyzer_log_var_income_non_wfh_2017 = UKSurveyBootstrap(df_17, 'p344p-non_wfh')\n",
    "analyzer_log_var_income_non_wfh_2018 = UKSurveyBootstrap(df_18, 'p344p-non_wfh')\n",
    "analyzer_log_var_income_non_wfh_2019 = UKSurveyBootstrap(df_19, 'p344p-non_wfh')\n",
    "analyzer_log_var_income_non_wfh_2020 = UKSurveyBootstrap(df_20, 'p344p-non_wfh')\n",
    "analyzer_log_var_income_non_wfh_2021 = UKSurveyBootstrap(df_21, 'p344p-non_wfh')\n",
    "analyzer_log_var_income_non_wfh_2022 = UKSurveyBootstrap(df_22, 'p344p-non_wfh')\n",
    "\n",
    "# Run bootstrap analysis\n",
    "bootstrap_log_var_income_non_wfh_2016 = analyzer_log_var_income_non_wfh_2016.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_non_wfh_2017 = analyzer_log_var_income_non_wfh_2017.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_non_wfh_2018 = analyzer_log_var_income_non_wfh_2018.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_non_wfh_2019 = analyzer_log_var_income_non_wfh_2019.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_non_wfh_2020 = analyzer_log_var_income_non_wfh_2020.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_non_wfh_2021 = analyzer_log_var_income_non_wfh_2021.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "bootstrap_log_var_income_non_wfh_2022 = analyzer_log_var_income_non_wfh_2022.bootstrap_analysis(n_bootstrap=1000, use_weights=True)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "ci_lower_log_var_income_non_wfh_2016, ci_upper_log_var_income_non_wfh_2016, se = analyzer_log_var_income_non_wfh_2016.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_non_wfh_2017, ci_upper_log_var_income_non_wfh_2017, se = analyzer_log_var_income_non_wfh_2017.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_non_wfh_2018, ci_upper_log_var_income_non_wfh_2018, se = analyzer_log_var_income_non_wfh_2018.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_non_wfh_2019, ci_upper_log_var_income_non_wfh_2019, se = analyzer_log_var_income_non_wfh_2019.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_non_wfh_2020, ci_upper_log_var_income_non_wfh_2020, se = analyzer_log_var_income_non_wfh_2020.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_non_wfh_2021, ci_upper_log_var_income_non_wfh_2021, se = analyzer_log_var_income_non_wfh_2021.calculate_confidence_interval(0.95)\n",
    "ci_lower_log_var_income_non_wfh_2022, ci_upper_log_var_income_non_wfh_2022, se = analyzer_log_var_income_non_wfh_2022.calculate_confidence_interval(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57501527-265f-4e74-b23e-004d42a351f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function to bootstrap average consumption measure\n",
    "\n",
    "class UKLivingCostBootstrap:\n",
    "    \"\"\"\n",
    "    A class to perform bootstrap analysis on UK Living Cost and Food Survey data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, consumption_column: str = 'total_consumption'):\n",
    "        \"\"\"\n",
    "        Initialize the bootstrap analysis\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            The UK Living Cost and Food Survey data\n",
    "        consumption_column : str\n",
    "            Name of the column containing consumption data\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.consumption_column = consumption_column\n",
    "        self.bootstrap_means = None\n",
    "        self.original_mean = None\n",
    "        \n",
    "    def load_sample_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create sample UK Living Cost and Food Survey data for demonstration\n",
    "        (In practice, you would load this from ONS data files)\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        n_households = 1000\n",
    "        \n",
    "        # Simulate realistic UK household consumption data\n",
    "        sample_data = pd.DataFrame({\n",
    "            'household_id': range(1, n_households + 1),\n",
    "            'total_consumption': np.random.lognormal(mean=9.5, sigma=0.6, size=n_households),\n",
    "            'food_consumption': np.random.lognormal(mean=7.8, sigma=0.4, size=n_households),\n",
    "            'housing_consumption': np.random.lognormal(mean=8.2, sigma=0.5, size=n_households),\n",
    "            'household_size': np.random.choice([1, 2, 3, 4, 5, 6], size=n_households, \n",
    "                                            p=[0.3, 0.35, 0.2, 0.1, 0.04, 0.01]),\n",
    "            'region': np.random.choice(['London', 'South East', 'North West', 'Scotland', \n",
    "                                     'Wales', 'Other'], size=n_households,\n",
    "                                    p=[0.15, 0.2, 0.15, 0.1, 0.05, 0.35])\n",
    "        })\n",
    "        \n",
    "        return sample_data\n",
    "    \n",
    "    def bootstrap_sample(self, n_bootstrap: int = 1000) -> List[float]:\n",
    "        \"\"\"\n",
    "        Perform bootstrap sampling and calculate means\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_bootstrap : int\n",
    "            Number of bootstrap samples to generate\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        List[float]\n",
    "            List of bootstrap sample means\n",
    "        \"\"\"\n",
    "        consumption_data = self.data[self.consumption_column].dropna()\n",
    "        self.original_mean = consumption_data.mean()\n",
    "        \n",
    "        bootstrap_means = []\n",
    "        n_samples = len(consumption_data)\n",
    "        \n",
    "        print(f\"Performing {n_bootstrap} bootstrap samples...\")\n",
    "        print(f\"Original sample size: {n_samples}\")\n",
    "        print(f\"Original mean consumption: {self.original_mean:.2f}\")\n",
    "        \n",
    "        for i in range(n_bootstrap):\n",
    "            # Bootstrap sample with replacement\n",
    "            bootstrap_sample = np.random.choice(consumption_data, size=n_samples, replace=True)\n",
    "            bootstrap_mean = np.mean(bootstrap_sample)\n",
    "            bootstrap_means.append(bootstrap_mean)\n",
    "            \n",
    "            if (i + 1) % 200 == 0:\n",
    "                print(f\"Completed {i + 1} bootstrap samples\")\n",
    "        \n",
    "        self.bootstrap_means = bootstrap_means\n",
    "        return bootstrap_means\n",
    "    \n",
    "    def calculate_confidence_interval(self, confidence_level: float = 0.95) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate confidence interval from bootstrap results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level (default 0.95 for 95% CI)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Tuple[float, float]\n",
    "            Lower and upper bounds of confidence interval\n",
    "        \"\"\"\n",
    "        if self.bootstrap_means is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        alpha = 1 - confidence_level\n",
    "        lower_percentile = (alpha/2) * 100\n",
    "        upper_percentile = (1 - alpha/2) * 100\n",
    "        \n",
    "        ci_lower = np.percentile(self.bootstrap_means, lower_percentile)\n",
    "        ci_upper = np.percentile(self.bootstrap_means, upper_percentile)\n",
    "        \n",
    "        return ci_lower, ci_upper\n",
    "    \n",
    "    def analyze_bootstrap_results(self, confidence_level: float = 0.95) -> dict:\n",
    "        \"\"\"\n",
    "        Comprehensive analysis of bootstrap results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level for CI calculation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing all analysis results\n",
    "        \"\"\"\n",
    "        if self.bootstrap_means is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        ci_lower, ci_upper = self.calculate_confidence_interval(confidence_level)\n",
    "        \n",
    "        results = {\n",
    "            'original_mean': self.original_mean,\n",
    "            'bootstrap_mean': np.mean(self.bootstrap_means),\n",
    "            'bootstrap_std': np.std(self.bootstrap_means),\n",
    "            'bootstrap_se': np.std(self.bootstrap_means),  # Standard error\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'confidence_level': confidence_level,\n",
    "            'n_bootstrap': len(self.bootstrap_means),\n",
    "            'bias': np.mean(self.bootstrap_means) - self.original_mean\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_bootstrap_distribution(self, confidence_level: float = 0.95) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Plot the bootstrap distribution with confidence intervals\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level for CI visualization\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        plt.Figure\n",
    "            Matplotlib figure object\n",
    "        \"\"\"\n",
    "        if self.bootstrap_means is None:\n",
    "            raise ValueError(\"Must run bootstrap_sample() first\")\n",
    "        \n",
    "        ci_lower, ci_upper = self.calculate_confidence_interval(confidence_level)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Histogram of bootstrap means\n",
    "        ax1.hist(self.bootstrap_means, bins=50, density=True, alpha=0.7, color='skyblue', \n",
    "                edgecolor='black', label='Bootstrap Distribution')\n",
    "        ax1.axvline(self.original_mean, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Original Mean: {self.original_mean:.2f}')\n",
    "        ax1.axvline(ci_lower, color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'{confidence_level*100}% CI Lower: {ci_lower:.2f}')\n",
    "        ax1.axvline(ci_upper, color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'{confidence_level*100}% CI Upper: {ci_upper:.2f}')\n",
    "        ax1.set_xlabel('Mean Consumption (£)')\n",
    "        ax1.set_ylabel('Density')\n",
    "        ax1.set_title('Bootstrap Distribution of Sample Means')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Q plot to check normality\n",
    "        stats.probplot(self.bootstrap_means, dist=\"norm\", plot=ax2)\n",
    "        ax2.set_title('Q-Q Plot: Bootstrap Means vs Normal Distribution')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def print_summary_report(self, confidence_level: float = 0.95) -> None:\n",
    "        \"\"\"\n",
    "        Print a comprehensive summary report\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        confidence_level : float\n",
    "            Confidence level for reporting\n",
    "        \"\"\"\n",
    "        results = self.analyze_bootstrap_results(confidence_level)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"UK LIVING COST AND FOOD SURVEY - BOOTSTRAP ANALYSIS REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nSample Information:\")\n",
    "        print(f\"  Original sample size: {len(self.data)}\")\n",
    "        print(f\"  Number of bootstrap samples: {results['n_bootstrap']}\")\n",
    "        print(f\"  Consumption variable: {self.consumption_column}\")\n",
    "        \n",
    "        print(f\"\\nConsumption Statistics:\")\n",
    "        print(f\"  Original sample mean: {results['original_mean']:.2f}\")\n",
    "        print(f\"  Bootstrap mean: {results['bootstrap_mean']:.2f}\")\n",
    "        print(f\"  Bootstrap standard error: {results['bootstrap_se']:.2f}\")\n",
    "        print(f\"  Bias (Bootstrap - Original): {results['bias']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nConfidence Interval ({confidence_level*100}%):\")\n",
    "        print(f\"  Lower bound: {results['ci_lower']:.2f}\")\n",
    "        print(f\"  Upper bound: {results['ci_upper']:.2f}\")\n",
    "        print(f\"  Interval width: {results['ci_upper'] - results['ci_lower']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"  We are {confidence_level*100}% confident that the true population mean\")\n",
    "        print(f\"  consumption lies between {results['ci_lower']:.2f} and £{results['ci_upper']:.2f}\")\n",
    "        \n",
    "        if abs(results['bias']) > 0.01:\n",
    "            print(f\"\\nNote: Bootstrap bias of {results['bias']:.2f} detected.\")\n",
    "            print(f\"  Bias-corrected estimate: {results['original_mean'] - results['bias']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6e3f467f-7f82-42cf-bdca-c178175990fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2542\n",
      "Original mean consumption: 169.67\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2841\n",
      "Original mean consumption: 173.46\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2876\n",
      "Original mean consumption: 154.64\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2783\n",
      "Original mean consumption: 147.32\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2871\n",
      "Original mean consumption: 86.89\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2973\n",
      "Original mean consumption: 129.39\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 2377\n",
      "Original mean consumption: 169.11\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# bootstrap the confidence interval for average consumption measure - overall\n",
    "\n",
    "# Initialize analysis\n",
    "analyser_consumption_overall_2016 = UKLivingCostBootstrap(df_16, consumption_column='derived_consumption')\n",
    "analyser_consumption_overall_2017 = UKLivingCostBootstrap(df_17, consumption_column='derived_consumption')\n",
    "analyser_consumption_overall_2018 = UKLivingCostBootstrap(df_18, consumption_column='derived_consumption')\n",
    "analyser_consumption_overall_2019 = UKLivingCostBootstrap(df_19, consumption_column='derived_consumption')\n",
    "analyser_consumption_overall_2020 = UKLivingCostBootstrap(df_20, consumption_column='derived_consumption')\n",
    "analyser_consumption_overall_2021 = UKLivingCostBootstrap(df_21, consumption_column='derived_consumption')\n",
    "analyser_consumption_overall_2022 = UKLivingCostBootstrap(df_22, consumption_column='derived_consumption')\n",
    "\n",
    "# Run bootstrap (1000 samples)\n",
    "bootstrap_consumption_means_overall_2016 = analyser_consumption_overall_2016.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_overall_2017 = analyser_consumption_overall_2017.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_overall_2018 = analyser_consumption_overall_2018.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_overall_2019 = analyser_consumption_overall_2019.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_overall_2020 = analyser_consumption_overall_2020.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_overall_2021 = analyser_consumption_overall_2021.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_overall_2022 = analyser_consumption_overall_2022.bootstrap_sample(n_bootstrap=1000)\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower_consumption_overall_2016, ci_upper_consumption_overall_2016 = analyser_consumption_overall_2016.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_overall_2017, ci_upper_consumption_overall_2017 = analyser_consumption_overall_2017.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_overall_2018, ci_upper_consumption_overall_2018 = analyser_consumption_overall_2018.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_overall_2019, ci_upper_consumption_overall_2019 = analyser_consumption_overall_2019.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_overall_2020, ci_upper_consumption_overall_2020 = analyser_consumption_overall_2020.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_overall_2021, ci_upper_consumption_overall_2021 = analyser_consumption_overall_2021.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_overall_2022, ci_upper_consumption_overall_2022 = analyser_consumption_overall_2022.calculate_confidence_interval(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1d879f5a-509a-4e8f-b748-52ab946f664e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1340\n",
      "Original mean consumption: 178.94\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1683\n",
      "Original mean consumption: 187.47\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1572\n",
      "Original mean consumption: 165.15\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1756\n",
      "Original mean consumption: 162.65\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1696\n",
      "Original mean consumption: 90.18\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1726\n",
      "Original mean consumption: 145.40\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 907\n",
      "Original mean consumption: 203.26\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# bootstrap the confidence interval for average consumption measure - wfh\n",
    "\n",
    "# Initialize analysis\n",
    "analyser_consumption_wfh_2016 = UKLivingCostBootstrap(df_16, consumption_column='derived_consumption-wfh')\n",
    "analyser_consumption_wfh_2017 = UKLivingCostBootstrap(df_17, consumption_column='derived_consumption-wfh')\n",
    "analyser_consumption_wfh_2018 = UKLivingCostBootstrap(df_18, consumption_column='derived_consumption-wfh')\n",
    "analyser_consumption_wfh_2019 = UKLivingCostBootstrap(df_19, consumption_column='derived_consumption-wfh')\n",
    "analyser_consumption_wfh_2020 = UKLivingCostBootstrap(df_20, consumption_column='derived_consumption-wfh')\n",
    "analyser_consumption_wfh_2021 = UKLivingCostBootstrap(df_21, consumption_column='derived_consumption-wfh')\n",
    "analyser_consumption_wfh_2022 = UKLivingCostBootstrap(df_22, consumption_column='derived_consumption-wfh')\n",
    "\n",
    "# Run bootstrap (1000 samples)\n",
    "bootstrap_consumption_means_wfh_2016 = analyser_consumption_wfh_2016.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_wfh_2017 = analyser_consumption_wfh_2017.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_wfh_2018 = analyser_consumption_wfh_2018.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_wfh_2019 = analyser_consumption_wfh_2019.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_wfh_2020 = analyser_consumption_wfh_2020.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_wfh_2021 = analyser_consumption_wfh_2021.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_wfh_2022 = analyser_consumption_wfh_2022.bootstrap_sample(n_bootstrap=1000)\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower_consumption_wfh_2016, ci_upper_consumption_wfh_2016 = analyser_consumption_wfh_2016.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_wfh_2017, ci_upper_consumption_wfh_2017 = analyser_consumption_wfh_2017.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_wfh_2018, ci_upper_consumption_wfh_2018 = analyser_consumption_wfh_2018.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_wfh_2019, ci_upper_consumption_wfh_2019 = analyser_consumption_wfh_2019.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_wfh_2020, ci_upper_consumption_wfh_2020 = analyser_consumption_wfh_2020.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_wfh_2021, ci_upper_consumption_wfh_2021 = analyser_consumption_wfh_2021.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_wfh_2022, ci_upper_consumption_wfh_2022 = analyser_consumption_wfh_2022.calculate_confidence_interval(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1bee63f8-5be1-48fc-9be3-4422c6c42207",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1202\n",
      "Original mean consumption: 159.33\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1158\n",
      "Original mean consumption: 153.09\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1304\n",
      "Original mean consumption: 141.98\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1027\n",
      "Original mean consumption: 121.11\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1175\n",
      "Original mean consumption: 82.14\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1247\n",
      "Original mean consumption: 107.23\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n",
      "Performing 1000 bootstrap samples...\n",
      "Original sample size: 1470\n",
      "Original mean consumption: 148.05\n",
      "Completed 200 bootstrap samples\n",
      "Completed 400 bootstrap samples\n",
      "Completed 600 bootstrap samples\n",
      "Completed 800 bootstrap samples\n",
      "Completed 1000 bootstrap samples\n"
     ]
    }
   ],
   "source": [
    "# bootstrap the confidence interval for average consumption measure - non_wfh\n",
    "\n",
    "# Initialize analysis\n",
    "analyser_consumption_non_wfh_2016 = UKLivingCostBootstrap(df_16, consumption_column='derived_consumption-non_wfh')\n",
    "analyser_consumption_non_wfh_2017 = UKLivingCostBootstrap(df_17, consumption_column='derived_consumption-non_wfh')\n",
    "analyser_consumption_non_wfh_2018 = UKLivingCostBootstrap(df_18, consumption_column='derived_consumption-non_wfh')\n",
    "analyser_consumption_non_wfh_2019 = UKLivingCostBootstrap(df_19, consumption_column='derived_consumption-non_wfh')\n",
    "analyser_consumption_non_wfh_2020 = UKLivingCostBootstrap(df_20, consumption_column='derived_consumption-non_wfh')\n",
    "analyser_consumption_non_wfh_2021 = UKLivingCostBootstrap(df_21, consumption_column='derived_consumption-non_wfh')\n",
    "analyser_consumption_non_wfh_2022 = UKLivingCostBootstrap(df_22, consumption_column='derived_consumption-non_wfh')\n",
    "\n",
    "# Run bootstrap (1000 samples)\n",
    "bootstrap_consumption_means_non_wfh_2016 = analyser_consumption_non_wfh_2016.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_non_wfh_2017 = analyser_consumption_non_wfh_2017.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_non_wfh_2018 = analyser_consumption_non_wfh_2018.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_non_wfh_2019 = analyser_consumption_non_wfh_2019.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_non_wfh_2020 = analyser_consumption_non_wfh_2020.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_non_wfh_2021 = analyser_consumption_non_wfh_2021.bootstrap_sample(n_bootstrap=1000)\n",
    "bootstrap_consumption_means_non_wfh_2022 = analyser_consumption_non_wfh_2022.bootstrap_sample(n_bootstrap=1000)\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower_consumption_non_wfh_2016, ci_upper_consumption_non_wfh_2016 = analyser_consumption_non_wfh_2016.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_non_wfh_2017, ci_upper_consumption_non_wfh_2017 = analyser_consumption_non_wfh_2017.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_non_wfh_2018, ci_upper_consumption_non_wfh_2018 = analyser_consumption_non_wfh_2018.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_non_wfh_2019, ci_upper_consumption_non_wfh_2019 = analyser_consumption_non_wfh_2019.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_non_wfh_2020, ci_upper_consumption_non_wfh_2020 = analyser_consumption_non_wfh_2020.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_non_wfh_2021, ci_upper_consumption_non_wfh_2021 = analyser_consumption_non_wfh_2021.calculate_confidence_interval(0.95)\n",
    "ci_lower_consumption_non_wfh_2022, ci_upper_consumption_non_wfh_2022 = analyser_consumption_non_wfh_2022.calculate_confidence_interval(0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2f1213ac-57c4-4b78-99b3-c63cb3b4dc8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function to bootstrap log variance of consumption measure\n",
    "\n",
    "class LCFSBootstrap:\n",
    "    \"\"\"\n",
    "    Bootstrap analysis for UK Living Cost and Food Survey data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, consumption_col: str = 'total_consumption'):\n",
    "        \"\"\"\n",
    "        Initialize the bootstrap analysis\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            The LCFS dataset with household ID as index\n",
    "        consumption_col : str\n",
    "            Column name for total consumption\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.consumption_col = consumption_col\n",
    "        \n",
    "        # Clean data and handle missing values\n",
    "        self.data = self.data.dropna(subset=[consumption_col])\n",
    "        self.data = self.data[self.data[consumption_col] > 0]  # Ensure positive consumption\n",
    "        \n",
    "        print(f\"Dataset loaded with {len(self.data)} observations\")\n",
    "        print(f\"Consumption range: £{self.data[consumption_col].min():.2f} - £{self.data[consumption_col].max():.2f}\")\n",
    "    \n",
    "    def calculate_log_variance(self, consumption_data: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate log variance of consumption\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        consumption_data : np.ndarray\n",
    "            Consumption values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Log variance of consumption\n",
    "        \"\"\"\n",
    "        # Take log of consumption\n",
    "        log_consumption = np.log(consumption_data)\n",
    "        \n",
    "        # Unweighted variance\n",
    "        return np.var(log_consumption, ddof=1)\n",
    "    \n",
    "    def bootstrap_sample(self, n_bootstrap: int = 1000, random_state: int = 42) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate bootstrap samples and calculate log variance for each\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_bootstrap : int\n",
    "            Number of bootstrap samples\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Array of bootstrap log variance estimates\n",
    "        \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Get the original data\n",
    "        consumption = self.data[self.consumption_col].values\n",
    "        n_obs = len(consumption)\n",
    "        \n",
    "        bootstrap_results = []\n",
    "        \n",
    "        print(f\"Performing {n_bootstrap} bootstrap samples...\")\n",
    "        \n",
    "        for i in range(n_bootstrap):\n",
    "            # Create bootstrap sample (sampling with replacement by index)\n",
    "            bootstrap_indices = np.random.choice(n_obs, size=n_obs, replace=True)\n",
    "            \n",
    "            # Get bootstrap sample using iloc for positional indexing\n",
    "            bootstrap_consumption = consumption[bootstrap_indices]\n",
    "            \n",
    "            # Calculate log variance for this bootstrap sample\n",
    "            log_var = self.calculate_log_variance(bootstrap_consumption)\n",
    "            bootstrap_results.append(log_var)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"  Completed {i + 1}/{n_bootstrap} samples\")\n",
    "        \n",
    "        return np.array(bootstrap_results)\n",
    "    \n",
    "    def calculate_confidence_interval(self, bootstrap_results: np.ndarray, \n",
    "                                    confidence_level: float = 0.95) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate confidence interval from bootstrap results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        bootstrap_results : np.ndarray\n",
    "            Bootstrap estimates\n",
    "        confidence_level : float\n",
    "            Confidence level (default 0.95 for 95% CI)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Tuple[float, float]\n",
    "            Lower and upper bounds of confidence interval\n",
    "        \"\"\"\n",
    "        alpha = 1 - confidence_level\n",
    "        lower_percentile = (alpha / 2) * 100\n",
    "        upper_percentile = (1 - alpha / 2) * 100\n",
    "        \n",
    "        lower_bound = np.percentile(bootstrap_results, lower_percentile)\n",
    "        upper_bound = np.percentile(bootstrap_results, upper_percentile)\n",
    "        \n",
    "        return lower_bound, upper_bound\n",
    "    \n",
    "    def run_analysis(self, n_bootstrap: int = 1000, confidence_level: float = 0.95, \n",
    "                    random_state: int = 42) -> dict:\n",
    "        \"\"\"\n",
    "        Run complete bootstrap analysis\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_bootstrap : int\n",
    "            Number of bootstrap samples\n",
    "        confidence_level : float\n",
    "            Confidence level for CI\n",
    "        random_state : int\n",
    "            Random seed\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing all results\n",
    "        \"\"\"\n",
    "        # Original log variance\n",
    "        original_consumption = self.data[self.consumption_col].values\n",
    "        original_log_var = self.calculate_log_variance(original_consumption)\n",
    "        \n",
    "        # Bootstrap analysis\n",
    "        bootstrap_results = self.bootstrap_sample(n_bootstrap, random_state)\n",
    "        \n",
    "        # Confidence interval\n",
    "        ci_lower, ci_upper = self.calculate_confidence_interval(bootstrap_results, confidence_level)\n",
    "        \n",
    "        # Summary statistics\n",
    "        results = {\n",
    "            'original_log_variance': original_log_var,\n",
    "            'bootstrap_mean': np.mean(bootstrap_results),\n",
    "            'bootstrap_std': np.std(bootstrap_results),\n",
    "            'bootstrap_results': bootstrap_results,\n",
    "            'confidence_interval': (ci_lower, ci_upper),\n",
    "            'confidence_level': confidence_level,\n",
    "            'n_bootstrap': n_bootstrap,\n",
    "            'n_observations': len(self.data)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_results(self, results: dict, save_fig: bool = False, filename: str = 'lcfs_bootstrap.png'):\n",
    "        \"\"\"\n",
    "        Create visualization of bootstrap results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        results : dict\n",
    "            Results from run_analysis\n",
    "        save_fig : bool\n",
    "            Whether to save the figure\n",
    "        filename : str\n",
    "            Filename for saving\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Bootstrap distribution\n",
    "        axes[0, 0].hist(results['bootstrap_results'], bins=50, alpha=0.7, color='skyblue', \n",
    "                       edgecolor='black', density=True)\n",
    "        axes[0, 0].axvline(results['original_log_variance'], color='red', linestyle='--', \n",
    "                          label=f'Original: {results[\"original_log_variance\"]:.4f}')\n",
    "        axes[0, 0].axvline(results['bootstrap_mean'], color='green', linestyle='--', \n",
    "                          label=f'Bootstrap Mean: {results[\"bootstrap_mean\"]:.4f}')\n",
    "        axes[0, 0].set_xlabel('Log Variance')\n",
    "        axes[0, 0].set_ylabel('Density')\n",
    "        axes[0, 0].set_title('Bootstrap Distribution of Log Variance')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Q plot to check normality\n",
    "        stats.probplot(results['bootstrap_results'], dist=\"norm\", plot=axes[0, 1])\n",
    "        axes[0, 1].set_title('Q-Q Plot: Bootstrap Results vs Normal Distribution')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confidence interval visualization\n",
    "        ci_lower, ci_upper = results['confidence_interval']\n",
    "        axes[1, 0].hist(results['bootstrap_results'], bins=50, alpha=0.7, color='lightcoral', \n",
    "                       edgecolor='black', density=True)\n",
    "        axes[1, 0].axvline(ci_lower, color='red', linestyle='-', linewidth=2, \n",
    "                          label=f'CI Lower: {ci_lower:.4f}')\n",
    "        axes[1, 0].axvline(ci_upper, color='red', linestyle='-', linewidth=2, \n",
    "                          label=f'CI Upper: {ci_upper:.4f}')\n",
    "        axes[1, 0].axvspan(ci_lower, ci_upper, alpha=0.2, color='red', \n",
    "                          label=f'{results[\"confidence_level\"]*100:.0f}% CI')\n",
    "        axes[1, 0].set_xlabel('Log Variance')\n",
    "        axes[1, 0].set_ylabel('Density')\n",
    "        axes[1, 0].set_title(f'{results[\"confidence_level\"]*100:.0f}% Confidence Interval')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Original consumption distribution\n",
    "        axes[1, 1].hist(np.log(self.data[self.consumption_col]), bins=50, alpha=0.7, \n",
    "                       color='lightgreen', edgecolor='black', density=True)\n",
    "        axes[1, 1].set_xlabel('Log Consumption')\n",
    "        axes[1, 1].set_ylabel('Density')\n",
    "        axes[1, 1].set_title('Distribution of Log Consumption (Original Data)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_fig:\n",
    "            plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Figure saved as {filename}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def print_summary(self, results: dict):\n",
    "        \"\"\"\n",
    "        Print summary of results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        results : dict\n",
    "            Results from run_analysis\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BOOTSTRAP ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Number of observations: {results['n_observations']:,}\")\n",
    "        print(f\"Number of bootstrap samples: {results['n_bootstrap']:,}\")\n",
    "        print(f\"Confidence level: {results['confidence_level']*100:.1f}%\")\n",
    "        print()\n",
    "        print(\"LOG VARIANCE ESTIMATES:\")\n",
    "        print(f\"  Original sample: {results['original_log_variance']:.6f}\")\n",
    "        print(f\"  Bootstrap mean:  {results['bootstrap_mean']:.6f}\")\n",
    "        print(f\"  Bootstrap std:   {results['bootstrap_std']:.6f}\")\n",
    "        print()\n",
    "        print(\"CONFIDENCE INTERVAL:\")\n",
    "        ci_lower, ci_upper = results['confidence_interval']\n",
    "        print(f\"  Lower bound: {ci_lower:.6f}\")\n",
    "        print(f\"  Upper bound: {ci_upper:.6f}\")\n",
    "        print(f\"  Width:       {ci_upper - ci_lower:.6f}\")\n",
    "        print()\n",
    "        print(\"BIAS ANALYSIS:\")\n",
    "        bias = results['bootstrap_mean'] - results['original_log_variance']\n",
    "        print(f\"  Bootstrap bias: {bias:.6f}\")\n",
    "        print(f\"  Relative bias:  {bias/results['original_log_variance']*100:.2f}%\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "\n",
    "def load_sample_data(n_households: int = 5000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate sample data that mimics UK LCFS structure with household ID as index\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_households : int\n",
    "        Number of households to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Sample dataset with household_id as index\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate household characteristics\n",
    "    household_size = np.random.choice([1, 2, 3, 4, 5, 6], n_households, \n",
    "                                     p=[0.28, 0.35, 0.16, 0.16, 0.04, 0.01])\n",
    "    \n",
    "    # Income affects consumption (log-normal distribution)\n",
    "    base_income = np.random.lognormal(mean=10.2, sigma=0.6, size=n_households)\n",
    "    \n",
    "    # Consumption as function of income and household size\n",
    "    consumption = (base_income * 0.7 * \n",
    "                  (1 + 0.2 * np.log(household_size)) * \n",
    "                  np.random.lognormal(0, 0.3, n_households))\n",
    "    \n",
    "    # Survey weights (some households over/under-represented) - REMOVED\n",
    "    # weights = np.random.gamma(2, 0.5, n_households)\n",
    "    \n",
    "    # Regional dummy\n",
    "    region = np.random.choice(['London', 'South East', 'North', 'Midlands', 'Wales', 'Scotland'], \n",
    "                             n_households, p=[0.15, 0.20, 0.25, 0.20, 0.10, 0.10])\n",
    "    \n",
    "    # Create household IDs\n",
    "    household_ids = range(1, n_households + 1)\n",
    "    \n",
    "    # Create DataFrame with household_id as index\n",
    "    data = pd.DataFrame({\n",
    "        'household_size': household_size,\n",
    "        'gross_income': base_income,\n",
    "        'total_consumption': consumption,\n",
    "        'region': region\n",
    "    }, index=household_ids)\n",
    "    \n",
    "    # Name the index\n",
    "    data.index.name = 'household_id'\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6b82ae55-a92c-42dc-ac00-0d4ed01faec6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 2531 observations\n",
      "Consumption range: £0.23 - £29607.72\n",
      "Dataset loaded with 2829 observations\n",
      "Consumption range: £0.78 - £8545.83\n",
      "Dataset loaded with 2857 observations\n",
      "Consumption range: £0.29 - £15149.09\n",
      "Dataset loaded with 2773 observations\n",
      "Consumption range: £1.19 - £12744.45\n",
      "Dataset loaded with 2855 observations\n",
      "Consumption range: £0.31 - £5003.84\n",
      "Dataset loaded with 2949 observations\n",
      "Consumption range: £2.08 - £20795.05\n",
      "Dataset loaded with 2346 observations\n",
      "Consumption range: £1.12 - £38918.00\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap the confidence interval of log variance of consumption - overall\n",
    "\n",
    "# Initialize bootstrap analysis\n",
    "analyser_log_var_consumption_overall_2016 = LCFSBootstrap(df_16, consumption_col='derived_consumption')\n",
    "analyser_log_var_consumption_overall_2017 = LCFSBootstrap(df_17, consumption_col='derived_consumption')\n",
    "analyser_log_var_consumption_overall_2018 = LCFSBootstrap(df_18, consumption_col='derived_consumption')\n",
    "analyser_log_var_consumption_overall_2019 = LCFSBootstrap(df_19, consumption_col='derived_consumption')\n",
    "analyser_log_var_consumption_overall_2020 = LCFSBootstrap(df_20, consumption_col='derived_consumption')\n",
    "analyser_log_var_consumption_overall_2021 = LCFSBootstrap(df_21, consumption_col='derived_consumption')\n",
    "analyser_log_var_consumption_overall_2022 = LCFSBootstrap(df_22, consumption_col='derived_consumption')\n",
    "\n",
    "# Run analysis\n",
    "bootstrap_log_var_consumption_overall_2016 = analyser_log_var_consumption_overall_2016.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_overall_2017 = analyser_log_var_consumption_overall_2017.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_overall_2018 = analyser_log_var_consumption_overall_2018.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_overall_2019 = analyser_log_var_consumption_overall_2019.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_overall_2020 = analyser_log_var_consumption_overall_2020.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_overall_2021 = analyser_log_var_consumption_overall_2021.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_overall_2022 = analyser_log_var_consumption_overall_2022.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower_log_var_consumption_overall_2016, ci_upper_log_var_consumption_overall_2016 = bootstrap_log_var_consumption_overall_2016['confidence_interval']\n",
    "ci_lower_log_var_consumption_overall_2017, ci_upper_log_var_consumption_overall_2017 = bootstrap_log_var_consumption_overall_2017['confidence_interval']\n",
    "ci_lower_log_var_consumption_overall_2018, ci_upper_log_var_consumption_overall_2018 = bootstrap_log_var_consumption_overall_2018['confidence_interval']\n",
    "ci_lower_log_var_consumption_overall_2019, ci_upper_log_var_consumption_overall_2019 = bootstrap_log_var_consumption_overall_2019['confidence_interval']\n",
    "ci_lower_log_var_consumption_overall_2020, ci_upper_log_var_consumption_overall_2020 = bootstrap_log_var_consumption_overall_2020['confidence_interval']\n",
    "ci_lower_log_var_consumption_overall_2021, ci_upper_log_var_consumption_overall_2021 = bootstrap_log_var_consumption_overall_2021['confidence_interval']\n",
    "ci_lower_log_var_consumption_overall_2022, ci_upper_log_var_consumption_overall_2022 = bootstrap_log_var_consumption_overall_2022['confidence_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d9c444bf-89da-4cfb-83c6-d79e6d26f5a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 1336 observations\n",
      "Consumption range: £2.06 - £29607.72\n",
      "Dataset loaded with 1679 observations\n",
      "Consumption range: £0.78 - £8545.83\n",
      "Dataset loaded with 1565 observations\n",
      "Consumption range: £0.29 - £4082.21\n",
      "Dataset loaded with 1749 observations\n",
      "Consumption range: £1.53 - £12744.45\n",
      "Dataset loaded with 1690 observations\n",
      "Consumption range: £0.31 - £3173.13\n",
      "Dataset loaded with 1716 observations\n",
      "Consumption range: £2.27 - £20795.05\n",
      "Dataset loaded with 898 observations\n",
      "Consumption range: £2.36 - £38918.00\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap the confidence interval of log variance of consumption - wfh\n",
    "\n",
    "# Initialize bootstrap analysis\n",
    "analyser_log_var_consumption_wfh_2016 = LCFSBootstrap(df_16, consumption_col='derived_consumption-wfh')\n",
    "analyser_log_var_consumption_wfh_2017 = LCFSBootstrap(df_17, consumption_col='derived_consumption-wfh')\n",
    "analyser_log_var_consumption_wfh_2018 = LCFSBootstrap(df_18, consumption_col='derived_consumption-wfh')\n",
    "analyser_log_var_consumption_wfh_2019 = LCFSBootstrap(df_19, consumption_col='derived_consumption-wfh')\n",
    "analyser_log_var_consumption_wfh_2020 = LCFSBootstrap(df_20, consumption_col='derived_consumption-wfh')\n",
    "analyser_log_var_consumption_wfh_2021 = LCFSBootstrap(df_21, consumption_col='derived_consumption-wfh')\n",
    "analyser_log_var_consumption_wfh_2022 = LCFSBootstrap(df_22, consumption_col='derived_consumption-wfh')\n",
    "\n",
    "# Run analysis\n",
    "bootstrap_log_var_consumption_wfh_2016 = analyser_log_var_consumption_wfh_2016.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_wfh_2017 = analyser_log_var_consumption_wfh_2017.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_wfh_2018 = analyser_log_var_consumption_wfh_2018.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_wfh_2019 = analyser_log_var_consumption_wfh_2019.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_wfh_2020 = analyser_log_var_consumption_wfh_2020.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_wfh_2021 = analyser_log_var_consumption_wfh_2021.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_wfh_2022 = analyser_log_var_consumption_wfh_2022.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower_log_var_consumption_wfh_2016, ci_upper_log_var_consumption_wfh_2016 = bootstrap_log_var_consumption_wfh_2016['confidence_interval']\n",
    "ci_lower_log_var_consumption_wfh_2017, ci_upper_log_var_consumption_wfh_2017 = bootstrap_log_var_consumption_wfh_2017['confidence_interval']\n",
    "ci_lower_log_var_consumption_wfh_2018, ci_upper_log_var_consumption_wfh_2018 = bootstrap_log_var_consumption_wfh_2018['confidence_interval']\n",
    "ci_lower_log_var_consumption_wfh_2019, ci_upper_log_var_consumption_wfh_2019 = bootstrap_log_var_consumption_wfh_2019['confidence_interval']\n",
    "ci_lower_log_var_consumption_wfh_2020, ci_upper_log_var_consumption_wfh_2020 = bootstrap_log_var_consumption_wfh_2020['confidence_interval']\n",
    "ci_lower_log_var_consumption_wfh_2021, ci_upper_log_var_consumption_wfh_2021 = bootstrap_log_var_consumption_wfh_2021['confidence_interval']\n",
    "ci_lower_log_var_consumption_wfh_2022, ci_upper_log_var_consumption_wfh_2022 = bootstrap_log_var_consumption_wfh_2022['confidence_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5caf717a-25c8-45d6-a950-2b51fbd5a9a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BOOTSTRAP ANALYSIS SUMMARY\n",
      "============================================================\n",
      "Number of observations: 1,024\n",
      "Number of bootstrap samples: 1,000\n",
      "Confidence level: 95.0%\n",
      "\n",
      "LOG VARIANCE ESTIMATES:\n",
      "  Original sample: 1.452129\n",
      "  Bootstrap mean:  1.449910\n",
      "  Bootstrap std:   0.062493\n",
      "\n",
      "CONFIDENCE INTERVAL:\n",
      "  Lower bound: 1.335548\n",
      "  Upper bound: 1.573801\n",
      "  Width:       0.238252\n",
      "\n",
      "BIAS ANALYSIS:\n",
      "  Bootstrap bias: -0.002219\n",
      "  Relative bias:  -0.15%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "analyser_log_var_consumption_wfh_2016.print_summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c02cb7a7-33ca-4103-9ce6-e9a95fabc921",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 1195 observations\n",
      "Consumption range: £0.23 - £7599.47\n",
      "Dataset loaded with 1150 observations\n",
      "Consumption range: £2.52 - £8044.39\n",
      "Dataset loaded with 1292 observations\n",
      "Consumption range: £0.73 - £15149.09\n",
      "Dataset loaded with 1024 observations\n",
      "Consumption range: £1.19 - £2078.12\n",
      "Dataset loaded with 1165 observations\n",
      "Consumption range: £1.53 - £5003.84\n",
      "Dataset loaded with 1233 observations\n",
      "Consumption range: £2.08 - £5257.69\n",
      "Dataset loaded with 1448 observations\n",
      "Consumption range: £1.12 - £12438.83\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n",
      "Performing 1000 bootstrap samples...\n",
      "  Completed 100/1000 samples\n",
      "  Completed 200/1000 samples\n",
      "  Completed 300/1000 samples\n",
      "  Completed 400/1000 samples\n",
      "  Completed 500/1000 samples\n",
      "  Completed 600/1000 samples\n",
      "  Completed 700/1000 samples\n",
      "  Completed 800/1000 samples\n",
      "  Completed 900/1000 samples\n",
      "  Completed 1000/1000 samples\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap the confidence interval of log variance of consumption - non_wfh\n",
    "\n",
    "# Initialize bootstrap analysis\n",
    "analyser_log_var_consumption_non_wfh_2016 = LCFSBootstrap(df_16, consumption_col='derived_consumption-non_wfh')\n",
    "analyser_log_var_consumption_non_wfh_2017 = LCFSBootstrap(df_17, consumption_col='derived_consumption-non_wfh')\n",
    "analyser_log_var_consumption_non_wfh_2018 = LCFSBootstrap(df_18, consumption_col='derived_consumption-non_wfh')\n",
    "analyser_log_var_consumption_non_wfh_2019 = LCFSBootstrap(df_19, consumption_col='derived_consumption-non_wfh')\n",
    "analyser_log_var_consumption_non_wfh_2020 = LCFSBootstrap(df_20, consumption_col='derived_consumption-non_wfh')\n",
    "analyser_log_var_consumption_non_wfh_2021 = LCFSBootstrap(df_21, consumption_col='derived_consumption-non_wfh')\n",
    "analyser_log_var_consumption_non_wfh_2022 = LCFSBootstrap(df_22, consumption_col='derived_consumption-non_wfh')\n",
    "\n",
    "# Run analysis\n",
    "bootstrap_log_var_consumption_non_wfh_2016 = analyser_log_var_consumption_non_wfh_2016.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_non_wfh_2017 = analyser_log_var_consumption_non_wfh_2017.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_non_wfh_2018 = analyser_log_var_consumption_non_wfh_2018.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_non_wfh_2019 = analyser_log_var_consumption_non_wfh_2019.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_non_wfh_2020 = analyser_log_var_consumption_non_wfh_2020.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_non_wfh_2021 = analyser_log_var_consumption_non_wfh_2021.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "bootstrap_log_var_consumption_non_wfh_2022 = analyser_log_var_consumption_non_wfh_2022.run_analysis(n_bootstrap=1000, confidence_level=0.95, random_state=42)\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower_log_var_consumption_non_wfh_2016, ci_upper_log_var_consumption_non_wfh_2016 = bootstrap_log_var_consumption_non_wfh_2016['confidence_interval']\n",
    "ci_lower_log_var_consumption_non_wfh_2017, ci_upper_log_var_consumption_non_wfh_2017 = bootstrap_log_var_consumption_non_wfh_2017['confidence_interval']\n",
    "ci_lower_log_var_consumption_non_wfh_2018, ci_upper_log_var_consumption_non_wfh_2018 = bootstrap_log_var_consumption_non_wfh_2018['confidence_interval']\n",
    "ci_lower_log_var_consumption_non_wfh_2019, ci_upper_log_var_consumption_non_wfh_2019 = bootstrap_log_var_consumption_non_wfh_2019['confidence_interval']\n",
    "ci_lower_log_var_consumption_non_wfh_2020, ci_upper_log_var_consumption_non_wfh_2020 = bootstrap_log_var_consumption_non_wfh_2020['confidence_interval']\n",
    "ci_lower_log_var_consumption_non_wfh_2021, ci_upper_log_var_consumption_non_wfh_2021 = bootstrap_log_var_consumption_non_wfh_2021['confidence_interval']\n",
    "ci_lower_log_var_consumption_non_wfh_2022, ci_upper_log_var_consumption_non_wfh_2022 = bootstrap_log_var_consumption_non_wfh_2022['confidence_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8767408a-590e-429a-a187-be1f0b798dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>avg_income_ci_lower_overall</th>\n",
       "      <th>avg_income_ci_upper_overall</th>\n",
       "      <th>90/10_ratio_income_ci_lower_overall</th>\n",
       "      <th>90/10_ratio_income_ci_upper_overall</th>\n",
       "      <th>log_var_income_ci_lower_overall</th>\n",
       "      <th>log_var_income_ci_upper_overall</th>\n",
       "      <th>avg_consumption_ci_lower_overall</th>\n",
       "      <th>avg_consumption_ci_upper_overall</th>\n",
       "      <th>log_var_consumption_ci_lower_overall</th>\n",
       "      <th>log_var_consumption_ci_upper_overall</th>\n",
       "      <th>avg_income_ci_lower_wfh</th>\n",
       "      <th>avg_income_ci_upper_wfh</th>\n",
       "      <th>90/10_ratio_income_ci_lower_wfh</th>\n",
       "      <th>90/10_ratio_income_ci_upper_wfh</th>\n",
       "      <th>log_var_income_ci_lower_wfh</th>\n",
       "      <th>log_var_income_ci_upper_wfh</th>\n",
       "      <th>avg_consumption_ci_lower_wfh</th>\n",
       "      <th>avg_consumption_ci_upper_wfh</th>\n",
       "      <th>log_var_consumption_ci_lower_wfh</th>\n",
       "      <th>log_var_consumption_ci_upper_wfh</th>\n",
       "      <th>avg_income_ci_lower_non_wfh</th>\n",
       "      <th>avg_income_ci_upper_non_wfh</th>\n",
       "      <th>90/10_ratio_income_ci_lower_non_wfh</th>\n",
       "      <th>90/10_ratio_income_ci_upper_non_wfh</th>\n",
       "      <th>log_var_income_ci_lower_non_wfh</th>\n",
       "      <th>log_var_income_ci_upper_non_wfh</th>\n",
       "      <th>avg_consumption_ci_lower_non_wfh</th>\n",
       "      <th>avg_consumption_ci_upper_non_wfh</th>\n",
       "      <th>log_var_consumption_ci_lower_non_wfh</th>\n",
       "      <th>log_var_consumption_ci_upper_non_wfh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>986.801687</td>\n",
       "      <td>1026.138221</td>\n",
       "      <td>986.801687</td>\n",
       "      <td>1026.138221</td>\n",
       "      <td>0.310218</td>\n",
       "      <td>0.349387</td>\n",
       "      <td>149.662397</td>\n",
       "      <td>200.469351</td>\n",
       "      <td>1.251307</td>\n",
       "      <td>1.433037</td>\n",
       "      <td>1127.651186</td>\n",
       "      <td>1183.694406</td>\n",
       "      <td>1127.651186</td>\n",
       "      <td>1183.694406</td>\n",
       "      <td>0.254652</td>\n",
       "      <td>0.303398</td>\n",
       "      <td>147.521187</td>\n",
       "      <td>229.200031</td>\n",
       "      <td>1.121314</td>\n",
       "      <td>1.345657</td>\n",
       "      <td>815.273354</td>\n",
       "      <td>862.936184</td>\n",
       "      <td>815.273354</td>\n",
       "      <td>862.936184</td>\n",
       "      <td>0.254652</td>\n",
       "      <td>0.354876</td>\n",
       "      <td>139.977692</td>\n",
       "      <td>181.790472</td>\n",
       "      <td>1.313706</td>\n",
       "      <td>1.586692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>1017.111126</td>\n",
       "      <td>1057.922731</td>\n",
       "      <td>1017.111126</td>\n",
       "      <td>1057.922731</td>\n",
       "      <td>0.341533</td>\n",
       "      <td>0.389914</td>\n",
       "      <td>157.913348</td>\n",
       "      <td>191.310933</td>\n",
       "      <td>1.284874</td>\n",
       "      <td>1.445198</td>\n",
       "      <td>1192.361006</td>\n",
       "      <td>1248.124885</td>\n",
       "      <td>1192.361006</td>\n",
       "      <td>1248.124885</td>\n",
       "      <td>0.271455</td>\n",
       "      <td>0.312605</td>\n",
       "      <td>167.262352</td>\n",
       "      <td>208.999342</td>\n",
       "      <td>1.268563</td>\n",
       "      <td>1.483261</td>\n",
       "      <td>745.391719</td>\n",
       "      <td>791.810719</td>\n",
       "      <td>745.391719</td>\n",
       "      <td>791.810719</td>\n",
       "      <td>0.271455</td>\n",
       "      <td>0.379250</td>\n",
       "      <td>130.632987</td>\n",
       "      <td>182.044993</td>\n",
       "      <td>1.215193</td>\n",
       "      <td>1.454886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>1006.719520</td>\n",
       "      <td>1044.283469</td>\n",
       "      <td>1006.719520</td>\n",
       "      <td>1044.283469</td>\n",
       "      <td>0.335908</td>\n",
       "      <td>0.383579</td>\n",
       "      <td>141.243484</td>\n",
       "      <td>170.471176</td>\n",
       "      <td>1.411336</td>\n",
       "      <td>1.583765</td>\n",
       "      <td>1242.882226</td>\n",
       "      <td>1295.915644</td>\n",
       "      <td>1242.882226</td>\n",
       "      <td>1295.915644</td>\n",
       "      <td>0.219035</td>\n",
       "      <td>0.253984</td>\n",
       "      <td>150.274004</td>\n",
       "      <td>180.671466</td>\n",
       "      <td>1.314078</td>\n",
       "      <td>1.538092</td>\n",
       "      <td>713.607826</td>\n",
       "      <td>753.409292</td>\n",
       "      <td>713.607826</td>\n",
       "      <td>753.409292</td>\n",
       "      <td>0.219035</td>\n",
       "      <td>0.360854</td>\n",
       "      <td>121.485532</td>\n",
       "      <td>173.656855</td>\n",
       "      <td>1.393387</td>\n",
       "      <td>1.671157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>1057.794307</td>\n",
       "      <td>1101.367129</td>\n",
       "      <td>1057.794307</td>\n",
       "      <td>1101.367129</td>\n",
       "      <td>0.308114</td>\n",
       "      <td>0.347923</td>\n",
       "      <td>135.598942</td>\n",
       "      <td>161.775601</td>\n",
       "      <td>1.362148</td>\n",
       "      <td>1.508629</td>\n",
       "      <td>1250.131222</td>\n",
       "      <td>1303.596171</td>\n",
       "      <td>1250.131222</td>\n",
       "      <td>1303.596171</td>\n",
       "      <td>0.232839</td>\n",
       "      <td>0.271881</td>\n",
       "      <td>146.789169</td>\n",
       "      <td>183.433935</td>\n",
       "      <td>1.308408</td>\n",
       "      <td>1.490179</td>\n",
       "      <td>721.281235</td>\n",
       "      <td>764.868232</td>\n",
       "      <td>721.281235</td>\n",
       "      <td>764.868232</td>\n",
       "      <td>0.232839</td>\n",
       "      <td>0.300381</td>\n",
       "      <td>109.465760</td>\n",
       "      <td>133.481763</td>\n",
       "      <td>1.335548</td>\n",
       "      <td>1.573801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>1088.861107</td>\n",
       "      <td>1131.200105</td>\n",
       "      <td>1088.861107</td>\n",
       "      <td>1131.200105</td>\n",
       "      <td>0.302339</td>\n",
       "      <td>0.337917</td>\n",
       "      <td>79.523845</td>\n",
       "      <td>94.044270</td>\n",
       "      <td>1.214206</td>\n",
       "      <td>1.363498</td>\n",
       "      <td>1228.885553</td>\n",
       "      <td>1283.472012</td>\n",
       "      <td>1228.885553</td>\n",
       "      <td>1283.472012</td>\n",
       "      <td>0.237666</td>\n",
       "      <td>0.272021</td>\n",
       "      <td>82.290496</td>\n",
       "      <td>98.765858</td>\n",
       "      <td>1.206655</td>\n",
       "      <td>1.392366</td>\n",
       "      <td>873.234169</td>\n",
       "      <td>928.728836</td>\n",
       "      <td>873.234169</td>\n",
       "      <td>928.728836</td>\n",
       "      <td>0.237666</td>\n",
       "      <td>0.364149</td>\n",
       "      <td>70.841933</td>\n",
       "      <td>95.218494</td>\n",
       "      <td>1.156070</td>\n",
       "      <td>1.390090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021</td>\n",
       "      <td>1155.409785</td>\n",
       "      <td>1196.477502</td>\n",
       "      <td>1155.409785</td>\n",
       "      <td>1196.477502</td>\n",
       "      <td>0.298309</td>\n",
       "      <td>0.338569</td>\n",
       "      <td>114.502324</td>\n",
       "      <td>151.853518</td>\n",
       "      <td>1.373310</td>\n",
       "      <td>1.534207</td>\n",
       "      <td>1320.899725</td>\n",
       "      <td>1378.641044</td>\n",
       "      <td>1320.899725</td>\n",
       "      <td>1378.641044</td>\n",
       "      <td>0.221636</td>\n",
       "      <td>0.257818</td>\n",
       "      <td>123.142024</td>\n",
       "      <td>176.990254</td>\n",
       "      <td>1.334476</td>\n",
       "      <td>1.543490</td>\n",
       "      <td>909.164212</td>\n",
       "      <td>962.733119</td>\n",
       "      <td>909.164212</td>\n",
       "      <td>962.733119</td>\n",
       "      <td>0.221636</td>\n",
       "      <td>0.367272</td>\n",
       "      <td>93.402795</td>\n",
       "      <td>124.132133</td>\n",
       "      <td>1.315566</td>\n",
       "      <td>1.563972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022</td>\n",
       "      <td>1167.033510</td>\n",
       "      <td>1216.550538</td>\n",
       "      <td>1167.033510</td>\n",
       "      <td>1216.550538</td>\n",
       "      <td>0.301476</td>\n",
       "      <td>0.341151</td>\n",
       "      <td>137.354803</td>\n",
       "      <td>209.480209</td>\n",
       "      <td>1.564780</td>\n",
       "      <td>1.786739</td>\n",
       "      <td>1369.128964</td>\n",
       "      <td>1450.098563</td>\n",
       "      <td>1369.128964</td>\n",
       "      <td>1450.098563</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.279617</td>\n",
       "      <td>139.911955</td>\n",
       "      <td>302.873141</td>\n",
       "      <td>1.442732</td>\n",
       "      <td>1.776183</td>\n",
       "      <td>1027.735515</td>\n",
       "      <td>1086.661015</td>\n",
       "      <td>1027.735515</td>\n",
       "      <td>1086.661015</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.350795</td>\n",
       "      <td>124.025805</td>\n",
       "      <td>177.000790</td>\n",
       "      <td>1.553946</td>\n",
       "      <td>1.811972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  avg_income_ci_lower_overall  avg_income_ci_upper_overall  \\\n",
       "0  2016                   986.801687                  1026.138221   \n",
       "1  2017                  1017.111126                  1057.922731   \n",
       "2  2018                  1006.719520                  1044.283469   \n",
       "3  2019                  1057.794307                  1101.367129   \n",
       "4  2020                  1088.861107                  1131.200105   \n",
       "5  2021                  1155.409785                  1196.477502   \n",
       "6  2022                  1167.033510                  1216.550538   \n",
       "\n",
       "   90/10_ratio_income_ci_lower_overall  90/10_ratio_income_ci_upper_overall  \\\n",
       "0                           986.801687                          1026.138221   \n",
       "1                          1017.111126                          1057.922731   \n",
       "2                          1006.719520                          1044.283469   \n",
       "3                          1057.794307                          1101.367129   \n",
       "4                          1088.861107                          1131.200105   \n",
       "5                          1155.409785                          1196.477502   \n",
       "6                          1167.033510                          1216.550538   \n",
       "\n",
       "   log_var_income_ci_lower_overall  log_var_income_ci_upper_overall  \\\n",
       "0                         0.310218                         0.349387   \n",
       "1                         0.341533                         0.389914   \n",
       "2                         0.335908                         0.383579   \n",
       "3                         0.308114                         0.347923   \n",
       "4                         0.302339                         0.337917   \n",
       "5                         0.298309                         0.338569   \n",
       "6                         0.301476                         0.341151   \n",
       "\n",
       "   avg_consumption_ci_lower_overall  avg_consumption_ci_upper_overall  \\\n",
       "0                        149.662397                        200.469351   \n",
       "1                        157.913348                        191.310933   \n",
       "2                        141.243484                        170.471176   \n",
       "3                        135.598942                        161.775601   \n",
       "4                         79.523845                         94.044270   \n",
       "5                        114.502324                        151.853518   \n",
       "6                        137.354803                        209.480209   \n",
       "\n",
       "   log_var_consumption_ci_lower_overall  log_var_consumption_ci_upper_overall  \\\n",
       "0                              1.251307                              1.433037   \n",
       "1                              1.284874                              1.445198   \n",
       "2                              1.411336                              1.583765   \n",
       "3                              1.362148                              1.508629   \n",
       "4                              1.214206                              1.363498   \n",
       "5                              1.373310                              1.534207   \n",
       "6                              1.564780                              1.786739   \n",
       "\n",
       "   avg_income_ci_lower_wfh  avg_income_ci_upper_wfh  \\\n",
       "0              1127.651186              1183.694406   \n",
       "1              1192.361006              1248.124885   \n",
       "2              1242.882226              1295.915644   \n",
       "3              1250.131222              1303.596171   \n",
       "4              1228.885553              1283.472012   \n",
       "5              1320.899725              1378.641044   \n",
       "6              1369.128964              1450.098563   \n",
       "\n",
       "   90/10_ratio_income_ci_lower_wfh  90/10_ratio_income_ci_upper_wfh  \\\n",
       "0                      1127.651186                      1183.694406   \n",
       "1                      1192.361006                      1248.124885   \n",
       "2                      1242.882226                      1295.915644   \n",
       "3                      1250.131222                      1303.596171   \n",
       "4                      1228.885553                      1283.472012   \n",
       "5                      1320.899725                      1378.641044   \n",
       "6                      1369.128964                      1450.098563   \n",
       "\n",
       "   log_var_income_ci_lower_wfh  log_var_income_ci_upper_wfh  \\\n",
       "0                     0.254652                     0.303398   \n",
       "1                     0.271455                     0.312605   \n",
       "2                     0.219035                     0.253984   \n",
       "3                     0.232839                     0.271881   \n",
       "4                     0.237666                     0.272021   \n",
       "5                     0.221636                     0.257818   \n",
       "6                     0.232323                     0.279617   \n",
       "\n",
       "   avg_consumption_ci_lower_wfh  avg_consumption_ci_upper_wfh  \\\n",
       "0                    147.521187                    229.200031   \n",
       "1                    167.262352                    208.999342   \n",
       "2                    150.274004                    180.671466   \n",
       "3                    146.789169                    183.433935   \n",
       "4                     82.290496                     98.765858   \n",
       "5                    123.142024                    176.990254   \n",
       "6                    139.911955                    302.873141   \n",
       "\n",
       "   log_var_consumption_ci_lower_wfh  log_var_consumption_ci_upper_wfh  \\\n",
       "0                          1.121314                          1.345657   \n",
       "1                          1.268563                          1.483261   \n",
       "2                          1.314078                          1.538092   \n",
       "3                          1.308408                          1.490179   \n",
       "4                          1.206655                          1.392366   \n",
       "5                          1.334476                          1.543490   \n",
       "6                          1.442732                          1.776183   \n",
       "\n",
       "   avg_income_ci_lower_non_wfh  avg_income_ci_upper_non_wfh  \\\n",
       "0                   815.273354                   862.936184   \n",
       "1                   745.391719                   791.810719   \n",
       "2                   713.607826                   753.409292   \n",
       "3                   721.281235                   764.868232   \n",
       "4                   873.234169                   928.728836   \n",
       "5                   909.164212                   962.733119   \n",
       "6                  1027.735515                  1086.661015   \n",
       "\n",
       "   90/10_ratio_income_ci_lower_non_wfh  90/10_ratio_income_ci_upper_non_wfh  \\\n",
       "0                           815.273354                           862.936184   \n",
       "1                           745.391719                           791.810719   \n",
       "2                           713.607826                           753.409292   \n",
       "3                           721.281235                           764.868232   \n",
       "4                           873.234169                           928.728836   \n",
       "5                           909.164212                           962.733119   \n",
       "6                          1027.735515                          1086.661015   \n",
       "\n",
       "   log_var_income_ci_lower_non_wfh  log_var_income_ci_upper_non_wfh  \\\n",
       "0                         0.254652                         0.354876   \n",
       "1                         0.271455                         0.379250   \n",
       "2                         0.219035                         0.360854   \n",
       "3                         0.232839                         0.300381   \n",
       "4                         0.237666                         0.364149   \n",
       "5                         0.221636                         0.367272   \n",
       "6                         0.232323                         0.350795   \n",
       "\n",
       "   avg_consumption_ci_lower_non_wfh  avg_consumption_ci_upper_non_wfh  \\\n",
       "0                        139.977692                        181.790472   \n",
       "1                        130.632987                        182.044993   \n",
       "2                        121.485532                        173.656855   \n",
       "3                        109.465760                        133.481763   \n",
       "4                         70.841933                         95.218494   \n",
       "5                         93.402795                        124.132133   \n",
       "6                        124.025805                        177.000790   \n",
       "\n",
       "   log_var_consumption_ci_lower_non_wfh  log_var_consumption_ci_upper_non_wfh  \n",
       "0                              1.313706                              1.586692  \n",
       "1                              1.215193                              1.454886  \n",
       "2                              1.393387                              1.671157  \n",
       "3                              1.335548                              1.573801  \n",
       "4                              1.156070                              1.390090  \n",
       "5                              1.315566                              1.563972  \n",
       "6                              1.553946                              1.811972  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe of confidence intervals\n",
    "data_ci = {\n",
    "    'Year': [2016, 2017, 2018, 2019, 2020, 2021, 2022],\n",
    "    'avg_income_ci_lower_overall' : [ci_lower_income_overall_2016,\n",
    "                                    ci_lower_income_overall_2017,\n",
    "                                    ci_lower_income_overall_2018,\n",
    "                                    ci_lower_income_overall_2019,\n",
    "                                    ci_lower_income_overall_2020,\n",
    "                                    ci_lower_income_overall_2021,\n",
    "                                    ci_lower_income_overall_2022],\n",
    "    'avg_income_ci_upper_overall' : [ci_upper_income_overall_2016,\n",
    "                                    ci_upper_income_overall_2017,\n",
    "                                    ci_upper_income_overall_2018,\n",
    "                                    ci_upper_income_overall_2019,\n",
    "                                    ci_upper_income_overall_2020,\n",
    "                                    ci_upper_income_overall_2021,\n",
    "                                    ci_upper_income_overall_2022],\n",
    "    '90/10_ratio_income_ci_lower_overall' : [ci_lower_income_overall_2016,\n",
    "                                             ci_lower_income_overall_2017,\n",
    "                                             ci_lower_income_overall_2018,\n",
    "                                             ci_lower_income_overall_2019,\n",
    "                                            ci_lower_income_overall_2020,\n",
    "                                             ci_lower_income_overall_2021,\n",
    "                                             ci_lower_income_overall_2022],\n",
    "    '90/10_ratio_income_ci_upper_overall' : [ci_upper_income_overall_2016,\n",
    "                                             ci_upper_income_overall_2017,\n",
    "                                             ci_upper_income_overall_2018,\n",
    "                                             ci_upper_income_overall_2019,\n",
    "                                            ci_upper_income_overall_2020,\n",
    "                                             ci_upper_income_overall_2021,\n",
    "                                             ci_upper_income_overall_2022],\n",
    "    'log_var_income_ci_lower_overall' : [ci_lower_log_var_income_overall_2016,\n",
    "                                         ci_lower_log_var_income_overall_2017,\n",
    "                                         ci_lower_log_var_income_overall_2018,\n",
    "                                        ci_lower_log_var_income_overall_2019,\n",
    "                                         ci_lower_log_var_income_overall_2020,\n",
    "                                         ci_lower_log_var_income_overall_2021,\n",
    "                                        ci_lower_log_var_income_overall_2022],\n",
    "    'log_var_income_ci_upper_overall' : [ci_upper_log_var_income_overall_2016,\n",
    "                                        ci_upper_log_var_income_overall_2017,\n",
    "                                        ci_upper_log_var_income_overall_2018,\n",
    "                                        ci_upper_log_var_income_overall_2019,\n",
    "                                        ci_upper_log_var_income_overall_2020,\n",
    "                                        ci_upper_log_var_income_overall_2021,\n",
    "                                        ci_upper_log_var_income_overall_2022],\n",
    "    'avg_consumption_ci_lower_overall' :[ci_lower_consumption_overall_2016,\n",
    "                                        ci_lower_consumption_overall_2017,\n",
    "                                        ci_lower_consumption_overall_2018,\n",
    "                                        ci_lower_consumption_overall_2019,\n",
    "                                        ci_lower_consumption_overall_2020,\n",
    "                                        ci_lower_consumption_overall_2021,\n",
    "                                        ci_lower_consumption_overall_2022],\n",
    "    'avg_consumption_ci_upper_overall' : [ci_upper_consumption_overall_2016,\n",
    "                                         ci_upper_consumption_overall_2017,\n",
    "                                         ci_upper_consumption_overall_2018,\n",
    "                                         ci_upper_consumption_overall_2019,\n",
    "                                         ci_upper_consumption_overall_2020,\n",
    "                                         ci_upper_consumption_overall_2021,\n",
    "                                         ci_upper_consumption_overall_2022],\n",
    "    'log_var_consumption_ci_lower_overall' : [ci_lower_log_var_consumption_overall_2016,\n",
    "                                             ci_lower_log_var_consumption_overall_2017,\n",
    "                                             ci_lower_log_var_consumption_overall_2018,\n",
    "                                             ci_lower_log_var_consumption_overall_2019,\n",
    "                                             ci_lower_log_var_consumption_overall_2020,\n",
    "                                             ci_lower_log_var_consumption_overall_2021,\n",
    "                                             ci_lower_log_var_consumption_overall_2022],\n",
    "    'log_var_consumption_ci_upper_overall' : [ci_upper_log_var_consumption_overall_2016,\n",
    "                                             ci_upper_log_var_consumption_overall_2017,\n",
    "                                             ci_upper_log_var_consumption_overall_2018,\n",
    "                                             ci_upper_log_var_consumption_overall_2019,\n",
    "                                             ci_upper_log_var_consumption_overall_2020,\n",
    "                                             ci_upper_log_var_consumption_overall_2021,\n",
    "                                             ci_upper_log_var_consumption_overall_2022],\n",
    "    ##############################################################\n",
    "     'avg_income_ci_lower_wfh' : [ci_lower_income_wfh_2016,\n",
    "                                 ci_lower_income_wfh_2017,\n",
    "                                 ci_lower_income_wfh_2018,\n",
    "                                 ci_lower_income_wfh_2019,\n",
    "                                 ci_lower_income_wfh_2020,\n",
    "                                 ci_lower_income_wfh_2021,\n",
    "                                 ci_lower_income_wfh_2022],\n",
    "    'avg_income_ci_upper_wfh' : [ci_upper_income_wfh_2016,\n",
    "                                ci_upper_income_wfh_2017,\n",
    "                                ci_upper_income_wfh_2018,\n",
    "                                ci_upper_income_wfh_2019,\n",
    "                                ci_upper_income_wfh_2020,\n",
    "                                ci_upper_income_wfh_2021,\n",
    "                                ci_upper_income_wfh_2022],\n",
    "    '90/10_ratio_income_ci_lower_wfh' : [ci_lower_income_wfh_2016,\n",
    "                                        ci_lower_income_wfh_2017,\n",
    "                                        ci_lower_income_wfh_2018,\n",
    "                                        ci_lower_income_wfh_2019,\n",
    "                                        ci_lower_income_wfh_2020,\n",
    "                                        ci_lower_income_wfh_2021,\n",
    "                                        ci_lower_income_wfh_2022],\n",
    "    '90/10_ratio_income_ci_upper_wfh' : [ci_upper_income_wfh_2016,\n",
    "                                        ci_upper_income_wfh_2017,\n",
    "                                        ci_upper_income_wfh_2018,\n",
    "                                        ci_upper_income_wfh_2019,\n",
    "                                        ci_upper_income_wfh_2020,\n",
    "                                        ci_upper_income_wfh_2021,\n",
    "                                        ci_upper_income_wfh_2022],\n",
    "    'log_var_income_ci_lower_wfh' : [ci_lower_log_var_income_wfh_2016,\n",
    "                                    ci_lower_log_var_income_wfh_2017,\n",
    "                                    ci_lower_log_var_income_wfh_2018,\n",
    "                                    ci_lower_log_var_income_wfh_2019,\n",
    "                                    ci_lower_log_var_income_wfh_2020,\n",
    "                                    ci_lower_log_var_income_wfh_2021,\n",
    "                                    ci_lower_log_var_income_wfh_2022],\n",
    "    'log_var_income_ci_upper_wfh' : [ci_upper_log_var_income_wfh_2016,\n",
    "                                    ci_upper_log_var_income_wfh_2017,\n",
    "                                    ci_upper_log_var_income_wfh_2018,\n",
    "                                    ci_upper_log_var_income_wfh_2019,\n",
    "                                    ci_upper_log_var_income_wfh_2020,\n",
    "                                    ci_upper_log_var_income_wfh_2021,\n",
    "                                    ci_upper_log_var_income_wfh_2022],\n",
    "    'avg_consumption_ci_lower_wfh' :[ci_lower_consumption_wfh_2016,\n",
    "                                    ci_lower_consumption_wfh_2017,\n",
    "                                    ci_lower_consumption_wfh_2018,\n",
    "                                    ci_lower_consumption_wfh_2019,\n",
    "                                    ci_lower_consumption_wfh_2020,\n",
    "                                    ci_lower_consumption_wfh_2021,\n",
    "                                    ci_lower_consumption_wfh_2022],\n",
    "    'avg_consumption_ci_upper_wfh' : [ci_upper_consumption_wfh_2016,\n",
    "                                     ci_upper_consumption_wfh_2017,\n",
    "                                     ci_upper_consumption_wfh_2018,\n",
    "                                     ci_upper_consumption_wfh_2019,\n",
    "                                     ci_upper_consumption_wfh_2020,\n",
    "                                     ci_upper_consumption_wfh_2021,\n",
    "                                     ci_upper_consumption_wfh_2022],\n",
    "    'log_var_consumption_ci_lower_wfh' : [ci_lower_log_var_consumption_wfh_2016,\n",
    "                                         ci_lower_log_var_consumption_wfh_2017,\n",
    "                                         ci_lower_log_var_consumption_wfh_2018,\n",
    "                                         ci_lower_log_var_consumption_wfh_2019,\n",
    "                                         ci_lower_log_var_consumption_wfh_2020,\n",
    "                                         ci_lower_log_var_consumption_wfh_2021,\n",
    "                                         ci_lower_log_var_consumption_wfh_2022],\n",
    "    'log_var_consumption_ci_upper_wfh' : [ci_upper_log_var_consumption_wfh_2016,\n",
    "                                         ci_upper_log_var_consumption_wfh_2017,\n",
    "                                         ci_upper_log_var_consumption_wfh_2018,\n",
    "                                         ci_upper_log_var_consumption_wfh_2019,\n",
    "                                         ci_upper_log_var_consumption_wfh_2020,\n",
    "                                         ci_upper_log_var_consumption_wfh_2021,\n",
    "                                         ci_upper_log_var_consumption_wfh_2022],\n",
    "    #####################################################################\n",
    "     'avg_income_ci_lower_non_wfh' : [ci_lower_income_non_wfh_2016,\n",
    "                                     ci_lower_income_non_wfh_2017,\n",
    "                                     ci_lower_income_non_wfh_2018,\n",
    "                                     ci_lower_income_non_wfh_2019,\n",
    "                                     ci_lower_income_non_wfh_2020,\n",
    "                                     ci_lower_income_non_wfh_2021,\n",
    "                                     ci_lower_income_non_wfh_2022],\n",
    "    'avg_income_ci_upper_non_wfh' : [ci_upper_income_non_wfh_2016,\n",
    "                                    ci_upper_income_non_wfh_2017,\n",
    "                                    ci_upper_income_non_wfh_2018,\n",
    "                                    ci_upper_income_non_wfh_2019,\n",
    "                                    ci_upper_income_non_wfh_2020,\n",
    "                                    ci_upper_income_non_wfh_2021,\n",
    "                                    ci_upper_income_non_wfh_2022],\n",
    "    '90/10_ratio_income_ci_lower_non_wfh' : [ci_lower_income_non_wfh_2016,\n",
    "                                            ci_lower_income_non_wfh_2017,\n",
    "                                            ci_lower_income_non_wfh_2018,\n",
    "                                            ci_lower_income_non_wfh_2019,\n",
    "                                            ci_lower_income_non_wfh_2020,\n",
    "                                            ci_lower_income_non_wfh_2021,\n",
    "                                            ci_lower_income_non_wfh_2022],\n",
    "    '90/10_ratio_income_ci_upper_non_wfh' : [ci_upper_income_non_wfh_2016,\n",
    "                                            ci_upper_income_non_wfh_2017,\n",
    "                                            ci_upper_income_non_wfh_2018,\n",
    "                                            ci_upper_income_non_wfh_2019,\n",
    "                                            ci_upper_income_non_wfh_2020,\n",
    "                                            ci_upper_income_non_wfh_2021,\n",
    "                                            ci_upper_income_non_wfh_2022],\n",
    "    'log_var_income_ci_lower_non_wfh' : [ci_lower_log_var_income_wfh_2016,\n",
    "                                        ci_lower_log_var_income_wfh_2017,\n",
    "                                        ci_lower_log_var_income_wfh_2018,\n",
    "                                        ci_lower_log_var_income_wfh_2019,\n",
    "                                        ci_lower_log_var_income_wfh_2020,\n",
    "                                        ci_lower_log_var_income_wfh_2021,\n",
    "                                        ci_lower_log_var_income_wfh_2022],\n",
    "    'log_var_income_ci_upper_non_wfh' : [ci_upper_log_var_income_non_wfh_2016,\n",
    "                                        ci_upper_log_var_income_non_wfh_2017,\n",
    "                                        ci_upper_log_var_income_non_wfh_2018,\n",
    "                                        ci_upper_log_var_income_non_wfh_2019,\n",
    "                                        ci_upper_log_var_income_non_wfh_2020,\n",
    "                                        ci_upper_log_var_income_non_wfh_2021,\n",
    "                                        ci_upper_log_var_income_non_wfh_2022],\n",
    "    'avg_consumption_ci_lower_non_wfh' :[ci_lower_consumption_non_wfh_2016,\n",
    "                                        ci_lower_consumption_non_wfh_2017,\n",
    "                                        ci_lower_consumption_non_wfh_2018,\n",
    "                                        ci_lower_consumption_non_wfh_2019,\n",
    "                                        ci_lower_consumption_non_wfh_2020,\n",
    "                                        ci_lower_consumption_non_wfh_2021,\n",
    "                                        ci_lower_consumption_non_wfh_2022],\n",
    "    'avg_consumption_ci_upper_non_wfh' : [ci_upper_consumption_non_wfh_2016,\n",
    "                                         ci_upper_consumption_non_wfh_2017,\n",
    "                                         ci_upper_consumption_non_wfh_2018,\n",
    "                                         ci_upper_consumption_non_wfh_2019,\n",
    "                                         ci_upper_consumption_non_wfh_2020,\n",
    "                                         ci_upper_consumption_non_wfh_2021,\n",
    "                                         ci_upper_consumption_non_wfh_2022],\n",
    "    'log_var_consumption_ci_lower_non_wfh' : [ci_lower_log_var_consumption_non_wfh_2016,\n",
    "                                             ci_lower_log_var_consumption_non_wfh_2017,\n",
    "                                             ci_lower_log_var_consumption_non_wfh_2018,\n",
    "                                             ci_lower_log_var_consumption_non_wfh_2019,\n",
    "                                             ci_lower_log_var_consumption_non_wfh_2020,\n",
    "                                             ci_lower_log_var_consumption_non_wfh_2021,\n",
    "                                             ci_lower_log_var_consumption_non_wfh_2022],\n",
    "    'log_var_consumption_ci_upper_non_wfh' : [ci_upper_log_var_consumption_non_wfh_2016,\n",
    "                                             ci_upper_log_var_consumption_non_wfh_2017,\n",
    "                                             ci_upper_log_var_consumption_non_wfh_2018,\n",
    "                                             ci_upper_log_var_consumption_non_wfh_2019,\n",
    "                                             ci_upper_log_var_consumption_non_wfh_2020,\n",
    "                                             ci_upper_log_var_consumption_non_wfh_2021,\n",
    "                                             ci_upper_log_var_consumption_non_wfh_2022],\n",
    "}\n",
    "\n",
    "df_ci = pd.DataFrame(data_ci)\n",
    "df_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0cc9312c-61f5-479a-8bd5-595c428744de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the calculated confidence intervals table for easy access\n",
    "\n",
    "df_ci.to_csv('confidence_intervals.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc039be-8db0-442c-bae9-65744c737f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
