{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f820d348-d3e7-490b-812e-8d3e61868726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyreadstat in c:\\users\\omen\\appdata\\roaming\\python\\python311\\site-packages (1.2.8)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyreadstat) (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#install pyreadstat ==> to read spss file\n",
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd27809-251b-4c49-9835-91c9e3db5ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\Anaconda3 Files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\AppData\\Local\\Temp\\ipykernel_1364\\3450391093.py:9: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  pd.set_option('use_inf_as_na', True)\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat as pyrs\n",
    "library(haven)\n",
    "#set no limits on data display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('use_inf_as_na', True)\n",
    "#getting the work directory\n",
    "import os \n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4adbe188-b0cb-4ec0-8369-363e4ba56876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in c:\\users\\omen\\appdata\\roaming\\python\\python311\\site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0477a901-8f0d-4ff3-9d50-21a8947f8786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting haven\n",
      "  Downloading haven-2.0.4-py3-none-any.whl.metadata (455 bytes)\n",
      "Collecting events (from haven)\n",
      "  Downloading Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting netkit (from haven)\n",
      "  Downloading netkit-3.1.12-py3-none-any.whl.metadata (294 bytes)\n",
      "Collecting setproctitle (from haven)\n",
      "  Downloading setproctitle-1.3.6-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Collecting gevent (from haven)\n",
      "  Downloading gevent-25.5.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting gevent-websocket (from haven)\n",
      "  Downloading gevent_websocket-0.10.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting greenlet>=3.2.2 (from gevent->haven)\n",
      "  Downloading greenlet-3.2.3-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting cffi>=1.17.1 (from gevent->haven)\n",
      "  Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting zope.event (from gevent->haven)\n",
      "  Downloading zope_event-5.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: zope.interface in c:\\programdata\\anaconda3\\lib\\site-packages (from gevent->haven) (5.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.17.1->gevent->haven) (2.21)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from zope.event->gevent->haven) (68.2.2)\n",
      "Downloading haven-2.0.4-py3-none-any.whl (13 kB)\n",
      "Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Downloading gevent-25.5.1-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.2/1.6 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.6/1.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.6 MB 8.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.6 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading gevent_websocket-0.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading netkit-3.1.12-py3-none-any.whl (9.5 kB)\n",
      "Downloading setproctitle-1.3.6-cp311-cp311-win_amd64.whl (12 kB)\n",
      "Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.4/181.4 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.3-cp311-cp311-win_amd64.whl (297 kB)\n",
      "   ---------------------------------------- 0.0/297.0 kB ? eta -:--:--\n",
      "   --------------------------------------  297.0/297.0 kB 17.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 297.0/297.0 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading zope_event-5.1-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: events, zope.event, setproctitle, netkit, greenlet, cffi, gevent, gevent-websocket, haven\n",
      "Successfully installed cffi-1.17.1 events-0.5 gevent-25.5.1 gevent-websocket-0.10.1 greenlet-3.2.3 haven-2.0.4 netkit-3.1.12 setproctitle-1.3.6 zope.event-5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install haven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1ae86d-9d19-49ec-9f4f-a5eb96f09f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing raw_per table from SAV to dataframe.\n",
    "df_16_raw_per, meta_16_raw_per = pyrs.read_sav(\"UK LCF Data Sets/2016_17_rawper_ukanon.sav\")\n",
    "df_17_raw_per, meta_17_raw_per = pyrs.read_sav(\"UK LCF Data Sets/2017_rawper_ukanon_2017-18.sav\")\n",
    "df_18_raw_per, meta_18_raw_per = pyrs.read_sav(\"UK LCF Data Sets/2018_rawper_ukanon_final.sav\")\n",
    "df_19_raw_per, meta_19_raw_per = pyrs.read_sav(\"UK LCF Data Sets/lcfs_2019_rawper_ukanon_final.sav\")\n",
    "df_20_raw_per, meta_20_raw_per = pyrs.read_sav(\"UK LCF Data Sets/lcfs_2020_rawper_ukanon_final.sav\")\n",
    "df_21_raw_per, meta_21_raw_per = pyrs.read_sav(\"UK LCF Data Sets/lcfs_2021_rawper_ukanon_final.sav\")\n",
    "df_22_raw_per, meta_22_raw_per = pyrs.read_sav(\"UK LCF Data Sets/lcfs_2022_rawper_ukanon_final.sav\")\n",
    "\n",
    "#rename column WfhCor2 to WfhCor to match other tables\n",
    "df_21_raw_per.rename(columns={\"WfhCor2\": \"WfhCor\"}, inplace=True)\n",
    "df_22_raw_per.rename(columns={\"wfhcor2\": \"WfhCor\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37366af-f78b-4498-bf9c-381fe5df5145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create WfhCor columns for dataset prior 2020, set to 0\n",
    "df_16_raw_per['WfhCor'] = 0\n",
    "df_17_raw_per['WfhCor'] = 0 \n",
    "df_18_raw_per['WfhCor'] = 0 \n",
    "df_19_raw_per['WfhCor'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fca42e-9274-44a9-bc11-8d6d55eb6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting NaN values to 2 (which means no WFH).\n",
    "#w['female'] = w['female'].map({'female': 1, 'male': 0})\n",
    "#df_20_raw_per['WfhCor'] = df_20_raw_per['WfhCor'].fillna(0)\n",
    "df_20_raw_per['WfhCor'] = df_20_raw_per['WfhCor'].fillna(2)\n",
    "df_20_raw_per['WfhCor'] = df_20_raw_per['WfhCor'].map({1:1, 2:0})\n",
    "df_21_raw_per['WfhCor'] = df_21_raw_per['WfhCor'].fillna(2)\n",
    "df_21_raw_per['WfhCor'] = df_21_raw_per['WfhCor'].map({1:1, 2:0})\n",
    "df_22_raw_per['WfhCor'] = df_22_raw_per['WfhCor'].fillna(2)\n",
    "df_22_raw_per['WfhCor'] = df_22_raw_per['WfhCor'].map({1:1, 2:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93fbb5e6-8e15-4420-bffa-d581d89299e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ClaWfhCor_p\n",
    "\n",
    "#df_20_raw_per = df_20_raw_per.drop('ClaWfhCor_p', axis=1)\n",
    "df_21_raw_per = df_21_raw_per.drop('ClaWfhCor_p', axis=1)\n",
    "df_22_raw_per = df_22_raw_per.drop('ClaWfhCor_p', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a536231-a229-4d62-8a27-60245616a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 \n",
      "\n",
      "WfhCor\n",
      "0    9837\n",
      "1    2400\n",
      "Name: count, dtype: int64\n",
      "\n",
      " 2021 \n",
      "\n",
      "WfhCor\n",
      "0    10466\n",
      "1     2376\n",
      "Name: count, dtype: int64\n",
      "\n",
      " 2022 \n",
      "\n",
      "WfhCor\n",
      "0    8908\n",
      "1    1167\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('2020 \\n')\n",
    "print(df_20_raw_per['WfhCor'].value_counts())\n",
    "print('\\n 2021 \\n')\n",
    "print(df_21_raw_per['WfhCor'].value_counts())\n",
    "print('\\n 2022 \\n')\n",
    "print(df_22_raw_per['WfhCor'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23324e00-3825-48dd-8f34-bfc07637702b",
   "metadata": {},
   "source": [
    "new code v9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "810354bf-0341-4032-90b4-fc6cf7a20f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class WorkFromHomeFeatureSelector:\n",
    "    def __init__(self, data, target_column):\n",
    "        \"\"\"\n",
    "        Initialize the feature selector for work-from-home analysis\n",
    "        \n",
    "        Args:\n",
    "            data: pandas DataFrame with survey data\n",
    "            target_column: string name of the target variable (work from home)\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.target_column = target_column\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.feature_scores = {}\n",
    "        self.selected_features = {}\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess the survey data for machine learning\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        self.y = self.data[self.target_column]\n",
    "        self.X = self.data.drop(columns=[self.target_column])\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        categorical_cols = self.X.select_dtypes(include=['object']).columns\n",
    "        label_encoders = {}\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            self.X[col] = le.fit_transform(self.X[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "        \n",
    "        # Handle missing values\n",
    "        self.X = self.X.fillna(self.X.median())\n",
    "        \n",
    "        # Encode target variable if it's categorical\n",
    "        if self.y.dtype == 'object':\n",
    "            le_target = LabelEncoder()\n",
    "            self.y = le_target.fit_transform(self.y)\n",
    "        \n",
    "        print(f\"Data shape: {self.X.shape}\")\n",
    "        print(f\"Target distribution:\\n{pd.Series(self.y).value_counts()}\")\n",
    "        \n",
    "        return label_encoders\n",
    "    \n",
    "    def mutual_information_selection(self, k=20):\n",
    "        \"\"\"Select features based on mutual information\"\"\"\n",
    "        print(f\"\\n=== Mutual Information Feature Selection (top {k}) ===\")\n",
    "        \n",
    "        mi_scores = mutual_info_classif(self.X, self.y, random_state=42)\n",
    "        mi_scores_df = pd.DataFrame({\n",
    "            'feature': self.X.columns,\n",
    "            'mi_score': mi_scores\n",
    "        }).sort_values('mi_score', ascending=False)\n",
    "        \n",
    "        top_features = mi_scores_df.head(k)['feature'].tolist()\n",
    "        self.selected_features['mutual_info'] = top_features\n",
    "        self.feature_scores['mutual_info'] = mi_scores_df\n",
    "        \n",
    "        print(\"Top 10 features by Mutual Information:\")\n",
    "        print(mi_scores_df.head(10))\n",
    "        \n",
    "        return top_features\n",
    "    \n",
    "    def random_forest_selection(self, n_estimators=100, max_features=20):\n",
    "        \"\"\"Select features using Random Forest importance\"\"\"\n",
    "        print(f\"\\n=== Random Forest Feature Selection (top {max_features}) ===\")\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "        rf.fit(self.X, self.y)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': self.X.columns,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        top_features = importance_df.head(max_features)['feature'].tolist()\n",
    "        self.selected_features['random_forest'] = top_features\n",
    "        self.feature_scores['random_forest'] = importance_df\n",
    "        \n",
    "        print(\"Top 10 features by Random Forest importance:\")\n",
    "        print(importance_df.head(10))\n",
    "        \n",
    "        return top_features\n",
    "    \n",
    "    def xgboost_selection(self, max_features=20):\n",
    "        \"\"\"Feature selection using XGBoost importance\"\"\"\n",
    "        print(f\"\\n=== XGBoost Feature Selection (top {max_features}) ===\")\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "        xgb_model.fit(self.X, self.y)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': self.X.columns,\n",
    "            'importance': xgb_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        top_features = importance_df.head(max_features)['feature'].tolist()\n",
    "        self.selected_features['xgboost'] = top_features\n",
    "        self.feature_scores['xgboost'] = importance_df\n",
    "        \n",
    "        print(\"Top 10 features by XGBoost importance:\")\n",
    "        print(importance_df.head(10))\n",
    "        \n",
    "        return top_features\n",
    "    \n",
    "    def get_consensus_features(self, min_methods=2):\n",
    "        \"\"\"Get features selected by multiple methods\"\"\"\n",
    "        print(f\"\\n=== Consensus Features (selected by at least {min_methods} methods) ===\")\n",
    "        \n",
    "        all_features = []\n",
    "        for method, features in self.selected_features.items():\n",
    "            all_features.extend(features)\n",
    "        \n",
    "        feature_counts = pd.Series(all_features).value_counts()\n",
    "        consensus_features = feature_counts[feature_counts >= min_methods].index.tolist()\n",
    "        \n",
    "        print(f\"Features selected by {min_methods}+ methods:\")\n",
    "        for feature in consensus_features:\n",
    "            count = feature_counts[feature]\n",
    "            print(f\"  {feature}: selected by {count} methods\")\n",
    "        \n",
    "        self.selected_features['consensus'] = consensus_features\n",
    "        return consensus_features\n",
    "    \n",
    "    def evaluate_feature_sets(self):\n",
    "        \"\"\"Evaluate different feature selection methods\"\"\"\n",
    "        print(f\"\\n=== Evaluating Feature Selection Methods ===\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for method, features in self.selected_features.items():\n",
    "            if len(features) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Select features\n",
    "            X_selected = self.X[features]\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_selected, self.y, test_size=0.2, random_state=42, stratify=self.y\n",
    "            )\n",
    "            \n",
    "            # Train Random Forest\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = rf.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            cv_scores = cross_val_score(rf, X_selected, self.y, cv=5)\n",
    "            \n",
    "            results[method] = {\n",
    "                'n_features': len(features),\n",
    "                'test_accuracy': accuracy,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std()\n",
    "            }\n",
    "            \n",
    "            print(f\"{method}: {len(features)} features, \"\n",
    "                  f\"CV Score: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}, \"\n",
    "                  f\"Test Accuracy: {accuracy:.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_feature_importance(self, method='random_forest', top_n=15):\n",
    "        \"\"\"Plot feature importance for a given method\"\"\"\n",
    "        if method not in self.feature_scores:\n",
    "            print(f\"Method {method} not found. Available methods: {list(self.feature_scores.keys())}\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        if method == 'random_forest' or method == 'xgboost':\n",
    "            data = self.feature_scores[method].head(top_n)\n",
    "            plt.barh(data['feature'], data['importance'])\n",
    "            plt.xlabel('Feature Importance')\n",
    "        elif method == 'mutual_info':\n",
    "            data = self.feature_scores[method].head(top_n)\n",
    "            plt.barh(data['feature'], data['mi_score'])\n",
    "            plt.xlabel('Mutual Information Score')\n",
    "        \n",
    "        plt.title(f'Top {top_n} Features - {method.title()}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def train_final_model(self, method='consensus', model_type='random_forest'):\n",
    "        \"\"\"Train the final model using selected features\"\"\"\n",
    "        print(f\"\\n=== Training Final Model ===\")\n",
    "        \n",
    "        # Get selected features\n",
    "        if method not in self.selected_features:\n",
    "            print(f\"Method {method} not found. Using random_forest method.\")\n",
    "            method = 'random_forest'\n",
    "        \n",
    "        selected_features = self.selected_features[method]\n",
    "        print(f\"Training with {len(selected_features)} features from {method} method\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X_selected = self.X[selected_features]\n",
    "        \n",
    "        # Split for final evaluation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_selected, self.y, test_size=0.2, random_state=42, stratify=self.y\n",
    "        )\n",
    "        \n",
    "        # Choose model\n",
    "        if model_type == 'random_forest':\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=200, \n",
    "                max_depth=10, \n",
    "                min_samples_split=5,\n",
    "                random_state=42, \n",
    "                n_jobs=-1\n",
    "            )\n",
    "        elif model_type == 'xgboost':\n",
    "            model = xgb.XGBClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                eval_metric='logloss'\n",
    "            )\n",
    "        elif model_type == 'logistic':\n",
    "            # Scale features for logistic regression\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Store scaler for prediction\n",
    "            self.scaler = scaler\n",
    "            self.final_model = model\n",
    "            self.final_features = selected_features\n",
    "            self.model_type = model_type\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"Final model test accuracy: {accuracy:.3f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            \n",
    "            return model, selected_features\n",
    "        \n",
    "        # Train model (for non-logistic models)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Store model and features for prediction\n",
    "        self.final_model = model\n",
    "        self.final_features = selected_features\n",
    "        self.model_type = model_type\n",
    "        if model_type == 'logistic':\n",
    "            # Scaler already stored above\n",
    "            pass\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Final model test accuracy: {accuracy:.3f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        return model, selected_features\n",
    "    \n",
    "    def save_model_pipeline(self, filepath):\n",
    "        \"\"\"Save the complete model pipeline for future use\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        pipeline = {\n",
    "            'model': self.final_model,\n",
    "            'features': self.final_features,\n",
    "            'model_type': self.model_type,\n",
    "            'scaler': self.scaler,\n",
    "            'label_encoders': self.label_encoders,\n",
    "            'feature_medians': self.X[self.final_features].median().to_dict()\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "        \n",
    "        print(f\"Model pipeline saved to {filepath}\")\n",
    "    \n",
    "    def predict_new_data(self, new_data, handle_missing_features='impute'):\n",
    "        \"\"\"\n",
    "        Predict work-from-home capability for new data\n",
    "        \n",
    "        Args:\n",
    "            new_data: pandas DataFrame (may be missing some features)\n",
    "            handle_missing_features: str, how to handle missing features\n",
    "                - 'impute': Fill missing features with default values\n",
    "                - 'retrain': Use subset of available features and retrain\n",
    "                - 'error': Raise error if features are missing\n",
    "        \n",
    "        Returns:\n",
    "            predictions: array of predictions (0/1)\n",
    "            probabilities: array of prediction probabilities\n",
    "            feature_status: dict with information about missing features\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'final_model'):\n",
    "            raise ValueError(\"No trained model found. Run train_final_model() first.\")\n",
    "        \n",
    "        print(f\"Making predictions for {len(new_data)} samples...\")\n",
    "        \n",
    "        # Check for missing features\n",
    "        missing_features = set(self.final_features) - set(new_data.columns)\n",
    "        available_features = set(self.final_features) & set(new_data.columns)\n",
    "        \n",
    "        feature_status = {\n",
    "            'required_features': self.final_features,\n",
    "            'available_features': list(available_features),\n",
    "            'missing_features': list(missing_features),\n",
    "            'missing_count': len(missing_features),\n",
    "            'coverage': len(available_features) / len(self.final_features)\n",
    "        }\n",
    "        \n",
    "        print(f\"Feature coverage: {feature_status['coverage']:.1%} ({len(available_features)}/{len(self.final_features)})\")\n",
    "        \n",
    "        if missing_features:\n",
    "            print(f\"Missing features: {list(missing_features)}\")\n",
    "            \n",
    "            if handle_missing_features == 'error':\n",
    "                raise ValueError(f\"Missing required features: {list(missing_features)}\")\n",
    "            \n",
    "            elif handle_missing_features == 'retrain':\n",
    "                return self._predict_with_subset_features(new_data, list(available_features), feature_status)\n",
    "            \n",
    "            elif handle_missing_features == 'impute':\n",
    "                new_data_processed = self._preprocess_new_data_with_imputation(new_data, missing_features)\n",
    "            else:\n",
    "                raise ValueError(\"handle_missing_features must be 'impute', 'retrain', or 'error'\")\n",
    "        else:\n",
    "            # All features available\n",
    "            new_data_processed = self._preprocess_new_data(new_data)\n",
    "        \n",
    "        # Select only the features used in training\n",
    "        X_new = new_data_processed[self.final_features]\n",
    "\n",
    "        # Handle NaN values before scaling/prediction\n",
    "        if X_new.isnull().any().any():\n",
    "            print(\"Warning: Input contains NaN values. Handling them...\")\n",
    "            \n",
    "            # Show rows with NaN values\n",
    "            nan_rows = X_new.isnull().any(axis=1)\n",
    "\n",
    "                \n",
    "            # Option 1: Drop rows with NaN (if you can afford to lose some data)\n",
    "            #X_new = X_new.dropna()\n",
    "            \n",
    "            # Option 2: Fill NaN with appropriate values\n",
    "            # For numerical columns - use mean, median, or 0\n",
    "            X_new_clean = X_new.fillna(X_new.mean())  # or .median() or 0\n",
    "            X_new = X_new_clean.copy()\n",
    "            # Option 3: Use the same imputation strategy used during training\n",
    "            # (You should have saved the imputer used during training)\n",
    "            # X_new_clean = self.imputer.transform(X_new)\n",
    "            \n",
    "        else:\n",
    "            X_new\n",
    "    \n",
    "        # Scale if needed\n",
    "        if self.scaler is not None:\n",
    "            X_new_scaled = self.scaler.transform(X_new)\n",
    "            predictions = self.final_model.predict(X_new_scaled)\n",
    "            probabilities = self.final_model.predict_proba(X_new_scaled)[:, 1]\n",
    "        else:\n",
    "            predictions = self.final_model.predict(X_new)\n",
    "            probabilities = self.final_model.predict_proba(X_new)[:, 1]\n",
    "        \n",
    "        return predictions, probabilities, feature_status\n",
    "    \n",
    "    def _preprocess_new_data_with_imputation(self, new_data, missing_features):\n",
    "        \"\"\"Preprocess new data and impute missing features\"\"\"\n",
    "        new_data_copy = self._preprocess_new_data(new_data)\n",
    "        \n",
    "        print(f\"Imputing {len(missing_features)} missing features...\")\n",
    "        \n",
    "        # Impute missing features using various strategies\n",
    "        for feature in missing_features:\n",
    "            if feature in self.X.columns:\n",
    "                # Use training data statistics for imputation\n",
    "                if self.X[feature].dtype in ['int64', 'float64']:\n",
    "                    # Numerical feature - use median\n",
    "                    imputed_value = self.X[feature].median()\n",
    "                    print(f\"  {feature}: imputed with median value {imputed_value}\")\n",
    "                else:\n",
    "                    # Categorical feature - use mode\n",
    "                    imputed_value = self.X[feature].mode().iloc[0]\n",
    "                    print(f\"  {feature}: imputed with mode value '{imputed_value}'\")\n",
    "                \n",
    "                new_data_copy[feature] = imputed_value\n",
    "            else:\n",
    "                # Feature not in training data - use neutral value\n",
    "                print(f\"  {feature}: unknown feature, imputing with 0\")\n",
    "                new_data_copy[feature] = 0\n",
    "        \n",
    "        return new_data_copy\n",
    "    \n",
    "    def _predict_with_subset_features(self, new_data, available_features, feature_status):\n",
    "        \"\"\"Retrain model with available features and make predictions\"\"\"\n",
    "        print(f\"Retraining model with {len(available_features)} available features...\")\n",
    "        \n",
    "        if len(available_features) < 3:\n",
    "            raise ValueError(f\"Too few features available ({len(available_features)}). Need at least 3 features for reliable predictions.\")\n",
    "        \n",
    "        # Preprocess new data\n",
    "        new_data_processed = self._preprocess_new_data(new_data)\n",
    "        \n",
    "        # Train a new model with available features only\n",
    "        X_subset = self.X[available_features]\n",
    "        \n",
    "        # Use same model type as original\n",
    "        if self.model_type == 'random_forest':\n",
    "            subset_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        elif self.model_type == 'xgboost':\n",
    "            subset_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "        elif self.model_type == 'logistic':\n",
    "            subset_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            # Scale features for logistic regression\n",
    "            scaler = StandardScaler()\n",
    "            X_subset_scaled = scaler.fit_transform(X_subset)\n",
    "            subset_model.fit(X_subset_scaled, self.y)\n",
    "            \n",
    "            # Make predictions\n",
    "            X_new_subset = new_data_processed[available_features]\n",
    "            X_new_scaled = scaler.transform(X_new_subset)\n",
    "            predictions = subset_model.predict(X_new_scaled)\n",
    "            probabilities = subset_model.predict_proba(X_new_scaled)[:, 1]\n",
    "            \n",
    "            # Add warning about reduced performance\n",
    "            feature_status['warning'] = f\"Model retrained with {len(available_features)}/{len(self.final_features)} features. Performance may be reduced.\"\n",
    "            \n",
    "            return predictions, probabilities, feature_status\n",
    "        \n",
    "        # For non-logistic models\n",
    "        subset_model.fit(X_subset, self.y)\n",
    "        \n",
    "        # Make predictions\n",
    "        X_new_subset = new_data_processed[available_features]\n",
    "        predictions = subset_model.predict(X_new_subset)\n",
    "        probabilities = subset_model.predict_proba(X_new_subset)[:, 1]\n",
    "        \n",
    "        # Add warning about reduced performance\n",
    "        feature_status['warning'] = f\"Model retrained with {len(available_features)}/{len(self.final_features)} features. Performance may be reduced.\"\n",
    "        \n",
    "        return predictions, probabilities, feature_status\n",
    "    \n",
    "    def _preprocess_new_data(self, new_data):\n",
    "        \"\"\"Preprocess new data using the same transformations as training data\"\"\"\n",
    "        new_data_copy = new_data.copy()\n",
    "        \n",
    "        # Apply label encoders to categorical variables\n",
    "        if hasattr(self, 'label_encoders'):\n",
    "            for col, encoder in self.label_encoders.items():\n",
    "                if col in new_data_copy.columns:\n",
    "                    # Handle unseen categories\n",
    "                    try:\n",
    "                        new_data_copy[col] = encoder.transform(new_data_copy[col].astype(str))\n",
    "                    except ValueError:\n",
    "                        # For unseen categories, assign the most frequent class\n",
    "                        print(f\"Warning: Unseen categories in {col}. Using most frequent class.\")\n",
    "                        most_frequent = encoder.classes_[0]  # First class (most frequent during fit)\n",
    "                        new_data_copy[col] = new_data_copy[col].apply(\n",
    "                            lambda x: x if str(x) in encoder.classes_ else most_frequent\n",
    "                        )\n",
    "                        new_data_copy[col] = encoder.transform(new_data_copy[col].astype(str))\n",
    "        \n",
    "        # Handle missing values using training medians\n",
    "        if hasattr(self, 'feature_medians'):\n",
    "            for col in new_data_copy.columns:\n",
    "                if col in self.feature_medians:\n",
    "                    new_data_copy[col] = new_data_copy[col].fillna(self.feature_medians[col])\n",
    "                else:\n",
    "                    new_data_copy[col] = new_data_copy[col].fillna(new_data_copy[col].median())\n",
    "        \n",
    "        return new_data_copy\n",
    "    \n",
    "    def get_feature_importance_for_missing(self, missing_features):\n",
    "        \"\"\"Get importance scores for missing features to assess impact\"\"\"\n",
    "        if not hasattr(self, 'feature_scores'):\n",
    "            return None\n",
    "        \n",
    "        importance_info = {}\n",
    "        \n",
    "        # Check Random Forest importance\n",
    "        if 'random_forest' in self.feature_scores:\n",
    "            rf_scores = self.feature_scores['random_forest'].set_index('feature')\n",
    "            for feature in missing_features:\n",
    "                if feature in rf_scores.index:\n",
    "                    importance_info[feature] = {\n",
    "                        'rf_importance': rf_scores.loc[feature, 'importance'],\n",
    "                        'rf_rank': rf_scores.index.get_loc(feature) + 1\n",
    "                    }\n",
    "        \n",
    "        # Check XGBoost importance\n",
    "        if 'xgboost' in self.feature_scores:\n",
    "            xgb_scores = self.feature_scores['xgboost'].set_index('feature')\n",
    "            for feature in missing_features:\n",
    "                if feature in xgb_scores.index:\n",
    "                    if feature not in importance_info:\n",
    "                        importance_info[feature] = {}\n",
    "                    importance_info[feature]['xgb_importance'] = xgb_scores.loc[feature, 'importance']\n",
    "                    importance_info[feature]['xgb_rank'] = xgb_scores.index.get_loc(feature) + 1\n",
    "        \n",
    "        # Check Mutual Information\n",
    "        if 'mutual_info' in self.feature_scores:\n",
    "            mi_scores = self.feature_scores['mutual_info'].set_index('feature')\n",
    "            for feature in missing_features:\n",
    "                if feature in mi_scores.index:\n",
    "                    if feature not in importance_info:\n",
    "                        importance_info[feature] = {}\n",
    "                    importance_info[feature]['mi_score'] = mi_scores.loc[feature, 'mi_score']\n",
    "                    importance_info[feature]['mi_rank'] = mi_scores.index.get_loc(feature) + 1\n",
    "        \n",
    "        return importance_info\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run the complete feature selection pipeline\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"WORK FROM HOME FEATURE SELECTION ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Preprocess data\n",
    "        self.label_encoders = self.preprocess_data()\n",
    "        \n",
    "        # Run feature selection methods\n",
    "        self.mutual_information_selection()\n",
    "        self.random_forest_selection()\n",
    "        self.xgboost_selection()\n",
    "        \n",
    "        # Get consensus features\n",
    "        self.get_consensus_features()\n",
    "        \n",
    "        # Evaluate methods\n",
    "        results = self.evaluate_feature_sets()\n",
    "        \n",
    "        return results\n",
    "\n",
    "def load_saved_model(filepath):\n",
    "    \"\"\"Load a previously saved model pipeline\"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def predict_with_saved_model(pipeline, new_data, handle_missing_features='impute'):\n",
    "    \"\"\"Make predictions using a saved model pipeline with missing feature handling\"\"\"\n",
    "    model = pipeline['model']\n",
    "    features = pipeline['features']\n",
    "    scaler = pipeline['scaler']\n",
    "    label_encoders = pipeline['label_encoders']\n",
    "    feature_medians = pipeline['feature_medians']\n",
    "    \n",
    "    # Check for missing features\n",
    "    missing_features = set(features) - set(new_data.columns)\n",
    "    available_features = set(features) & set(new_data.columns)\n",
    "    \n",
    "    feature_status = {\n",
    "        'required_features': features,\n",
    "        'available_features': list(available_features),\n",
    "        'missing_features': list(missing_features),\n",
    "        'missing_count': len(missing_features),\n",
    "        'coverage': len(available_features) / len(features)\n",
    "    }\n",
    "    \n",
    "    print(f\"Feature coverage: {feature_status['coverage']:.1%} ({len(available_features)}/{len(features)})\")\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"Missing features: {list(missing_features)}\")\n",
    "        \n",
    "        if handle_missing_features == 'error':\n",
    "            raise ValueError(f\"Missing required features: {list(missing_features)}\")\n",
    "        elif handle_missing_features == 'impute':\n",
    "            new_data_copy = _impute_missing_features_saved_model(\n",
    "                new_data, missing_features, label_encoders, feature_medians, features\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"For saved models, only 'impute' and 'error' modes are supported\")\n",
    "    else:\n",
    "        new_data_copy = new_data.copy()\n",
    "    \n",
    "    # Preprocess data\n",
    "    for col, encoder in label_encoders.items():\n",
    "        if col in new_data_copy.columns:\n",
    "            try:\n",
    "                new_data_copy[col] = encoder.transform(new_data_copy[col].astype(str))\n",
    "            except ValueError:\n",
    "                most_frequent = encoder.classes_[0]\n",
    "                new_data_copy[col] = new_data_copy[col].apply(\n",
    "                    lambda x: x if str(x) in encoder.classes_ else most_frequent\n",
    "                )\n",
    "                new_data_copy[col] = encoder.transform(new_data_copy[col].astype(str))\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in features:\n",
    "        if col in new_data_copy.columns:\n",
    "            new_data_copy[col] = new_data_copy[col].fillna(feature_medians.get(col, 0))\n",
    "    \n",
    "    # Select features and make predictions\n",
    "    X_new = new_data_copy[features]\n",
    "    \n",
    "    if scaler is not None:\n",
    "        X_new_scaled = scaler.transform(X_new)\n",
    "        predictions = model.predict(X_new_scaled)\n",
    "        probabilities = model.predict_proba(X_new_scaled)[:, 1]\n",
    "    else:\n",
    "        predictions = model.predict(X_new)\n",
    "        probabilities = model.predict_proba(X_new)[:, 1]\n",
    "    \n",
    "    return predictions, probabilities, feature_status\n",
    "\n",
    "def _impute_missing_features_saved_model(new_data, missing_features, label_encoders, feature_medians, all_features):\n",
    "    \"\"\"Impute missing features for saved model predictions\"\"\"\n",
    "    new_data_copy = new_data.copy()\n",
    "    \n",
    "    print(f\"Imputing {len(missing_features)} missing features...\")\n",
    "    \n",
    "    for feature in missing_features:\n",
    "        if feature in feature_medians:\n",
    "            # Use stored median/mode from training\n",
    "            imputed_value = feature_medians[feature]\n",
    "            print(f\"  {feature}: imputed with training value {imputed_value}\")\n",
    "        else:\n",
    "            # Use neutral value for unknown features\n",
    "            imputed_value = 0\n",
    "            print(f\"  {feature}: unknown feature, imputing with 0\")\n",
    "        \n",
    "        new_data_copy[feature] = imputed_value\n",
    "    \n",
    "    return new_data_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5a1ca-9b89-4082-a03e-2afda8e623fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage using real data\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Complete example showing training and prediction workflow\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create sample training data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    \n",
    "    df_train = df_20_raw_per.copy()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPLETE WORK-FROM-HOME PREDICTION WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # STEP 1: Feature Selection and Model Training\n",
    "    print(\"\\n1. TRAINING PHASE\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    selector = WorkFromHomeFeatureSelector(df_train, 'WfhCor')\n",
    "    results = selector.run_complete_analysis()\n",
    "    \n",
    "    # Train final model\n",
    "    model, features = selector.train_final_model(method='consensus', model_type='random_forest')\n",
    "    \n",
    "    # Save model for future use\n",
    "    selector.save_model_pipeline('wfh_model.pkl')\n",
    "    \n",
    "     # STEP 2: Create new unseen data for prediction\n",
    "    print(\"\\n2. PREDICTION ON NEW DATA\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Simulate new data (same structure but without target variable)\n",
    "    np.random.seed(123)  # Different seed for new data\n",
    "    \n",
    "    \n",
    "    df_new = df_19_raw_per.copy()\n",
    "    \n",
    "    # Method 1: Using the trained selector object\n",
    "    predictions, probabilities, features = selector.predict_new_data(df_new)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = df_new.copy()\n",
    "    results_df['predicted_wfh'] = predictions\n",
    "    results_df['wfh_probability'] = probabilities\n",
    "    results_df['wfh_prediction'] = results_df['predicted_wfh'].map({0: 'Cannot WFH', 1: 'Can WFH'})\n",
    "    \n",
    "    print(f\"Predictions completed for {len(df_new)} samples\")\n",
    "    print(f\"Predicted WFH capability: {predictions.sum()} out of {len(predictions)} ({predictions.mean():.1%})\")\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print('\\nIf stopped here, here is the error')\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(results_df[[ 'wfh_probability']].head(10))\n",
    "    print(results_df[[ 'wfh_prediction']].head(10))\n",
    "#    print(results_df[['age', 'education_level', 'occupation_category', 'wfh_prediction', 'wfh_probability']].head(10))\n",
    "    \n",
    "    # STEP 3: Using saved model (for production use)\n",
    "    print(\"\\n3. USING SAVED MODEL\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Load saved model\n",
    "    saved_pipeline = load_saved_model('wfh_model.pkl')\n",
    "    \n",
    "    # Make predictions with saved model\n",
    "    saved_predictions, saved_probabilities, saved_features = predict_with_saved_model(saved_pipeline, df_new)\n",
    "    \n",
    "    print(f\"Saved model predictions match: {np.array_equal(predictions, saved_predictions)}\")\n",
    "    \n",
    "    # Return the selector and results for further use\n",
    "    return selector, results_df\n",
    "\n",
    "\n",
    "def missing_features_example():\n",
    "    \"\"\"\n",
    "    Demonstrate handling of missing features in new data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MISSING FEATURES HANDLING EXAMPLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create training data with more features\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    \n",
    "    df_train = df_20_raw_per.copy()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"1. Training model with full feature set...\")\n",
    "    selector = WorkFromHomeFeatureSelector(df_train, 'WfhCor')\n",
    "    selector.run_complete_analysis()\n",
    "    model, features = selector.train_final_model(method='consensus')\n",
    "    \n",
    "    print(f\"Selected features: {features}\")\n",
    "    \n",
    "    # Create new data with missing features\n",
    "    np.random.seed(123)\n",
    "    n_new = 50\n",
    "    \n",
    "    # Scenario 1: Missing some features\n",
    "    incomplete_data = df_19_raw_per.copy()\n",
    "    \n",
    "    print(f\"\\n2. Testing with incomplete data ({incomplete_data.shape[1]} out of {len(features)} features)...\")\n",
    "    \n",
    "    # Method 1: Impute missing features (recommended)\n",
    "    print(\"\\n--- Method 1: Impute Missing Features ---\")\n",
    "    try:\n",
    "        predictions1, probabilities1, status1 = selector.predict_new_data(\n",
    "            incomplete_data, handle_missing_features='impute'\n",
    "        )\n",
    "        print(f\"Success! Predictions made for {len(predictions1)} samples\")\n",
    "        print(f\"WFH Rate: {predictions1.mean():.1%}\")\n",
    "        print(f\"Average confidence: {probabilities1.mean():.1%}\")\n",
    "        \n",
    "        # Show which features were imputed\n",
    "        if status1['missing_features']:\n",
    "            print(\"Imputed features and their importance:\")\n",
    "            importance_info = selector.get_feature_importance_for_missing(status1['missing_features'])\n",
    "            for feature in status1['missing_features']:\n",
    "                if importance_info and feature in importance_info:\n",
    "                    print(f\"  {feature}: RF importance = {importance_info[feature]['rf_importance']:.3f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with imputation: {e}\")\n",
    "    \n",
    "    # Method 2: Retrain with available features\n",
    "    print(\"\\n--- Method 2: Retrain with Available Features ---\")\n",
    "    try:\n",
    "        predictions2, probabilities2, status2 = selector.predict_new_data(\n",
    "            incomplete_data, handle_missing_features='retrain'\n",
    "        )\n",
    "        print(f\"Success! Predictions made for {len(predictions2)} samples\")\n",
    "        print(f\"WFH Rate: {predictions2.mean():.1%}\")\n",
    "        print(f\"Average confidence: {probabilities2.mean():.1%}\")\n",
    "        if 'warning' in status2:\n",
    "            print(f\"Warning: {status2['warning']}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with retraining: {e}\")\n",
    "    \n",
    "    # Method 3: Error on missing features\n",
    "    print(\"\\n--- Method 3: Strict Mode (Error on Missing) ---\")\n",
    "    try:\n",
    "        predictions3, probabilities3, status3 = selector.predict_new_data(\n",
    "            incomplete_data, handle_missing_features='error'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Expected error: {e}\")\n",
    "    \n",
    "    # Scenario 2: Extremely limited data\n",
    "    print(f\"\\n3. Testing with very limited data...\")\n",
    "    \n",
    "    very_limited_data = pd.DataFrame({\n",
    "        'age': np.random.randint(25, 45, n_new),\n",
    "        'education_level': np.random.choice(['Bachelor', 'Master'], n_new),\n",
    "        # Only 2 features available\n",
    "    })\n",
    "    \n",
    "    print(f\"Very limited data: {very_limited_data.shape[1]} features\")\n",
    "    \n",
    "    try:\n",
    "        predictions_limited, _, status_limited = selector.predict_new_data(\n",
    "            very_limited_data, handle_missing_features='impute'\n",
    "        )\n",
    "        print(f\"Success with heavy imputation! Coverage: {status_limited['coverage']:.1%}\")\n",
    "        print(f\"WFH Rate: {predictions_limited.mean():.1%}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with very limited data: {e}\")\n",
    "    \n",
    "    # Scenario 3: Using saved model with missing features\n",
    "    print(f\"\\n4. Testing saved model with missing features...\")\n",
    "    \n",
    "    # Save and reload model\n",
    "    selector.save_model_pipeline('wfh_model_missing_test.pkl')\n",
    "    pipeline = load_saved_model('wfh_model_missing_test.pkl')\n",
    "    \n",
    "    try:\n",
    "        predictions_saved, probabilities_saved, status_saved = predict_with_saved_model(\n",
    "            pipeline, incomplete_data, handle_missing_features='impute'\n",
    "        )\n",
    "        print(f\"Saved model success! Coverage: {status_saved['coverage']:.1%}\")\n",
    "        print(f\"Results match: {np.array_equal(predictions1, predictions_saved)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with saved model: {e}\")\n",
    "    \n",
    "    return incomplete_data, predictions1, probabilities1, status1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run complete example\n",
    "    selector, results = main()\n",
    "    \n",
    "    # Demonstrate missing features handling\n",
    "    missing_features_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68c2009e-dd39-49b4-9343-dca34fb92174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MISSING FEATURES HANDLING EXAMPLE\n",
      "============================================================\n",
      "1. Training model with full feature set...\n",
      "============================================================\n",
      "WORK FROM HOME FEATURE SELECTION ANALYSIS\n",
      "============================================================\n",
      "Preprocessing data...\n",
      "Data shape: (12237, 943)\n",
      "Target distribution:\n",
      "WfhCor\n",
      "0    9837\n",
      "1    2400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Mutual Information Feature Selection (top 20) ===\n",
      "Top 10 features by Mutual Information:\n",
      "         feature  mi_score\n",
      "888       DVPayp  0.220942\n",
      "187       DVnino  0.215836\n",
      "907  dvgrosspayp  0.213103\n",
      "900      DVTAX1p  0.212746\n",
      "871      SICCODE  0.162705\n",
      "876       EmpStY  0.158682\n",
      "78        Wrking  0.144918\n",
      "915      GWkIncp  0.122190\n",
      "916      dvage_p  0.119450\n",
      "99        DVUsHr  0.118391\n",
      "\n",
      "=== Random Forest Feature Selection (top 20) ===\n",
      "Top 10 features by Random Forest importance:\n",
      "         feature  importance\n",
      "900      DVTAX1p    0.043345\n",
      "915      GWkIncp    0.038873\n",
      "907  dvgrosspayp    0.036841\n",
      "888       DVPayp    0.036767\n",
      "187       DVnino    0.036607\n",
      "78        Wrking    0.025817\n",
      "871      SICCODE    0.025578\n",
      "906    dedpenamp    0.023232\n",
      "197      PyDVPen    0.019757\n",
      "97         UOTHr    0.019734\n",
      "\n",
      "=== XGBoost Feature Selection (top 20) ===\n",
      "Top 10 features by XGBoost importance:\n",
      "         feature  importance\n",
      "78        Wrking    0.288269\n",
      "907  dvgrosspayp    0.122805\n",
      "79        JbAway    0.064038\n",
      "214      GrossPd    0.026460\n",
      "931  ClaWfhCor_p    0.020320\n",
      "91       DVILO4a    0.017826\n",
      "105         Stat    0.017747\n",
      "86        Looked    0.014644\n",
      "182        NetPd    0.014071\n",
      "921     MarSta_p    0.012210\n",
      "\n",
      "=== Consensus Features (selected by at least 2 methods) ===\n",
      "Features selected by 2+ methods:\n",
      "  DVAge18: selected by 3 methods\n",
      "  dvgrosspayp: selected by 3 methods\n",
      "  SICCODE: selected by 3 methods\n",
      "  DVILO4a: selected by 3 methods\n",
      "  Wrking: selected by 3 methods\n",
      "  DVILO3a: selected by 3 methods\n",
      "  DVPayp: selected by 2 methods\n",
      "  ClaWfhCor_p: selected by 2 methods\n",
      "  DVnino: selected by 2 methods\n",
      "  dedpenamp: selected by 2 methods\n",
      "  PyDVPen: selected by 2 methods\n",
      "  JDate: selected by 2 methods\n",
      "  EdAgeCor: selected by 2 methods\n",
      "  DVUsHr: selected by 2 methods\n",
      "  dvage_p: selected by 2 methods\n",
      "  GWkIncp: selected by 2 methods\n",
      "  EmpStY: selected by 2 methods\n",
      "  DVTAX1p: selected by 2 methods\n",
      "  UOTHr: selected by 2 methods\n",
      "\n",
      "=== Evaluating Feature Selection Methods ===\n",
      "mutual_info: 20 features, CV Score: 0.880 ± 0.007, Test Accuracy: 0.887\n",
      "random_forest: 20 features, CV Score: 0.895 ± 0.005, Test Accuracy: 0.902\n",
      "xgboost: 20 features, CV Score: 0.898 ± 0.005, Test Accuracy: 0.896\n",
      "consensus: 19 features, CV Score: 0.895 ± 0.005, Test Accuracy: 0.898\n",
      "\n",
      "=== Training Final Model ===\n",
      "Training with 19 features from consensus method\n",
      "Final model test accuracy: 0.895\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      1968\n",
      "           1       0.75      0.70      0.73       480\n",
      "\n",
      "    accuracy                           0.90      2448\n",
      "   macro avg       0.84      0.82      0.83      2448\n",
      "weighted avg       0.89      0.90      0.89      2448\n",
      "\n",
      "Selected features: ['DVAge18', 'dvgrosspayp', 'SICCODE', 'DVILO4a', 'Wrking', 'DVILO3a', 'DVPayp', 'ClaWfhCor_p', 'DVnino', 'dedpenamp', 'PyDVPen', 'JDate', 'EdAgeCor', 'DVUsHr', 'dvage_p', 'GWkIncp', 'EmpStY', 'DVTAX1p', 'UOTHr']\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate handling of missing features in new data\n",
    "# Train the model\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING FEATURES HANDLING EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create training data with more features\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "df_train = df_20_raw_per.copy()\n",
    "\n",
    "# Train model\n",
    "print(\"1. Training model with full feature set...\")\n",
    "selector = WorkFromHomeFeatureSelector(df_train, 'WfhCor')\n",
    "selector.run_complete_analysis()\n",
    "model, features = selector.train_final_model(method='consensus')\n",
    "\n",
    "print(f\"Selected features: {features}\")\n",
    "\n",
    "# Create new data with missing features\n",
    "np.random.seed(123)\n",
    "n_new = 50\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af2fadd9-b1e5-4502-8447-a0d2462539e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Testing with incomplete data (945 out of 19 features)...\n",
      "\n",
      "--- Method 1: Impute Missing Features ---\n",
      "Making predictions for 12658 samples...\n",
      "Feature coverage: 94.7% (18/19)\n",
      "Missing features: ['ClaWfhCor_p']\n",
      "Imputing 1 missing features...\n",
      "  ClaWfhCor_p: imputed with median value 2.0\n",
      "Warning: Input contains NaN values. Handling them...\n",
      "Success! Predictions made for 12658 samples\n",
      "WFH Rate: 22.9%\n",
      "Average confidence: 44.9%\n",
      "\n",
      "If stopped here, here is the error\n",
      "\n",
      "Sample Predictions:\n",
      "   wfh_probability\n",
      "0         0.488865\n",
      "1         0.330214\n",
      "2         0.394589\n",
      "3         0.183176\n",
      "4         0.601255\n",
      "5         0.336407\n",
      "6         0.557354\n",
      "7         0.574756\n",
      "8         0.373730\n",
      "9         0.401591\n",
      "   predicted_wfh\n",
      "0              0\n",
      "1              0\n",
      "2              0\n",
      "3              0\n",
      "4              1\n",
      "5              0\n",
      "6              1\n",
      "7              1\n",
      "8              0\n",
      "9              0\n",
      "Imputed features and their importance:\n",
      "  ClaWfhCor_p: RF importance = 0.016\n"
     ]
    }
   ],
   "source": [
    "# Predicting the WFH indicator of 2019 data\n",
    "\n",
    "# Scenario 1: Missing some features\n",
    "incomplete_data = df_19_raw_per.copy()\n",
    "\n",
    "print(f\"\\n2. Testing with incomplete data ({incomplete_data.shape[1]} out of {len(features)} features)...\")\n",
    "\n",
    "# Method 1: Impute missing features (recommended)\n",
    "print(\"\\n--- Method 1: Impute Missing Features ---\")\n",
    "try:\n",
    "    predictions1, probabilities1, status1 = selector.predict_new_data(\n",
    "        incomplete_data, handle_missing_features='impute'\n",
    "    )\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = incomplete_data.copy()\n",
    "    results_df['predicted_wfh'] = predictions1\n",
    "    results_df['wfh_probability'] = probabilities1\n",
    "    #results_df['wfh_prediction'] = results_df['predicted_wfh'].map({0: 'Cannot WFH', 1: 'Can WFH'})\n",
    "    \n",
    "    print(f\"Success! Predictions made for {len(predictions1)} samples\")\n",
    "    print(f\"WFH Rate: {predictions1.mean():.1%}\")\n",
    "    print(f\"Average confidence: {probabilities1.mean():.1%}\")\n",
    "    \n",
    "    # Show predictions\n",
    "    print('\\nIf stopped here, here is the error')\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(results_df[[ 'wfh_probability']].head(10))\n",
    "    print(results_df[[ 'predicted_wfh']].head(10))\n",
    "    \n",
    "    # Show which features were imputed\n",
    "    if status1['missing_features']:\n",
    "        print(\"Imputed features and their importance:\")\n",
    "        importance_info = selector.get_feature_importance_for_missing(status1['missing_features'])\n",
    "        for feature in status1['missing_features']:\n",
    "            if importance_info and feature in importance_info:\n",
    "                print(f\"  {feature}: RF importance = {importance_info[feature]['rf_importance']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with imputation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b914941-3a00-4afb-b8b4-33ed0373b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pseudo 2019 data\n",
    "df_19_raw_per_pseudo = results_df.copy()\n",
    "pyrs.write_sav(df_19_raw_per_pseudo, 'lcfs_2019_rawper_pseudo.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "564310c2-29e1-4e60-b433-afa2045f4b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Testing with incomplete data (970 out of 19 features)...\n",
      "\n",
      "--- Method 1: Impute Missing Features ---\n",
      "Making predictions for 12763 samples...\n",
      "Feature coverage: 94.7% (18/19)\n",
      "Missing features: ['ClaWfhCor_p']\n",
      "Imputing 1 missing features...\n",
      "  ClaWfhCor_p: imputed with median value 2.0\n",
      "Warning: Input contains NaN values. Handling them...\n",
      "Success! Predictions made for 12763 samples\n",
      "WFH Rate: 19.4%\n",
      "Average confidence: 42.7%\n",
      "\n",
      "If stopped here, here is the error\n",
      "\n",
      "Sample Predictions:\n",
      "   wfh_probability\n",
      "0         0.635521\n",
      "1         0.573231\n",
      "2         0.581858\n",
      "3         0.619152\n",
      "4         0.437720\n",
      "5         0.437720\n",
      "6         0.251349\n",
      "7         0.177090\n",
      "8         0.398594\n",
      "9         0.346051\n",
      "   predicted_wfh\n",
      "0              1\n",
      "1              1\n",
      "2              1\n",
      "3              1\n",
      "4              0\n",
      "5              0\n",
      "6              0\n",
      "7              0\n",
      "8              0\n",
      "9              0\n",
      "Imputed features and their importance:\n",
      "  ClaWfhCor_p: RF importance = 0.016\n"
     ]
    }
   ],
   "source": [
    "# Predicting the WFH indicator of 2018 data\n",
    "\n",
    "# Scenario 1: Missing some features\n",
    "incomplete_data = df_18_raw_per.copy()\n",
    "\n",
    "print(f\"\\n2. Testing with incomplete data ({incomplete_data.shape[1]} out of {len(features)} features)...\")\n",
    "\n",
    "# Method 1: Impute missing features (recommended)\n",
    "print(\"\\n--- Method 1: Impute Missing Features ---\")\n",
    "try:\n",
    "    predictions1, probabilities1, status1 = selector.predict_new_data(\n",
    "        incomplete_data, handle_missing_features='impute'\n",
    "    )\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = incomplete_data.copy()\n",
    "    results_df['predicted_wfh'] = predictions1\n",
    "    results_df['wfh_probability'] = probabilities1\n",
    "    #results_df['wfh_prediction'] = results_df['predicted_wfh'].map({0: 'Cannot WFH', 1: 'Can WFH'})\n",
    "    \n",
    "    print(f\"Success! Predictions made for {len(predictions1)} samples\")\n",
    "    print(f\"WFH Rate: {predictions1.mean():.1%}\")\n",
    "    print(f\"Average confidence: {probabilities1.mean():.1%}\")\n",
    "    \n",
    "    # Show predictions\n",
    "    print('\\nIf stopped here, here is the error')\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(results_df[[ 'wfh_probability']].head(10))\n",
    "    print(results_df[[ 'predicted_wfh']].head(10))\n",
    "    \n",
    "    # Show which features were imputed\n",
    "    if status1['missing_features']:\n",
    "        print(\"Imputed features and their importance:\")\n",
    "        importance_info = selector.get_feature_importance_for_missing(status1['missing_features'])\n",
    "        for feature in status1['missing_features']:\n",
    "            if importance_info and feature in importance_info:\n",
    "                print(f\"  {feature}: RF importance = {importance_info[feature]['rf_importance']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with imputation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdcfcba9-e192-4b32-adfb-fad2b9281702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pseudo 2018 data\n",
    "df_18_raw_per_pseudo = results_df.copy()\n",
    "pyrs.write_sav(df_18_raw_per_pseudo, 'lcfs_2018_rawper_pseudo.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d57f66bb-2ef8-4a4e-a27c-59611b5ac1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Testing with incomplete data (894 out of 19 features)...\n",
      "\n",
      "--- Method 1: Impute Missing Features ---\n",
      "Making predictions for 12753 samples...\n",
      "Feature coverage: 78.9% (15/19)\n",
      "Missing features: ['dvgrosspayp', 'dedpenamp', 'ClaWfhCor_p', 'EdAgeCor']\n",
      "Imputing 4 missing features...\n",
      "  dvgrosspayp: imputed with median value 473.11\n",
      "  dedpenamp: imputed with median value 100.0\n",
      "  ClaWfhCor_p: imputed with median value 2.0\n",
      "  EdAgeCor: imputed with median value 18.0\n",
      "Warning: Input contains NaN values. Handling them...\n",
      "Success! Predictions made for 12753 samples\n",
      "WFH Rate: 20.8%\n",
      "Average confidence: 41.3%\n",
      "\n",
      "If stopped here, here is the error\n",
      "\n",
      "Sample Predictions:\n",
      "   wfh_probability\n",
      "0         0.386917\n",
      "1         0.337589\n",
      "2         0.411641\n",
      "3         0.473323\n",
      "4         0.467454\n",
      "5         0.411641\n",
      "6         0.411641\n",
      "7         0.310489\n",
      "8         0.312494\n",
      "9         0.306239\n",
      "   predicted_wfh\n",
      "0              0\n",
      "1              0\n",
      "2              0\n",
      "3              0\n",
      "4              0\n",
      "5              0\n",
      "6              0\n",
      "7              0\n",
      "8              0\n",
      "9              0\n",
      "Imputed features and their importance:\n",
      "  dvgrosspayp: RF importance = 0.037\n",
      "  dedpenamp: RF importance = 0.023\n",
      "  ClaWfhCor_p: RF importance = 0.016\n",
      "  EdAgeCor: RF importance = 0.019\n"
     ]
    }
   ],
   "source": [
    "# Predicting the WFH indicator of 2017 data\n",
    "\n",
    "# Scenario 1: Missing some features\n",
    "incomplete_data = df_17_raw_per.copy()\n",
    "\n",
    "print(f\"\\n2. Testing with incomplete data ({incomplete_data.shape[1]} out of {len(features)} features)...\")\n",
    "\n",
    "# Method 1: Impute missing features (recommended)\n",
    "print(\"\\n--- Method 1: Impute Missing Features ---\")\n",
    "try:\n",
    "    predictions1, probabilities1, status1 = selector.predict_new_data(\n",
    "        incomplete_data, handle_missing_features='impute'\n",
    "    )\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = incomplete_data.copy()\n",
    "    results_df['predicted_wfh'] = predictions1\n",
    "    results_df['wfh_probability'] = probabilities1\n",
    "    #results_df['wfh_prediction'] = results_df['predicted_wfh'].map({0: 'Cannot WFH', 1: 'Can WFH'})\n",
    "    \n",
    "    print(f\"Success! Predictions made for {len(predictions1)} samples\")\n",
    "    print(f\"WFH Rate: {predictions1.mean():.1%}\")\n",
    "    print(f\"Average confidence: {probabilities1.mean():.1%}\")\n",
    "    \n",
    "    # Show predictions\n",
    "    print('\\nIf stopped here, here is the error')\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(results_df[[ 'wfh_probability']].head(10))\n",
    "    print(results_df[[ 'predicted_wfh']].head(10))\n",
    "    \n",
    "    # Show which features were imputed\n",
    "    if status1['missing_features']:\n",
    "        print(\"Imputed features and their importance:\")\n",
    "        importance_info = selector.get_feature_importance_for_missing(status1['missing_features'])\n",
    "        for feature in status1['missing_features']:\n",
    "            if importance_info and feature in importance_info:\n",
    "                print(f\"  {feature}: RF importance = {importance_info[feature]['rf_importance']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with imputation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4070c261-b955-4551-a9fa-eeca43cbf0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pseudo 2017 data\n",
    "df_17_raw_per_pseudo = results_df.copy()\n",
    "pyrs.write_sav(df_17_raw_per_pseudo, 'lcfs_2017_rawper_pseudo.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f730362-5ef6-49f5-85ee-d6b884abdb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Testing with incomplete data (1301 out of 19 features)...\n",
      "\n",
      "--- Method 1: Impute Missing Features ---\n",
      "Making predictions for 11938 samples...\n",
      "Feature coverage: 73.7% (14/19)\n",
      "Missing features: ['ClaWfhCor_p', 'GWkIncp', 'dedpenamp', 'dvgrosspayp', 'EdAgeCor']\n",
      "Imputing 5 missing features...\n",
      "  ClaWfhCor_p: imputed with median value 2.0\n",
      "  GWkIncp: imputed with median value 394.33000000000004\n",
      "  dedpenamp: imputed with median value 100.0\n",
      "  dvgrosspayp: imputed with median value 473.11\n",
      "  EdAgeCor: imputed with median value 18.0\n",
      "Warning: Input contains NaN values. Handling them...\n",
      "Success! Predictions made for 11938 samples\n",
      "WFH Rate: 15.3%\n",
      "Average confidence: 38.3%\n",
      "\n",
      "If stopped here, here is the error\n",
      "\n",
      "Sample Predictions:\n",
      "   wfh_probability\n",
      "0         0.489804\n",
      "1         0.263577\n",
      "2         0.520864\n",
      "3         0.153445\n",
      "4         0.531869\n",
      "5         0.395250\n",
      "6         0.273157\n",
      "7         0.311068\n",
      "8         0.557119\n",
      "9         0.259474\n",
      "   predicted_wfh\n",
      "0              0\n",
      "1              0\n",
      "2              1\n",
      "3              0\n",
      "4              1\n",
      "5              0\n",
      "6              0\n",
      "7              0\n",
      "8              1\n",
      "9              0\n",
      "Imputed features and their importance:\n",
      "  ClaWfhCor_p: RF importance = 0.016\n",
      "  GWkIncp: RF importance = 0.039\n",
      "  dedpenamp: RF importance = 0.023\n",
      "  dvgrosspayp: RF importance = 0.037\n",
      "  EdAgeCor: RF importance = 0.019\n"
     ]
    }
   ],
   "source": [
    "# Predicting the WFH indicator of 2016 data\n",
    "\n",
    "# Scenario 1: Missing some features\n",
    "incomplete_data = df_16_raw_per.copy()\n",
    "\n",
    "print(f\"\\n2. Testing with incomplete data ({incomplete_data.shape[1]} out of {len(features)} features)...\")\n",
    "\n",
    "# Method 1: Impute missing features (recommended)\n",
    "print(\"\\n--- Method 1: Impute Missing Features ---\")\n",
    "try:\n",
    "    predictions1, probabilities1, status1 = selector.predict_new_data(\n",
    "        incomplete_data, handle_missing_features='impute'\n",
    "    )\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = incomplete_data.copy()\n",
    "    results_df['predicted_wfh'] = predictions1\n",
    "    results_df['wfh_probability'] = probabilities1\n",
    "    #results_df['wfh_prediction'] = results_df['predicted_wfh'].map({0: 'Cannot WFH', 1: 'Can WFH'})\n",
    "    \n",
    "    print(f\"Success! Predictions made for {len(predictions1)} samples\")\n",
    "    print(f\"WFH Rate: {predictions1.mean():.1%}\")\n",
    "    print(f\"Average confidence: {probabilities1.mean():.1%}\")\n",
    "    \n",
    "    # Show predictions\n",
    "    print('\\nIf stopped here, here is the error')\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(results_df[[ 'wfh_probability']].head(10))\n",
    "    print(results_df[[ 'predicted_wfh']].head(10))\n",
    "    \n",
    "    # Show which features were imputed\n",
    "    if status1['missing_features']:\n",
    "        print(\"Imputed features and their importance:\")\n",
    "        importance_info = selector.get_feature_importance_for_missing(status1['missing_features'])\n",
    "        for feature in status1['missing_features']:\n",
    "            if importance_info and feature in importance_info:\n",
    "                print(f\"  {feature}: RF importance = {importance_info[feature]['rf_importance']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with imputation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95fabcfa-72f9-4f36-a267-d04ceb91aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pseudo 2016 data\n",
    "df_16_raw_per_pseudo = results_df.copy()\n",
    "pyrs.write_sav(df_16_raw_per_pseudo, 'lcfs_2016_rawper_pseudo.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94753007-9832-4461-898a-dd606b23f5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
